[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "At its core, machine learning is primarily interested in making sense of complex data. A machine learning algorithm’s job is to read data and identify patterns that can be used for: prediction, feature-relevance detection, model-free classification, among other actions."
  },
  {
    "objectID": "introduction.html#about-this-course",
    "href": "introduction.html#about-this-course",
    "title": "Introduction",
    "section": "About this course",
    "text": "About this course\nPlease watch our Welcome video to meet the organisers and hear from Alex Hunns about the dynamics of the course.In case you do not make it until the end, here are some highlights:\n\nIt is a (mostly) self-paced and fully online course.\nWe provide a non-exhaustive way to go about using the R and Python languages for basic Machine Learning purposes. Some of you may be able to optimise a few lines of code, and we encourage that!\nThere are social hours in gather.town. Join so that you can meet other Machine Learners around the globe!\nThere are no grades to this course. If you attend the Collaborative Policy Event, you may be eligible for some form of certification.\nThe content is introductory.\nBe respectful and kind to other participants!"
  },
  {
    "objectID": "introduction.html#a-general-introduction-to-machine-learning-for-public-policy",
    "href": "introduction.html#a-general-introduction-to-machine-learning-for-public-policy",
    "title": "Introduction",
    "section": "A general introduction to Machine learning for Public Policy",
    "text": "A general introduction to Machine learning for Public Policy\nEmpirical Public Policy research is primarily concerned with causality and understanding counterfactuals. What is the impact of Policy or Programme X on Population Outcome Y? What would have happened in the absence of such a policy? However, there are policy problems that may be solved without necessarily looking for a causal link. We call these `prediction policy problems’ (Kleinberg et al. 2015). Some examples include:\n\nAllocating fire and health inspectors in cities (Glaeser et al. 2016).\nPredicting unemployment spell length to help workers decide on savings rates and job search strategies.\nPredicting highest risk youth for targeting interventions (Chandler,Levitt, and List 2011).\n\n\nHow does a Machine Learn?\nMachine learning is classified in three major branches:\n1. Supervised Learning:\nThis course will be primarily concerned with supervised learning. Supervised learning is analogous to statistical learning: suppose that we observe a quantitative response \\(Y\\) and \\(p\\) predictors, \\(X_1, X_2,..., X_p\\).\nWe can rewrite this in a general linear form as:\n\\[Y = f(X) + u\\] where \\(u\\) is an error term.\n2. Unsupervised Learning:\nUnsupervised learning is known for reading data without labels; it is more computationally complex than supervised learning (because it will look for all possible patterns in the data), but it is also more flexible than supervised learning. You can think of clustering, anomaly detection, neural networks, etc.\n3. Reinforcement Learning:\nReinforcement learning is categorised as Artificial Intelligence. It is more focused on goal-directed learning from interaction than are other approaches to machine learning. As per Sutton and Barto (2015), the three most distinguishing features of reinforcement learning are:\n\nBeing closed-loop; i.e. the learning system’s actions influence its later inputs.\nDoes not have direct instructions as to what actions to take; instead it must discover which actions yield the most reward by trying them out.\nNot knowing where the consequences of actions, including reward signals, play out over extended time periods.\n\n\n\n\n\n\nRelevant trade-offs in Machine Learning\n\nFlexibility vs. interpretability: not all found patterns are directly interpretable.\nPrediction vs. inference: high predictive power does not allow for proper inference.\nGoodness-of-fit vs. over-fitting: how do we know when the fit is good?"
  },
  {
    "objectID": "introduction.html#next-steps",
    "href": "introduction.html#next-steps",
    "title": "Introduction",
    "section": "Next steps",
    "text": "Next steps\nNow is time to start programming! Choose between R and Python to follow the course. We have prepared an introduction, so you can get started:\nAn introduction to R programming\nAn introduction to Python programming"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n\nCopyright © 2025 Michelle González Amador & Stephan Dietrich. All rights reserved."
  },
  {
    "objectID": "treebasedmodels.html",
    "href": "treebasedmodels.html",
    "title": "Tree-based models for classification problems",
    "section": "",
    "text": "This section will cover:"
  },
  {
    "objectID": "treebasedmodels.html#a-general-overview-of-tree-based-methods",
    "href": "treebasedmodels.html#a-general-overview-of-tree-based-methods",
    "title": "Tree-based models for classification problems",
    "section": "A general overview of tree-based methods",
    "text": "A general overview of tree-based methods\nAn introduction to Tree-based machine learning models is given to us by Dr. Francisco Rosales, Assistant Professor at ESAN University (Perú) and Lead Data Scientist (BREIN). You can watch the pre-recorded session below:\n\nSome key points to keep in mind when working through the practical exercise include:\n\nTree-based methods work for both classification and regression problems.\nDecision Trees are both a logical and a technical tool:\n\nthey involve stratifying or segmenting the predictor space into a number of simple regions\nfrom each region, we obtain a relevant metric (e.g. mean/average) and then use that information to make predictions about the observations that belong to that region\n\nDecision Trees are the simplest version of a tree-based method. To improve on a simple splitting algorithm, there exist ensemble learning techniques such as bagging and boosting:\n\nbagging: also known as bootstrap aggregating, it is an ensemble technique used to decrease a model’s variance. A  Random Forest  is a tree-based method that functions on the concept of bagging. The main idea behind a Random Forest model is that, if you partition the data that would be used to create a single decision tree into different parts, create one tree for each of these partitions, and then use a method to “average” the results of all of these different trees, you should end up with a better model.\nboosting: an ensemble technique mainly used to decrease a model’s bias. Like bagging, we create multiple trees from various splits of our training dataset. However, whilst bagging uses bootstrap to create the various data splits (from which each tree is born), in boosting each tree is grown sequentially, using information from the previously built tree. So, boosting doesn’t use bootstrap. Instead each tree is a modified version of the original dataset (each subsequent tree is built from the residuals of the previous model).\n\n\nTo conclude our Malawi case study, we will implement a Random Forest algorithm to our classification problem: given a set of features X (e.g. ownership of a toilet, size of household, etc.), how likely are we to correctly identify an individual’s income class? Recall that this problem has already been approached using a linear regression model (and a lasso linear model) and a logistic classification (i.e. an eager learner model) and whilst there was no improvement between a linear and a lasso linear model, we did increase our model’s predictive ability when we switched from a linear prediction to a classification approach. I had previously claimed that the improvement was marginal — but since the model will be used to determine who gets and who doesn’t get an income supplement (i.e. who’s an eligible recipient of a cash transfer, as part of Malawi’s social protection policies), any improvement is critical and we should try various methods until we find the one that best fits our data.\nSome discussion points before the practical:\n\nWhy did we decide to switch models (from linear to classification)?\nIntuitively, why did a classification model perform better than a linear regression at predicting an individual’s social class based on their monthly per capita consumption?\nHow would a Random Forest classification approach improve our predictive ability? (hint, the answer may be similar to the above one)"
  },
  {
    "objectID": "treebasedmodels.html#practical-example",
    "href": "treebasedmodels.html#practical-example",
    "title": "Tree-based models for classification problems",
    "section": "Practical Example",
    "text": "Practical Example\nAs always, start by opening the libraries that you’ll need to reproduce the script below. We will continue to use the Caret library for machine learning purposes, and some other general libraries for data wrangling and visualisation.\n\nrm(list = ls()) # this line cleans your Global Environment.\nsetwd(\"/Users/lucas/Documents/UNU-CDO/courses/ml4p/ml4p-website-v2\") # set your working directory\n\n# Do not forget to install a package with the install.packages() function if it's the first time you use it!\n\nlibrary(dplyr) # core package for dataframe manipulation. Usually installed and loaded with the tidyverse, but sometimes needs to be loaded in conjunction to avoid warnings.\nlibrary(tidyverse) # a large collection of packages for data manipulation and visualisation.  \nlibrary(caret) # a library with key functions that streamline the process for predictive modelling \nlibrary(skimr) # a package with a set of functions to describe dataframes and more\nlibrary(plyr) # a package for data wrangling\nlibrary(party) # provides a user-friendly interface for creating and analyzing decision trees using recursive partitioning\nlibrary(rpart) # recursive partitioning and regression trees\nlibrary(rpart.plot) # visualising decision trees\nlibrary(rattle) # to obtain a fancy wrapper for the rpart.plot\nlibrary(RColorBrewer) # import more colours \n\n# import data\ndata_malawi &lt;- read_csv(\"data/malawi.csv\") # the file is directly read from the working directory/folder previously set\n\nFor this exercise, we will skip all the data pre-processing steps. At this point, we are all well acquainted with the Malawi dataset, and should be able to create our binary outcome, poor (or not), and clean the dataset in general. If you need to, you can always go back to the Logistic Classification tab and repeat the data preparation process described there.\n\nData Split and Fit\n\nset.seed(1234) # ensures reproducibility of our data split\n\n# data partitioning: train and test datasets\ntrain_idx &lt;- createDataPartition(data_malawi$poor, p = .8, list = FALSE, times = 1) \n\nTrain_df &lt;- data_malawi[ train_idx,]\nTest_df  &lt;- data_malawi[-train_idx,]\n\n# data fit: fit a random forest model\n# (be warned that this may take longer to run than previous models)\n\nrf_train &lt;- train(poor ~ .,\n                  data = Train_df,\n                  method = \"ranger\" # estimates a Random Forest algorithm via the ranger pkg (you may need to install the ranger pkg)\n                  )\n\n# First glimpse at our random forest model\nprint(rf_train)\n\nRandom Forest \n\n9025 samples\n  29 predictor\n   2 classes: 'Y', 'N' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 9025, 9025, 9025, 9025, 9025, 9025, ... \nResampling results across tuning parameters:\n\n  mtry  splitrule   Accuracy   Kappa    \n   2    gini        0.8108829  0.5557409\n   2    extratrees  0.7698647  0.4280448\n  16    gini        0.7999474  0.5472253\n  16    extratrees  0.8023850  0.5525424\n  30    gini        0.7946432  0.5359787\n  30    extratrees  0.7974024  0.5425408\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 2, splitrule = gini\n and min.node.size = 1.\n\n\nIf you read the final box of the print() output, you’ll notice that, given our input Y and X features, and no other information, the optimal random forest model, uses the following:\n\nmtry = 2: mtry is the number of variables to sample at random at each split. This is the number we feed to the recursive partitioning algorithm. At each split, the algorithm will search mtry (=2) variables (a completely different set from the previous split) chosen at random, and pick the best split point.\nsplitrule = gini: the splitting rule/algorithm used. Gini, or the Gini Impurity is a probability that ranges from \\(0\\) to \\(1\\). The lower the value, the more pure the node. Recall that a node that is \\(100\\%\\) pure includes only data from a single class (no noise!), and therefore the splitting stops.\nAccuracy (or \\(1\\) - the error rate): at \\(0.81\\), it improves from our eager learner classification (logistic) approach by \\(0.01\\) and it is highly accurate.\nKappa (adjusted accuracy): at \\(0.55\\), it indicates that our random forest model (on the training data) seems to perform the same as out logistic model. To make a proper comparison, we need to look at the out-of-sample predictions evaluation statistics.\n\n\n\nOut-of-sample predictions\n\n# make predictions using the trained model and the test dataset\n\nset.seed(12345)\npr1 &lt;- predict(rf_train, Test_df, type = \"raw\")\nhead(pr1) # Yes and No output\n\n[1] Y Y Y Y Y Y\nLevels: Y N\n\n# evaluate the predictions using the ConfusionMatrix function from Caret pkg\n\nconfusionMatrix(pr1, Test_df[[\"poor\"]], positive = \"Y\") # positive = \"Y\" indicates that our category of interest is Y (1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    Y    N\n         Y 1344  324\n         N  122  465\n                                          \n               Accuracy : 0.8022          \n                 95% CI : (0.7852, 0.8185)\n    No Information Rate : 0.6501          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.5379          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.9168          \n            Specificity : 0.5894          \n         Pos Pred Value : 0.8058          \n         Neg Pred Value : 0.7922          \n             Prevalence : 0.6501          \n         Detection Rate : 0.5960          \n   Detection Prevalence : 0.7397          \n      Balanced Accuracy : 0.7531          \n                                          \n       'Positive' Class : Y               \n                                          \n\n\nBased on our out-of-sample predictions, the Random Forest algorithm seems to yield pretty similar accuracy in its predictions as the logistic classification algorithm. The performance metrics (accuracy, sensitivity, specificity, kappa) remain the same (as for most classification problems). If you want a refresher of what they mean and how to interpret them, go back one session for a more thorough explanation!\n\n\nFine-tuning parameters\nWe can try to improve our Random Forest model by fine-tuning two parameters: grid and cross-validation\n\n# prepare the grid (create a larger random draw space)\n\ntuneGrid &lt;- expand.grid(mtry = c(1,2, 3, 4),\n                      splitrule = c(\"gini\", \"extratrees\"),\n                      min.node.size = c(1, 3, 5)) \n\n# prepare the folds\ntrControl &lt;- trainControl( method = \"cv\",\n                                    number=5,\n                                    search = 'grid',\n                                    classProbs = TRUE,\n                                    savePredictions = \"final\"\n                           ) # 5-folds cross-validation \n\n\n# fine-tune the model with optimised paramters\n# (again, be ready to wait a few minutes for this to run)\n\nrf_train_tuned &lt;- train(poor ~ .,\n                            data = Train_df,\n                            method = \"ranger\",\n                            tuneGrid = tuneGrid, \n                            trControl = trControl\n                  )\n\n# let's see how the fine-tuned model fared\nprint(rf_train_tuned)\n\nRandom Forest \n\n9025 samples\n  29 predictor\n   2 classes: 'Y', 'N' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 7219, 7220, 7220, 7221, 7220 \nResampling results across tuning parameters:\n\n  mtry  splitrule   min.node.size  Accuracy   Kappa    \n  1     gini        1              0.7819382  0.4594359\n  1     gini        3              0.7822701  0.4593051\n  1     gini        5              0.7826031  0.4600870\n  1     extratrees  1              0.7404976  0.3310123\n  1     extratrees  3              0.7396100  0.3287570\n  1     extratrees  5              0.7404971  0.3308480\n  2     gini        1              0.8142918  0.5674387\n  2     gini        3              0.8134056  0.5653133\n  2     gini        5              0.8137385  0.5661639\n  2     extratrees  1              0.7828240  0.4695433\n  2     extratrees  3              0.7840429  0.4730871\n  2     extratrees  5              0.7830448  0.4705301\n  3     gini        1              0.8160649  0.5769315\n  3     gini        3              0.8144026  0.5730246\n  3     gini        5              0.8156218  0.5755611\n  3     extratrees  1              0.8089749  0.5519067\n  3     extratrees  3              0.8073122  0.5469591\n  3     extratrees  5              0.8070911  0.5471121\n  4     gini        1              0.8139609  0.5730598\n  4     gini        3              0.8157331  0.5778242\n  4     gini        5              0.8146244  0.5748192\n  4     extratrees  1              0.8115228  0.5636714\n  4     extratrees  3              0.8122979  0.5662566\n  4     extratrees  5              0.8131842  0.5681043\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 3, splitrule = gini\n and min.node.size = 1.\n\n\nFine tuning parameters has not done much for our in-sample model. The chosen mtry value and splitting rule were the same. The only parameter where I see improvement is in the (training set) Kappa, from \\(0.55\\) to \\(0.56\\). Will out of sample predictions improve?\n\n# make predictions using the trained model and the test dataset\n\nset.seed(12345)\npr2 &lt;- predict(rf_train_tuned, Test_df, type = \"raw\")\nhead(pr2) # Yes and No output\n\n[1] Y Y Y Y Y Y\nLevels: Y N\n\n# evaluate the predictions using the ConfusionMatrix function from Caret pkg\n\nconfusionMatrix(pr2, Test_df[[\"poor\"]], positive = \"Y\") # positive = \"Y\" indicates that our category of interest is Y (1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    Y    N\n         Y 1316  291\n         N  150  498\n                                          \n               Accuracy : 0.8044          \n                 95% CI : (0.7875, 0.8206)\n    No Information Rate : 0.6501          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.5516          \n                                          \n Mcnemar's Test P-Value : 2.617e-11       \n                                          \n            Sensitivity : 0.8977          \n            Specificity : 0.6312          \n         Pos Pred Value : 0.8189          \n         Neg Pred Value : 0.7685          \n             Prevalence : 0.6501          \n         Detection Rate : 0.5836          \n   Detection Prevalence : 0.7126          \n      Balanced Accuracy : 0.7644          \n                                          \n       'Positive' Class : Y               \n                                          \n\n\nConsistent with the improvements on the train set, the out-of-sample predictions also return a higher adjusted accurcacy (Kappa statistic), and improved specificity and sensitivity. Not by much (e.g. Kappa increase of \\(0.01\\)), but we’ll take what we can get.\nThese results also show that the biggest prediction improvements happen when we make big decisions - such as foregoing the variability of continuous outcomes in favour of classes. Exploring classification algorithms - in this case a logistic and a random forest model - was definitely worthwhile, but did not yield large returns on our predictive abilities.\n\n\nVisualising our model\nTo close the chapter, let’s have a quick look at the sort of plots we can make with a Random Forest algorithm.\n\n# we'll need to re-estimate the rf model using rpart\n\nMyRandomForest &lt;- rpart(poor ~ ., data = Train_df)\n\n# visualise the decision tree (first of many in the forest)\nfancyRpartPlot(MyRandomForest, palettes = c(\"Oranges\",\"Blues\"), main = \"Visualising nodes and splits\") \n\n\n\n\n\n\n\n\nThe fancy Rpart Plot returns the flow chart that we have now learned to call a decision tree. Recall that we have used different packages (and different specifications) for the Random Forest. So, the visualisation that we’re looking at now is not the exact replica of our preferred fine-tuned model. It is, nonetheless, a good way to help you understand how classifications and decisions are made with tree-based methods. If you’d like an in-depth explanation of the plot, you can visit the Rpart.plot pkg documentation."
  },
  {
    "objectID": "treebasedmodels.html#readings",
    "href": "treebasedmodels.html#readings",
    "title": "Tree-based models for classification problems",
    "section": "Readings",
    "text": "Readings\nOptional Readings\n\nDietrich et al. (2022) - Economic Development, weather shocks, and child marriage in South Asia: A machine learning approach.\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "neuralNets.html",
    "href": "neuralNets.html",
    "title": "An Introduction to Neural Networks",
    "section": "",
    "text": "In this session, Prof. Dr. Robin Cowan will give is an intuitive introduction to neural networks. He has also prepared an exercise using data from SHARE, the Survey of Health, Ageing and Retirement in Europe. Please watch his video-lesson to get the intuition behind neural network algorithms and you can then follow policy-relevant application: predicting income-vulnerable older people in Europe.\n\nWhy would being able to predict what will make an older person struggle financially be policy-relevant?\nThis is a discussion point that you can explore. But you might want to investigate what the average old-age pension is in some European countries, and what the average cost of living is. After working for more than half of your life, I’m sure you’d like to live comfortably…"
  },
  {
    "objectID": "neuralNets.html#what-are-neural-networks",
    "href": "neuralNets.html#what-are-neural-networks",
    "title": "An Introduction to Neural Networks",
    "section": "",
    "text": "In this session, Prof. Dr. Robin Cowan will give is an intuitive introduction to neural networks. He has also prepared an exercise using data from SHARE, the Survey of Health, Ageing and Retirement in Europe. Please watch his video-lesson to get the intuition behind neural network algorithms and you can then follow policy-relevant application: predicting income-vulnerable older people in Europe.\n\nWhy would being able to predict what will make an older person struggle financially be policy-relevant?\nThis is a discussion point that you can explore. But you might want to investigate what the average old-age pension is in some European countries, and what the average cost of living is. After working for more than half of your life, I’m sure you’d like to live comfortably…"
  },
  {
    "objectID": "neuralNets.html#r-practical",
    "href": "neuralNets.html#r-practical",
    "title": "An Introduction to Neural Networks",
    "section": "R practical",
    "text": "R practical\nAs always, start by opening the libraries that you’ll need to reproduce the script below.\nUnfortunately, we are unable to share the dataset ourselves. However, if you wish to replicate this exercise at home (and use one of the many target variables that Robin has proposed to see how our model fares for those), you can request access to the dataset by creating an account with SHARE. You’ll need to specify this is for learning purposes, but you won’t be denied it.\n\nrm(list = ls()) # this line cleans your Global Environment.\n\nsetwd(\"/Users/lucas/Documents/UNU-CDO/courses/ml4p/ml4p-website-v2\") # set your working directory\n\n# do not forget to install neuralnet and scales, which are packages we haven't used before\nlibrary(tidyverse) # our favourite data wrangling ackagy \nlibrary(neuralnet) # a package specific for neural networks\nlibrary(scales)    # to control the appearance of axis and legend labels\nlibrary(skimr)     # dataset summary\n\n# import data\nload(\"data/SHARE_DATA.rda\")\n## Notice that we're using the load() function, this is because the dataset is in .rda format, the standard R dataset format\n\n##  Put it into a structure with an easy name, the remove the original\nz &lt;- easySHARE_rel8_0_0\nrm(easySHARE_rel8_0_0)\n\nYou can explore the dataset now (and refer to the SHARE website if you have any questions about the variables).\n\nnames(z)\n\n  [1] \"mergeid\"          \"hhid\"             \"coupleid\"        \n  [4] \"wave\"             \"wavepart\"         \"int_version\"     \n  [7] \"int_year\"         \"int_month\"        \"country\"         \n [10] \"country_mod\"      \"language\"         \"female\"          \n [13] \"dn002_mod\"        \"dn003_mod\"        \"dn004_mod\"       \n [16] \"age\"              \"birth_country\"    \"citizenship\"     \n [19] \"iv009_mod\"        \"q34_re\"           \"isced1997_r\"     \n [22] \"eduyears_mod\"     \"mar_stat\"         \"hhsize\"          \n [25] \"partnerinhh\"      \"int_partner\"      \"age_partner\"     \n [28] \"gender_partner\"   \"mother_alive\"     \"father_alive\"    \n [31] \"siblings_alive\"   \"ch001_\"           \"ch021_mod\"       \n [34] \"ch007_hh\"         \"ch007_km\"         \"sp002_mod\"       \n [37] \"sp003_1_mod\"      \"sp003_2_mod\"      \"sp003_3_mod\"     \n [40] \"sp008_\"           \"sp009_1_mod\"      \"sp009_2_mod\"     \n [43] \"sp009_3_mod\"      \"books_age10\"      \"maths_age10\"     \n [46] \"language_age10\"   \"vaccinated\"       \"childhood_health\"\n [49] \"sphus\"            \"chronic_mod\"      \"casp\"            \n [52] \"euro1\"            \"euro2\"            \"euro3\"           \n [55] \"euro4\"            \"euro5\"            \"euro6\"           \n [58] \"euro7\"            \"euro8\"            \"euro9\"           \n [61] \"euro10\"           \"euro11\"           \"euro12\"          \n [64] \"eurod\"            \"bfi10_extra_mod\"  \"bfi10_agree_mod\" \n [67] \"bfi10_consc_mod\"  \"bfi10_neuro_mod\"  \"bfi10_open_mod\"  \n [70] \"hc002_mod\"        \"hc012_\"           \"hc029_\"          \n [73] \"maxgrip\"          \"adlwa\"            \"adla\"            \n [76] \"iadla\"            \"iadlza\"           \"mobilityind\"     \n [79] \"lgmuscle\"         \"grossmotor\"       \"finemotor\"       \n [82] \"recall_1\"         \"recall_2\"         \"orienti\"         \n [85] \"numeracy_1\"       \"numeracy_2\"       \"bmi\"             \n [88] \"bmi2\"             \"smoking\"          \"ever_smoked\"     \n [91] \"br010_mod\"        \"br015_\"           \"ep005_\"          \n [94] \"ep009_mod\"        \"ep011_mod\"        \"ep013_mod\"       \n [97] \"ep026_mod\"        \"ep036_mod\"        \"co007_\"          \n[100] \"thinc_m\"          \"income_pct_w1\"    \"income_pct_w2\"   \n[103] \"income_pct_w4\"    \"income_pct_w5\"    \"income_pct_w6\"   \n[106] \"income_pct_w7\"    \"income_pct_w8\"   \n\n## and how big it is\ndim(z) \n\n[1] 412110    107\n\n# ==== we can also use our trusted skimr package ==== #\n# skim(z)\n# =================================================== # \n\n# Remember to take out the hashtag to print the command! \n\n\n1. Data Preparation\nNow we are going to clean up some things in the data to make it useful.\n\nSelect a subset of the countries: Spain, France, Italy, Germany, Poland. These are identified in the data with numbers:\n\nSpain 724; France 250; Italy 380; Germany 276; NL 528; Poland 616\n\ncountries &lt;- c(724, 250, 380, 276, 528, 616)\n\n\nIn the dataset, negative numbers indicate some kind of missing data, so we will replace them with NA (R-speak for missing values).\nWe then select years since 2013 (let’s focus on the most recent cohorts)\nRestrict our data to observations that have certain qualities: we want people who are retired (ep005 ==1).\n\n\nz1 &lt;- z %&gt;%\n        filter(country_mod %in% countries )%&gt;% # this line subsets the z dataset to only the countries we're interested in (expressed in the line above)\n        mutate(across(everything(), function(x){replace(x, which(x&lt;0), NA)})) %&gt;% # this line replaces all values across the entire dataframe that are less than 0 to NA (missing)\n        filter(int_year &gt;=2013) %&gt;% # now we're subsetting the dataset to the years 2013 and after\n        filter(ep005_ == 1) # and finally, keeping only people old enought for retirement\n\nAt this point you should have decreased the number of observation by 366431 (new obs. = 45679). z1 now contains a cleaner version of the dataset (feel free to delete z)\nPS. The following symbols %&gt;% are called pipe operators. They belong to the dplyr packaged, which is masked within the tidyverse. They allow you to indicate a series of actions to do to the object in a sequence, just as above.\n\nNow let’s create some variables for our model\n\n## Create the variable migrant  \n## change the nature of married to a dummy variable\n## change the nature of our vaccination variable to zero or 1\n\nz1 &lt;- z1 %&gt;%         \n  mutate(migrant = ifelse(birth_country==country,0,1)) %&gt;%\n  mutate(married=ifelse((mar_stat==1 | mar_stat==2),1,0))%&gt;%\n  mutate(vaccinated=ifelse(vaccinated==1,1,0))\n\nAt this point we should have 109 variables (because we created two new variables and rewrote 1.\n\n\nSelect the variables we want in the analysis\nTo access the full survey with variable definitions, here’s a link to the PDF in English.\n\n## get rid of crazy income values (the people with high income are not not part of our population of interest (regular folks who need to save for retirement))\n\n## and make our dependent variable (co007, which is whether the household struggles to make ends meet) a factor\nz1 &lt;- z1 %&gt;% \n        dplyr::select(female,age,married,hhsize,vaccinated,books_age10,maths_age10,language_age10,childhood_health,migrant,eduyears_mod,co007_,thinc_m,eurod,country_mod,iv009_mod) %&gt;%\n        filter(thinc_m &lt; 100000)%&gt;% # people earning above 100,000 are excluded \n        mutate(co007_ = as.factor(co007_)) \n\n\n\nWhat is our target variable?\nIn the English Questionnaire of the SHARE dataset, the variable asks:\nThinking of your household's total monthly income, would you say that your household is able to make ends meet... (Income struggle)\n\n(the possible answers include: )\n\n    1. With great difficulty\n\n    2. With some difficulty \n\n    3. Fairly easily \n\n    4. Easily\nLet’s work with this variable to turn this into a classification problem.\n\n##  aggregate income struggle variable into 2 categories and add to our data\nz1$co007_mod &lt;- z1$co007_ # here we're just creating a duplicate of the co007_ variable but with a different name\n\n# it's usually a good idea to manipulate a duplicated variable in case you make a mistake and need to call on the original/untransformed data again\n\nz1$co007_mod[z1$co007_ %in% c(1,2)] &lt;- 1 # if the values in var z1$co007_ are 1 or 2, transform them into 1, store this in our new z1$co007_mod variable\n\nz1$co007_mod[z1$co007_ %in% c(3,4)] &lt;- 2 # if the values in var z1$co007_ are 3 or 4, transform them into 2, store this in our new z1$co007_mod variable\n\n## change the way to factor is defined to have only 2 levels\nz1$co007_mod &lt;- as.factor(as.integer(z1$co007_mod))\n\nNow we have a variable that indicates whether a household struggles (1) or doesn’t struggle (2) to make ends meet.\nA different dependent variable could just be income. To make that sensible we make income bands (or ‘bins’): var thinc_m directly asks annual salary.\n\n# we're creating quartiles (to which income quartile do you belong, given your annual salary? the lowest? the highest?)\nz1$inc_bin = cut(z1$thinc_m,quantile(z1$thinc_m,breaks=c(0,0.25,0.5,075,1),na.rm=T))\n\nWe won’t work with the inc_bin (classification) variable, but it’s there if you wish to challenge yourself to create a neural network model for it.\n\n\nCleaning missing values (recall ML needs a full dataset)\n\n## get rid of any observation that contains NA\nsum(is.na(z1))\n\n[1] 140821\n\n#You can get a glimpse of which variables have the most missing values with the skim() function\nskim(z1)\n\n\nData summary\n\n\nName\nz1\n\n\nNumber of rows\n37286\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n15\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nco007_\n623\n0.98\nFALSE\n4\n4: 12700, 3: 12382, 2: 8843, 1: 2738\n\n\nco007_mod\n623\n0.98\nFALSE\n2\n2: 25082, 1: 11581\n\n\ninc_bin\n318\n0.99\nFALSE\n4\n(2.: 9322, (1.: 9321, (3.: 9321, (0,: 9004\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nfemale\n0\n1.00\n0.47\n0.50\n0.0\n0.00\n0.00\n1.00\n1.0\n▇▁▁▁▇\n\n\nage\n2\n1.00\n73.32\n7.86\n42.2\n67.20\n72.30\n78.80\n102.3\n▁▃▇▃▁\n\n\nmarried\n208\n0.99\n0.71\n0.45\n0.0\n0.00\n1.00\n1.00\n1.0\n▃▁▁▁▇\n\n\nhhsize\n0\n1.00\n2.02\n0.88\n1.0\n2.00\n2.00\n2.00\n10.0\n▇▁▁▁▁\n\n\nvaccinated\n29307\n0.21\n0.93\n0.25\n0.0\n1.00\n1.00\n1.00\n1.0\n▁▁▁▁▇\n\n\nbooks_age10\n24733\n0.34\n1.85\n1.11\n1.0\n1.00\n1.00\n3.00\n5.0\n▇▃▂▁▁\n\n\nmaths_age10\n25127\n0.33\n2.80\n0.85\n1.0\n2.00\n3.00\n3.00\n5.0\n▁▃▇▂▁\n\n\nlanguage_age10\n25161\n0.33\n2.81\n0.81\n1.0\n2.00\n3.00\n3.00\n5.0\n▁▃▇▂▁\n\n\nchildhood_health\n29173\n0.22\n2.33\n1.03\n1.0\n1.00\n2.00\n3.00\n6.0\n▇▅▁▁▁\n\n\nmigrant\n250\n0.99\n1.00\n0.00\n1.0\n1.00\n1.00\n1.00\n1.0\n▁▁▇▁▁\n\n\neduyears_mod\n2542\n0.93\n10.21\n4.44\n0.0\n7.00\n10.00\n13.00\n25.0\n▃▇▇▂▁\n\n\nthinc_m\n0\n1.00\n25379.33\n16076.56\n0.0\n14357.39\n21731.12\n32783.51\n99829.1\n▇▇▂▁▁\n\n\neurod\n1485\n0.96\n2.57\n2.31\n0.0\n1.00\n2.00\n4.00\n12.0\n▇▃▂▁▁\n\n\ncountry_mod\n0\n1.00\n434.20\n184.53\n250.0\n276.00\n380.00\n616.00\n724.0\n▇▃▂▂▃\n\n\niv009_mod\n1269\n0.97\n3.68\n1.34\n1.0\n3.00\n4.00\n5.00\n5.0\n▂▂▃▆▇\n\n\n\n\n# we'll use the drop_na() function, which will delete any row if it has at least one missing value (be careful when doing this in your own data cleaning)\nz2 &lt;- drop_na(z1)\ndim(z2)\n\n[1] 6881   18\n\n# z2 is a small subset of the original dataset which contains i) no missing values, ii) only relevant variables for our model on retirement, and iii) a more manageable dataset size\n\n\n\nRescaling data\n\n## age, years of education and income (thinc_m) have a big range, so let's rescale it to  between plus and minus 1\n## scaling allows us to compare data that aren't measured in the same way\nz4 &lt;- z2 %&gt;%\n        mutate(ScaledAge = rescale(age,to=c(-1,1)))%&gt;%\n        mutate(EduYears=rescale(eduyears_mod,to=c(-1,1)))%&gt;%\n        mutate(income = rescale(thinc_m,to=c(-1,1)))\n# z4 is now the working dataset, with 3 more (scaled) variables\n\n## check what we have\nsummary(z4$ScaledAge)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-1.00000 -0.16230  0.01222  0.04492  0.22862  1.00000 \n\n## check what variables we now have in the data\nnames(z4)\n\n [1] \"female\"           \"age\"              \"married\"          \"hhsize\"          \n [5] \"vaccinated\"       \"books_age10\"      \"maths_age10\"      \"language_age10\"  \n [9] \"childhood_health\" \"migrant\"          \"eduyears_mod\"     \"co007_\"          \n[13] \"thinc_m\"          \"eurod\"            \"country_mod\"      \"iv009_mod\"       \n[17] \"co007_mod\"        \"inc_bin\"          \"ScaledAge\"        \"EduYears\"        \n[21] \"income\"          \n\n## let's look at the data just to see if there is anything observable at the start\n## plot the first 100 observations\n## we will use a pairs plot\nplot(head(z4,100))\n\n\n\n\n\n\n\n# you can use the zoom function of the image if you're replicating this script locally (that way you can read the variable names).\n\n## look more closely at migrant and vaccination (the others seem to have a good spread)\ntable(z4$migrant)\n\n\n   1 \n6881 \n\n## there is only one value so no point in including it\n## look at vaccination\ntable(z4$vaccinated)\n\n\n   0    1 \n 367 6514 \n\n\n\n\nSelect a subset of the data, including only relevant variables\n\nz5 &lt;- z4 %&gt;%\n            dplyr::select(female,married,hhsize,books_age10, maths_age10, language_age10, EduYears,eurod, country_mod,iv009_mod, inc_bin, co007_,co007_mod)\n\nNotice that we create new datasets everytime we subset, instead of rewriting the old one. This is probably a good idea in case we need to take a step back.\nTo be able to work with the neuralnet package, it’s best to have dummy variables instead of one variable with various categories. So, let’s start that process:\n\n## now change country to dummy variables\ncountry &lt;- as.factor(z5$country_mod)\ncmat    &lt;- model.matrix(~0+country)\n\nThe model.matrix() function takes a formula and a data frame (or similar structure) and returns a matrix with rows corresponding to cases and columns to predictor variables.\n\n## add to z5\nz5 &lt;- cbind(z5,cmat)\nhead(z5)\n\n       female married hhsize books_age10 maths_age10 language_age10 EduYears\n104800      1       1      2           1           3              3    -0.12\n104803      0       1      2           1           2              3    -0.12\n104808      1       1      2           1           3              3     0.12\n104812      0       1      2           1           3              3    -0.04\n104816      0       1      2           1           3              3     0.20\n104852      0       0      1           3           4              3    -0.12\n       eurod country_mod iv009_mod             inc_bin co007_ co007_mod\n104800     5         276         5 (1.44e+04,2.17e+04]      2         1\n104803     5         276         5 (1.44e+04,2.17e+04]      2         1\n104808     3         276         5 (3.28e+04,9.98e+04]      4         2\n104812     0         276         5 (3.28e+04,9.98e+04]      4         2\n104816     4         276         4 (3.28e+04,9.98e+04]      4         2\n104852     3         276         4 (1.44e+04,2.17e+04]      2         1\n       country250 country276 country380 country528 country724\n104800          0          1          0          0          0\n104803          0          1          0          0          0\n104808          0          1          0          0          0\n104812          0          1          0          0          0\n104816          0          1          0          0          0\n104852          0          1          0          0          0\n\nclass(z5)\n\n[1] \"data.frame\"\n\n\nTo finalise the data preparation, let’s do some variable cleaning:\n\n## fix level names of inc_bin\nlevels(z5$inc_bin) &lt;- c(\"first\",\"second\",\"third\",\"fourth\")\nnames(z5)\n\n [1] \"female\"         \"married\"        \"hhsize\"         \"books_age10\"   \n [5] \"maths_age10\"    \"language_age10\" \"EduYears\"       \"eurod\"         \n [9] \"country_mod\"    \"iv009_mod\"      \"inc_bin\"        \"co007_\"        \n[13] \"co007_mod\"      \"country250\"     \"country276\"     \"country380\"    \n[17] \"country528\"     \"country724\"    \n\n\n\n\n\n2. Model Preparation\nThis time around, we’re not using caret functions to split our data, or define our target and predictors. We’ll do this “manually”. The first thing we want to do, is create and object of the form [target ~ x1 + x2 + …+ xk]. This is how R reads target variables (on the left hand side of the squiggle) and predictors (on the right hand side of the squiggle and separated by + signs).\n\nPrepare model\n\n## now make the formula we want to estimate\nmyform0 &lt;- paste(names(z5)[c(1:8,10,14:18)],collapse=\" + \")\n# the object myform0 contains all the predictor variables for our neural network model\n\nmyform &lt;- paste( \"co007_mod\",c(myform0),sep=\" ~ \")\n\nall.equal(myform,myform0) # returns one mistmatch.\n\n[1] \"1 string mismatch\"\n\n# myform includes the income variable as a predictor, so it's all our previous predictors + co007_mod (as target!)\n\n## look at the formula to make sure we got what we wanted\nprint(myform)\n\n[1] \"co007_mod ~ female + married + hhsize + books_age10 + maths_age10 + language_age10 + EduYears + eurod + iv009_mod + country250 + country276 + country380 + country528 + country724\"\n\n\n\n\nData Split: train and test\n\n## set the random seed so we can duplicate things if we want to \nset.seed(4)\n\n# we're doing this manually, instead of using our trusted caret() package\ntrainRows &lt;- sample(1:nrow(z5),0.8*nrow(z5)) # 80% of data to train\ntestRows  &lt;- (1:nrow(z5))[-trainRows]\n\n## now we have training data: trainz5; and testing data: testz5\ntrainz5   &lt;- z5[trainRows,]\ntestz5    &lt;- z5[testRows,]\n\n\n\nTrain our neural network model!\n\nset.seed(4)\n\nmodel &lt;- neuralnet(\n            myform, ## use the formula we defined above\n            data = trainz5, ## tell it what data to use\n            hidden=c(6), ## define the number and size of hidden layers: here we have one layer with 5 nodes in it\n            linear.output = F, # F to show this is a classification problem (since our predictor is a factor) T returns a linear regression output. This also means that the (default) activation function is the sigmoid!\n            stepmax = 1000000, ## how many iterations to use to train it (1 million, but it converges before that mark)\n            lifesign=\"full\",  ## get some output while it works\n            algorithm = \"rprop+\", # it is a gradient descent algorithm \"Resilient Propagation\". \n            learningrate.limit = NULL,\n            learningrate.factor =\n                list(minus = 0.5, plus = 1.2),\n            threshold = 0.01\n            )\n\nhidden: 6    thresh: 0.01    rep: 1/1    steps:    1000 min thresh: 1.22795192815468\n                                                   2000 min thresh: 0.408629618104168\n                                                   3000 min thresh: 0.255375732991587\n                                                   4000 min thresh: 0.255375732991587\n                                                   5000 min thresh: 0.206075288342767\n                                                   6000 min thresh: 0.181738092119449\n                                                   7000 min thresh: 0.142443872190076\n                                                   8000 min thresh: 0.108056969567729\n                                                   9000 min thresh: 0.0965760103945601\n                                                  10000 min thresh: 0.0924441234865121\n                                                  11000 min thresh: 0.0924441234865121\n                                                  12000 min thresh: 0.0924441234865121\n                                                  13000 min thresh: 0.0903834987550307\n                                                  14000 min thresh: 0.0903834987550307\n                                                  15000 min thresh: 0.0903834987550307\n                                                  16000 min thresh: 0.0903834987550307\n                                                  17000 min thresh: 0.0903834987550307\n                                                  18000 min thresh: 0.0903834987550307\n                                                  19000 min thresh: 0.0903834987550307\n                                                  20000 min thresh: 0.0903834987550307\n                                                  21000 min thresh: 0.0903834987550307\n                                                  22000 min thresh: 0.0903834987550307\n                                                  23000 min thresh: 0.0903834987550307\n                                                  24000 min thresh: 0.0903834987550307\n                                                  25000 min thresh: 0.0903834987550307\n                                                  26000 min thresh: 0.0903834987550307\n                                                  27000 min thresh: 0.0903834987550307\n                                                  28000 min thresh: 0.0903834987550307\n                                                  29000 min thresh: 0.0903834987550307\n                                                  30000 min thresh: 0.0903834987550307\n                                                  31000 min thresh: 0.0903834987550307\n                                                  32000 min thresh: 0.0903834987550307\n                                                  33000 min thresh: 0.0903834987550307\n                                                  34000 min thresh: 0.0903834987550307\n                                                  35000 min thresh: 0.0903834987550307\n                                                  36000 min thresh: 0.0903834987550307\n                                                  37000 min thresh: 0.0903834987550307\n                                                  38000 min thresh: 0.0903834987550307\n                                                  39000 min thresh: 0.080031973683847\n                                                  40000 min thresh: 0.080031973683847\n                                                  41000 min thresh: 0.080031973683847\n                                                  42000 min thresh: 0.0781654073237371\n                                                  43000 min thresh: 0.077217534035844\n                                                  44000 min thresh: 0.077217534035844\n                                                  45000 min thresh: 0.0663306621049722\n                                                  46000 min thresh: 0.0663306621049722\n                                                  47000 min thresh: 0.0663306621049722\n                                                  48000 min thresh: 0.0663306621049722\n                                                  49000 min thresh: 0.0640628426880909\n                                                  50000 min thresh: 0.0549832161036877\n                                                  51000 min thresh: 0.0549832161036877\n                                                  52000 min thresh: 0.0549832161036877\n                                                  53000 min thresh: 0.0535065197695844\n                                                  54000 min thresh: 0.0451843906210169\n                                                  55000 min thresh: 0.0407706834064874\n                                                  56000 min thresh: 0.0407706834064874\n                                                  57000 min thresh: 0.0407706834064874\n                                                  58000 min thresh: 0.0407706834064874\n                                                  59000 min thresh: 0.0407706834064874\n                                                  60000 min thresh: 0.0398750949182902\n                                                  61000 min thresh: 0.0347740947416519\n                                                  62000 min thresh: 0.0347740947416519\n                                                  63000 min thresh: 0.0347740947416519\n                                                  64000 min thresh: 0.0333526116102184\n                                                  65000 min thresh: 0.0333526116102184\n                                                  66000 min thresh: 0.0333526116102184\n                                                  67000 min thresh: 0.0333526116102184\n                                                  68000 min thresh: 0.0333526116102184\n                                                  69000 min thresh: 0.0316000239700686\n                                                  70000 min thresh: 0.0306565221261171\n                                                  71000 min thresh: 0.0274365035081705\n                                                  72000 min thresh: 0.0274365035081705\n                                                  73000 min thresh: 0.026496818727535\n                                                  74000 min thresh: 0.026496818727535\n                                                  75000 min thresh: 0.026496818727535\n                                                  76000 min thresh: 0.026496818727535\n                                                  77000 min thresh: 0.026496818727535\n                                                  78000 min thresh: 0.026496818727535\n                                                  79000 min thresh: 0.026496818727535\n                                                  80000 min thresh: 0.026496818727535\n                                                  81000 min thresh: 0.0259845739797748\n                                                  82000 min thresh: 0.0259845739797748\n                                                  83000 min thresh: 0.0259845739797748\n                                                  84000 min thresh: 0.0240579990735271\n                                                  85000 min thresh: 0.0226252873882434\n                                                  86000 min thresh: 0.0226252873882434\n                                                  87000 min thresh: 0.0221455597930601\n                                                  88000 min thresh: 0.0221455597930601\n                                                  89000 min thresh: 0.0221455597930601\n                                                  90000 min thresh: 0.0221455597930601\n                                                  91000 min thresh: 0.0203603362622581\n                                                  92000 min thresh: 0.0200746219013495\n                                                  93000 min thresh: 0.0200746219013495\n                                                  94000 min thresh: 0.0200746219013495\n                                                  95000 min thresh: 0.0200746219013495\n                                                  96000 min thresh: 0.0200746219013495\n                                                  97000 min thresh: 0.0163554171793395\n                                                  98000 min thresh: 0.0163554171793395\n                                                  99000 min thresh: 0.0163554171793395\n                                                  1e+05 min thresh: 0.0151167048702273\n                                                 101000 min thresh: 0.0151167048702273\n                                                 102000 min thresh: 0.0151167048702273\n                                                 103000 min thresh: 0.0151167048702273\n                                                 104000 min thresh: 0.0151167048702273\n                                                 105000 min thresh: 0.0151167048702273\n                                                 106000 min thresh: 0.0151167048702273\n                                                 107000 min thresh: 0.0151167048702273\n                                                 108000 min thresh: 0.0151167048702273\n                                                 109000 min thresh: 0.0142885225751462\n                                                 110000 min thresh: 0.0142885225751462\n                                                 111000 min thresh: 0.0142885225751462\n                                                 112000 min thresh: 0.0142885225751462\n                                                 113000 min thresh: 0.0142885225751462\n                                                 114000 min thresh: 0.0142885225751462\n                                                 115000 min thresh: 0.0142455799962317\n                                                 116000 min thresh: 0.0142455799962317\n                                                 117000 min thresh: 0.0142455799962317\n                                                 118000 min thresh: 0.012107497406225\n                                                 119000 min thresh: 0.012107497406225\n                                                 120000 min thresh: 0.012107497406225\n                                                 121000 min thresh: 0.012107497406225\n                                                 122000 min thresh: 0.012107497406225\n                                                 123000 min thresh: 0.0108934985445289\n                                                 124000 min thresh: 0.0108934985445289\n                                                 125000 min thresh: 0.0108934985445289\n                                                 126000 min thresh: 0.0108934985445289\n                                                 127000 min thresh: 0.0108934985445289\n                                                 128000 min thresh: 0.0108934985445289\n                                                 129000 min thresh: 0.0108934985445289\n                                                 130000 min thresh: 0.0108934985445289\n                                                 131000 min thresh: 0.0108934985445289\n                                                 132000 min thresh: 0.0108934985445289\n                                                 133000 min thresh: 0.0108934985445289\n                                                 134000 min thresh: 0.0108934985445289\n                                                 135000 min thresh: 0.0108934985445289\n                                                 136000 min thresh: 0.0108934985445289\n                                                 137000 min thresh: 0.0101758648461206\n                                                 138000 min thresh: 0.0101758648461206\n                                                 139000 min thresh: 0.0101758648461206\n                                                 140000 min thresh: 0.0101758648461206\n                                                 141000 min thresh: 0.0101758648461206\n                                                 142000 min thresh: 0.0101758648461206\n                                                 143000 min thresh: 0.0101758648461206\n                                                 143866 error: 944.06645    time: 5.7 mins\n\n# if you get an error, don't worry about it. It's not an issue for our estimation, and it indicates the last iteration (way before 1000000)\n\nNow, let’s plot our neural network:\n\nplot(model)\n\n# notice that this is a vanilla network (no deep learning for us!).\n\n  \nNow it is time to test our model’s predictive abilities.\n\n# use our fitted neural network to try to predict what the income states in our test data\nset.seed(4)\npred &lt;- predict(model,testz5)\n\nAs before, we will not use the caret package to call the ConfusionMatrix function, we’ll do all of it manually. We’ll have to manipulate the variables a little, to visualise the confusion matrix in a helpful way:\n\n# add the levels from our target variable as labels (stored in an object)\n\nmyLabels &lt;- levels(testz5$co007_mod)\npointPred &lt;- max.col(pred) # find the index of the maximum value of my predictions\npointPred &lt;- myLabels[pointPred] # add the labels (or column names from our predictions) \n\n# Now, we'll store the actual/observed values in an objext as well:\nactual &lt;- testz5$co007_mod\nactual &lt;- as.factor(as.integer(actual))\n# Create the Confusion Matrix using the predicted, observed values and the labels\nt1 &lt;- table(actual,pointPred)\n\n# voilà! manual confusion matrix!\nprint(t1)\n\n      pointPred\nactual   1   2\n     1 134 259\n     2 114 870\n\n\nNow that we have a confusion matrix, we can analyse the performance of our neural network model.\n\n# How many older people struggle to make ends meet?\nprop.table(table(testz5$co007_mod))\n\n\n        1         2 \n0.2854031 0.7145969 \n\n# about 28%... \n\n# How accurate is our model?\n\nsum(diag(t1))/sum(t1)\n\n[1] 0.7291213\n\n# this returns the proportion of correct predictions! \n# 72% of correctly predicted income status\n# so, our neural network model does relatively well in predicting whether older people / pensioneers struggle to make ends meet in selected European countries\n\nIf we recall from previous sessions, it is hard to have accurate predictions of that of which we have less (struggling older people)… look again at the confusion matrix: what do we see? the ratio of correct predictions for non-strugglers (2x2) is higher than for the strugglers (1x1).\nLet’s remember what information we can obtain from a confusion matrix:\n\nTrue Positives (TP): 134 (actual = 1, predicted = 1)\nFalse Negatives (FN): 259 (actual = 1, predicted = 2)\nFalse Positives (FP): 114 (actual = 2, predicted = 1)\nTrue Negatives (TN): 870 (actual = 2, predicted = 2)\n\nBased on these confusion matrix values (and the formulas provided in session 2: logistic classification), we can get our neural network model’s performance metrics:\n\nAccuracy: 72.9%. (we got this above!). It is the proportion of true results regardless of the case.\nRecall (Sensitivity): 34.09% (we might want to know this, since we’re trying to identify vulnerable elderly people). It is the proportion of correctly identified vulnerable cases. The formula (if you want to check yourself) is TP/(TP+FN) = 134/(134+259) = 0.340\n\nAlternatively…\n\n# select the needed values from the confusion matrix t1\n(t1[1,1])/(t1[1,1]+t1[1,2])\n\n[1] 0.3409669"
  },
  {
    "objectID": "neuralNets.html#conclusion",
    "href": "neuralNets.html#conclusion",
    "title": "An Introduction to Neural Networks",
    "section": "Conclusion",
    "text": "Conclusion\nHow did we do? Neural networks are all the rage these days, it’s arguably the most famous machine learning algorithm. But don’t be fooled, it is still subject to the same data challenges as the rest of the algorithms we have explored so far."
  },
  {
    "objectID": "neuralNets.html#readings",
    "href": "neuralNets.html#readings",
    "title": "An Introduction to Neural Networks",
    "section": "Readings",
    "text": "Readings\nOptional Readings\n\nChatsiou and Mikhaylov (2020). Deep Learning for Political Science. Arxiv preprint."
  },
  {
    "objectID": "r-intro.html",
    "href": "r-intro.html",
    "title": "An Introduction to R Programming",
    "section": "",
    "text": "For the purposes of this course, we will be working with the integrated development environment (IDE) Rstudio. Make sure you have downloaded it and have familiarised yourself with the interface before proceeding."
  },
  {
    "objectID": "r-intro.html#features-of-r",
    "href": "r-intro.html#features-of-r",
    "title": "An Introduction to R Programming",
    "section": "Features of R",
    "text": "Features of R\nR is a statistical programming language. As such, it understands and categorises its input as data types:\n(Note that R is not too strict about data types, but you need to be able to identify them to use them in math operations.)\nExamples of data types:\n\n# Numeric (or double): \nnumeric_vector &lt;- c(1, 1.0, 65.5)\nprint(numeric_vector)\n\n[1]  1.0  1.0 65.5\n\n#Integers: \ninteger_vector &lt;- c(1L, 3L, 45L)\nprint(integer_vector)\n\n[1]  1  3 45\n\n#Logical (or boolean): \nboolean_vector &lt;- c(TRUE, FALSE)\nprint(boolean_vector)\n\n[1]  TRUE FALSE\n\n#Character: \ncharacter_vector &lt;- c(\"Harry Potter\", \"Star Wars\", \"Lord of the Rings\")\nprint(character_vector)\n\n[1] \"Harry Potter\"      \"Star Wars\"         \"Lord of the Rings\"\n\n#Factor: (also knows as categorical variables)\nfactor_vector &lt;- as.factor(c(\"male\",\"female\"))\nprint(factor_vector)\n\n[1] male   female\nLevels: female male\n\n#Missing: \nNA\n\n[1] NA\n\n\n… and data structures. A data structure is either homogeneous (all elements are of the same data type) or heterogeneous (elements can be of more than one data type).\n\nExamples of data structures:\nVectors: think of a row, or a column of the same element.\n\n#Vectors: \nx &lt;- c(1L,3L,5L,7L,9L) # we call this an integer vector\ny &lt;- c(1.3, 1, 5, 7, 11.2) # we call this a numerical (or double) vector\n\nprint(x)\n\n[1] 1 3 5 7 9\n\nprint(y)\n\n[1]  1.3  1.0  5.0  7.0 11.2\n\n\nMatrices: they have rows and columns containing elements of the same type.\n\n#Matrices: \nA &lt;- matrix(1:9, ncol=3, nrow=3, byrow= TRUE)\nprint(A)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\nArrays: A vector is a one-dimensional array. A matrix is a two-dimensional array. In short, an array is a collection of data of the same type.\n\n#Arrays:\nn &lt;- 5*5*3 \nB &lt;- array(1:n, c(5,5,3))\nprint(B)\n\n, , 1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    6   11   16   21\n[2,]    2    7   12   17   22\n[3,]    3    8   13   18   23\n[4,]    4    9   14   19   24\n[5,]    5   10   15   20   25\n\n, , 2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   26   31   36   41   46\n[2,]   27   32   37   42   47\n[3,]   28   33   38   43   48\n[4,]   29   34   39   44   49\n[5,]   30   35   40   45   50\n\n, , 3\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   51   56   61   66   71\n[2,]   52   57   62   67   72\n[3,]   53   58   63   68   73\n[4,]   54   59   64   69   74\n[5,]   55   60   65   70   75\n\n\nLists: a list is a one-dimensional, heterogeneous data structure. Basically, it is an object that stores all object types.\n\nmy_list &lt;- list(42, \"The answer is\", TRUE)\nprint(my_list)\n\n[[1]]\n[1] 42\n\n[[2]]\n[1] \"The answer is\"\n\n[[3]]\n[1] TRUE\n\n\nData frames: a data frame is a list of column vectors. Each vector must contain the same data type, but the different vectors can store different data types. Note, however, that in a data frame all vectors must have the same length.\n\na &lt;- 1L:5L\nclass(a)\n\n[1] \"integer\"\n\nb &lt;- c(\"a1\", \"b2\", \"c3\", \"d4\", \"e5\")\nclass(b)\n\n[1] \"character\"\n\nc &lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE)\nclass(c)\n\n[1] \"logical\"\n\ndf &lt;- as.data.frame(cbind(a,b,c))\n\nstr(df)\n\n'data.frame':   5 obs. of  3 variables:\n $ a: chr  \"1\" \"2\" \"3\" \"4\" ...\n $ b: chr  \"a1\" \"b2\" \"c3\" \"d4\" ...\n $ c: chr  \"TRUE\" \"TRUE\" \"FALSE\" \"TRUE\" ...\n\n\nNotice that even though vector a in dataframe df is of class integer, vector b is of class character, and vector ^c is of class boolean/logical, when binding them together they have been coerced into factors. You’ll have to manually transform them into their original class to be able to use them in math operations.\n\ndf$a &lt;- as.integer(df$a)\ndf$b &lt;- as.character(df$b)\ndf$c &lt;- as.logical(df$c)\n\nstr(df)\n\n'data.frame':   5 obs. of  3 variables:\n $ a: int  1 2 3 4 5\n $ b: chr  \"a1\" \"b2\" \"c3\" \"d4\" ...\n $ c: logi  TRUE TRUE FALSE TRUE FALSE\n\n\nFrom here on, we will write an R script together, and learn some basic commands and tools that will allow you to explore and manipulate data. Please note that this is not an exhaustive tutorial. It is nonetheless a good place to start."
  },
  {
    "objectID": "r-intro.html#setting-up-the-rstudio-working-environment",
    "href": "r-intro.html#setting-up-the-rstudio-working-environment",
    "title": "An Introduction to R Programming",
    "section": "1. Setting up the Rstudio working environment",
    "text": "1. Setting up the Rstudio working environment\nIt is good practice to make sure that the working environment is empty/clean before you start running any code.\n\n# The r base command rm() stands for remove\nrm(list = ls()) # this line indicates R to clear absolutely everything from the environment\n\nOnce that has been taken care of, you need to load the libraries you will be working with. While r base has a large number of commands to explore, wrangle, and manipulate data, the open source feature of R means that people all over the world are constantly working on packages and functions to make our lives easier. These can be used by calling the libraries in which they are stored:\n\n#  My personal favourite are the Tidyverse library, by Hadley Whickam, and data.table. Both are brilliant for data exploration, manipulation, and visualisation. \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\nIf you’re working with an imported data set, you should probably set up your working directory as well:\n\nsetwd(\"/Users/lucas/Documents/UNU-CDO/courses/ml4p/ml4p-website-v2\")\n\nFrom the working directory, you can call documents: .csv files, .xls and .xlsx, images, .txt, .dta (yes, STATA files!), and more. You’ll need to use the right libraries to do so. For instance: readxl (from the Tidyverse) uses the function read_excel() to import .xlsx and .xls files. If you want to export a data frame in .xlsx format, you can use the package write_xlsx()."
  },
  {
    "objectID": "r-intro.html#r-base-commands-for-data-set-exploration",
    "href": "r-intro.html#r-base-commands-for-data-set-exploration",
    "title": "An Introduction to R Programming",
    "section": "2. R base commands for data set exploration",
    "text": "2. R base commands for data set exploration\nNow that we have all the basic stuff set up, let’s start with some basic r base commands that will allow us to explore our data. To do so, we will work with the toy data set mtcars that can be called from R without the need to upload data or call data from a website.\n\n# some basics to explore your data \nstr(mtcars) # show the structure of the object in a compact format\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\ndim(mtcars) # inspect the dimension of the dataset (returns #rows, #columns)\n\n[1] 32 11\n\nclass(mtcars) # evaluate the class of the object (e.g. numeric, factor, character...)\n\n[1] \"data.frame\"\n\nlength(mtcars$mpg) # evaluate the number of elements in vector mpg\n\n[1] 32\n\nmean(mtcars$mpg) # mean of all elements in vector mpg\n\n[1] 20.09062\n\nsum(mtcars$mpg) # sum of all elements in vector mpg (similar to a column sum)\n\n[1] 642.9\n\nsd(mtcars$mpg) # standard deviation\n\n[1] 6.026948\n\nmedian(mtcars$mpg) # median\n\n[1] 19.2\n\ncor(mtcars$mpg, mtcars$wt) # default is pearson correlation, specify method within function to change it.  \n\n[1] -0.8676594\n\ntable(mtcars$am) #categorical data in a table: counts\n\n\n 0  1 \n19 13 \n\nprop.table(table(mtcars$am)) #categorical data in a table: proportions\n\n\n      0       1 \n0.59375 0.40625"
  },
  {
    "objectID": "r-intro.html#objects-and-assignments",
    "href": "r-intro.html#objects-and-assignments",
    "title": "An Introduction to R Programming",
    "section": "3. Objects and assignments",
    "text": "3. Objects and assignments\nAnother important feature of the R programming language is that it is object oriented. For the most part, for every function used, there must be an object assigned! Let’s see an example of object assignment with a bivariate linear regression model:\n\nols_model &lt;- lm(mpg ~ wt, data = mtcars) # lm stands for linear model. In parenthesis, dependent variable first, independent variable after the squiggly.\nsummary(ols_model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nIf you would like to see the results from your regression, you do not need to run it again. Instead, you print the object (ols_model) you have assigned for the linear model function. Similarly, you can call information stored in that object at any time, for example, the estimated coefficients:\n\nols_model$coefficients\n\n(Intercept)          wt \n  37.285126   -5.344472 \n\n\nIf you want to read more about object oriented programming in R, check out Hadley Whickam’s site"
  },
  {
    "objectID": "r-intro.html#plotting-with-and-without-special-libraries",
    "href": "r-intro.html#plotting-with-and-without-special-libraries",
    "title": "An Introduction to R Programming",
    "section": "4. Plotting with and without special libraries",
    "text": "4. Plotting with and without special libraries\nWe had previously loaded a couple of libraries. Why did we do that if we’ve only used r base commands so far? We’re going to exemplify the power of libraries by drawing plots using r base, and ggplot2. Ggplot2 is the plotting function from the Tidyverse, and arguably one of the best data visualisation tools across programming languages. If you’d like to read more about why that is the case, check out the Grammar of Graphics site.\nPlotting with R base\n\nplot(mtcars$wt, mtcars$mpg, pch = 14, col = \"grey\", main =\"Mileage and Weight\")\nabline(ols_model, col =\"blue\") # Note that to add a line of best fit, we had to call our previously estimate linear model, stored in the ols_model object-\n\n\n\n\n\n\n\n# with base R, you cannot directly assign an object to a plot, you need to use...\np_rbase &lt;- recordPlot()\n# plot.new() # don't forget to clean up your device afterwards!\n\nPlotting with ggplot2, using the grammar of graphics\n\n# Steps in the Grammar of Graphics\n# 1: linking plot to dataset, \n# 2: defining (aes)thetic mapping, \n# 3: use (geom)etric objects such as points, bars, etc. as markers, \n# 4: the plot has layers\n\np_ggplot &lt;- ggplot(data = mtcars, aes(x = wt, y = mpg, col=am )) + #the color is defined by car type (automatic 0 or manual 1)\n    geom_smooth(method = \"lm\", col= \"orange\") + # no need to run a regression ex-ante to add a line of best fit \n    geom_point(alpha=0.5) + #alpha controls transparency of the geom (a.k.a. data point)\n    theme(legend.position=\"none\") #removing legend\nprint(p_ggplot)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThanks to the grammar of graphics, we can continue to edit the plot after we have finished it. Perhaps we’ve come up with ideas to make it more stylish? Or helpful? Let’s see an example:\n\np_ggplot &lt;- p_ggplot + theme(legend.position = \"right\") # I am saving it with the same name again, but I could easily choose another name and keep two versions of the plot. \nprint(p_ggplot) # the legend we just added is NOT helpful. Why is that? \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Remember when we talked about data types?!\n\nclass(mtcars$am) #for legends, we might prefer levels/categories \n\n[1] \"numeric\"\n\nmtcars$am &lt;- as.factor(mtcars$am) # we have now transformed the numeric am into a factor variable \n#the importance of assigning objects :)\n\n#  Now we can plot our scatterplot without issues\np_ggplot &lt;- ggplot(data = mtcars, aes(x = wt, y = mpg, col = am )) +\n    geom_smooth(method = \"lm\", col= \"red\") +\n    geom_point(alpha=0.5) +\n    theme(legend.position=\"right\") +\n    theme_classic() \n\nprint(p_ggplot)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Shall we continue?\n\np_ggplot &lt;- p_ggplot + ggtitle(\"Scatterplot of mileage vs weight of car\") +\n    xlab(\"Weight of car\") + ylab(\"Miles per gallon\") +\n    theme(plot.title = element_text(color=\"black\", size=14, face=\"bold\", hjust = 0.5))\n\n\np_ggplot &lt;- p_ggplot + scale_colour_manual(name = \"Automatic or Manual\", \n                                           labels = c(\"0.Automatic\", \"1.Manual\"),\n                                           values = c(\"darkorange2\", \"darkgoldenrod1\"))\n\nprint(p_ggplot)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Finally, perhaps we want two lines of best fit that follow the shape of the value dispersion by car type, and not the linear model function?\n\np &lt;- ggplot(data = mtcars, aes(x = wt, y = mpg, col = am )) +\n    geom_smooth() +\n    geom_point(alpha=0.5) +\n    theme(legend.position=\"right\") +\n    theme_classic() + ggtitle(\"Scatterplot of mileage vs weight of car\") +\n    xlab(\"Weight of car\") + ylab(\"Miles per gallon\") +\n    theme(plot.title = element_text(color=\"black\", size=14, face=\"bold\", hjust = 0.5)) + \n    scale_colour_manual(name = \"Automatic or Manual\", \n                        labels = c(\"0.Automatic\", \"1.Manual\"),\n                        values = c(\"darkorange2\", \"darkgoldenrod1\"))\nprint(p)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "r-intro.html#r-base-capabilities",
    "href": "r-intro.html#r-base-capabilities",
    "title": "An Introduction to R Programming",
    "section": "5. R base capabilities",
    "text": "5. R base capabilities\nWe’ve had some fun, now let’s go back to some r base basics. These are going to be relevant for making algorithms of your own:\n\nArithmetic and other operations\n\n2+2 #addition\n\n[1] 4\n\n5-2 #subtraction\n\n[1] 3\n\n33.3/2 #division\n\n[1] 16.65\n\n5^2 #exponentiation\n\n[1] 25\n\n200 %/% 60 #integer division  [or, how many full hours in 200 minutes?]/aka quotient\n\n[1] 3\n\n200 %% 60 #remainder [or, how many minutes are left over? ]\n\n[1] 20\n\n\n\n\nLogical operators\n\n34 &lt; 35 #smaller than\n\n[1] TRUE\n\n34 &lt; 35 | 33 #smaller than OR than (returns true if it is smaller than any one of them)\n\n[1] TRUE\n\n34 &gt; 35 & 33 #bigger than AND than (returns true only if both conditions apply)\n\n[1] FALSE\n\n34 != 34 #negation\n\n[1] FALSE\n\n34 %in% 1:100 #value matching (is the object contained in the list of items?)\n\n[1] TRUE\n\n`%ni%` &lt;- Negate(`%in%`) #let's create a function for NOT IN\n1001 %ni% 1:100 # home-made function! :)\n\n[1] TRUE\n\n34==34 #evaluation\n\n[1] TRUE\n\n\n\n\nIndexing\nThe tidyverse comes with a cool Starwars dataframe (or tibble, in the tidyverse language). As long as you have loaded the tidyverse library, you can use it.\n\nhead(starwars) # print the first 5 elements of the dataframe/tibble\n\n# A tibble: 6 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Luke Sky…    172    77 blond      fair       blue            19   male  mascu…\n2 C-3PO        167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n3 R2-D2         96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n4 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n5 Leia Org…    150    49 brown      light      brown           19   fema… femin…\n6 Owen Lars    178   120 brown, gr… light      blue            52   male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars$name[1] #indexing: extract the first element in the \"name\" vector from the starwars dataframe\n\n[1] \"Luke Skywalker\"\n\nstarwars[1,1] #alternative indexing: extract the element of row 1, col 1\n\n# A tibble: 1 × 1\n  name          \n  &lt;chr&gt;         \n1 Luke Skywalker\n\nstarwars$name[2:4] # elements 2, 3, 4 of \"name\" vector\n\n[1] \"C-3PO\"       \"R2-D2\"       \"Darth Vader\"\n\nstarwars[,1] #extract all elements from column 1\n\n# A tibble: 87 × 1\n   name              \n   &lt;chr&gt;             \n 1 Luke Skywalker    \n 2 C-3PO             \n 3 R2-D2             \n 4 Darth Vader       \n 5 Leia Organa       \n 6 Owen Lars         \n 7 Beru Whitesun Lars\n 8 R5-D4             \n 9 Biggs Darklighter \n10 Obi-Wan Kenobi    \n# ℹ 77 more rows\n\nstarwars[1,] #extract all elements from row 1\n\n# A tibble: 1 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Luke Sky…    172    77 blond      fair       blue              19 male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars$height[starwars$height&lt;150] # returns a logical vector TRUE for elements &gt;150 in height vector\n\n [1]  96  97  66  NA  88 137 112  79  94 122  96  NA  NA  NA  NA  NA\n\nstarwars[c('height', 'name')] #returns c(oncatenated) vectors\n\n# A tibble: 87 × 2\n   height name              \n    &lt;int&gt; &lt;chr&gt;             \n 1    172 Luke Skywalker    \n 2    167 C-3PO             \n 3     96 R2-D2             \n 4    202 Darth Vader       \n 5    150 Leia Organa       \n 6    178 Owen Lars         \n 7    165 Beru Whitesun Lars\n 8     97 R5-D4             \n 9    183 Biggs Darklighter \n10    182 Obi-Wan Kenobi    \n# ℹ 77 more rows\n\n\nWe are now reaching the end of this brief introduction to R and Rstudio. We will not go into the fun stuff you can do with data.table, you can find that out on your own if you need to (but know it is a powerful data wrangling package), and instead we will finalise with Reserved Names in R. These are names you cannot use for your objects, because they serve a programming purpose."
  },
  {
    "objectID": "r-intro.html#reserved-names",
    "href": "r-intro.html#reserved-names",
    "title": "An Introduction to R Programming",
    "section": "6. Reserved names",
    "text": "6. Reserved names\n\n1. Not-observed data\n\n# 1.1 NaN: results that cannot be reasonably defined\nh &lt;- 0/0\nis.nan(h)\n\n[1] TRUE\n\nclass(h)\n\n[1] \"numeric\"\n\nprint(h)\n\n[1] NaN\n\n# 1.2 NA: missing data\ncolSums(is.na(starwars)) #How many missings in the starwars tibble*?\n\n      name     height       mass hair_color skin_color  eye_color birth_year \n         0          6         28          5          0          0         44 \n       sex     gender  homeworld    species      films   vehicles  starships \n         4          4         10          4          0          0          0 \n\nmean(starwars$height) # evaluation gives NA? Does that mean that the height vector is empty?!\n\n[1] NA\n\nis.na(starwars$height) # logical vector returns 6 true statements for is.na (coincides with colSums table)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n[85]  TRUE  TRUE  TRUE\n\nclass(starwars$height) # class evaluation returns integer...\n\n[1] \"integer\"\n\nmean(as.integer(starwars$height), na.rm = TRUE) # read as integer, ignore NAs :) (rm stands for remove)\n\n[1] 174.6049\n\n# missing values (NA) are regarded by R as non comparables, even to themselves, so if you ever encounter a missing values in a vector, make sure tto explicitly tell the function you are using what to do with them.\n\n\n\n2. if, else, ifelse, function, for, print, length (etc…)*\nThese words are reserved for control flow and looping. \n\n# if else statements\nmeanheight &lt;- 174.358\n\nif ( meanheight &lt; 175) {\n    print('very tall!')\n} else {\n    print('average height')\n} \n\n[1] \"very tall!\"\n\n# a note on using if else: else should be on the same line as if, otherwise it is not recognised\n\n# home-made functions\nf1 &lt;- function(x){\n    return(sum(x+1))\n}\nprint(f1(5)) # returns value of x+1 when x = 5\n\n[1] 6\n\n# for loops\nclass(starwars$height) # if it is not integer or numeric, then transform it! \n\n[1] \"integer\"\n\nstarwars$height &lt;- as.numeric(as.character(starwars$height)) # transforming the height vector to numeric\nstarwars$height_judge = NA # if you're using a new object within a for loop, make sure you initialize it before running it\n\nfor (i in 1:length(starwars$height)) {\n    \n    starwars$height_judge[i] &lt;- ifelse(starwars$height[i]&lt;100, \"Short\", \"Tall\")\n}\nprint(starwars[c(\"name\", \"height_judge\", \"height\")])\n\n# A tibble: 87 × 3\n   name               height_judge height\n   &lt;chr&gt;              &lt;chr&gt;         &lt;dbl&gt;\n 1 Luke Skywalker     Tall            172\n 2 C-3PO              Tall            167\n 3 R2-D2              Short            96\n 4 Darth Vader        Tall            202\n 5 Leia Organa        Tall            150\n 6 Owen Lars          Tall            178\n 7 Beru Whitesun Lars Tall            165\n 8 R5-D4              Short            97\n 9 Biggs Darklighter  Tall            183\n10 Obi-Wan Kenobi     Tall            182\n# ℹ 77 more rows\n\n\nYou can download the R script by clicking on the button below.\n Download Intro to R script (.R) \n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "python-intro.html",
    "href": "python-intro.html",
    "title": "An introduction to Python programming",
    "section": "",
    "text": "If you would like to follow the course with Python, we will offer some guidance. Bear in mind that the R and Python scripts are not meant to be exact replicas of one another, this is because the languages offer different data wrangling packages. However, you’ll be able to get to the core of the course content.\nWith that said, let’s start with the first difference. Python does not have a standard IDE like Rstudio for R. You can choose to work directly with the console REPL, with Jupyter notebooks or the IDE Visual Studio Code. We recommend to use VSCode, because it offers many functionalities for code-sharing and collaboration, and you can integrate AI-helpers to guide you throughout your coding tasks.\nDownload VSCode and let’s get started! \nWhen you open the IDE VSCode, you’ll receive a prompt to start a new project. Please make sure that when you do, you automatically save it in a dedicated folder. This will be relevant for many reasons, but the most pressing one is that you’ll want to create a Virtual Environment before starting your script.\nA Python Virtual Environment, also referred to as venv(s), is there to help us decouple and isolate package installs and manage them, independently from pkgs that are provided by the system or used by other projects. Although not strictly necessary, it is a good practice that will save us a few headaches in the future.\nTo create a virtual environment, once you’ve opened a new blank project in VSC, click on shift + cmd + P (or F1) to open the Command Palette (it should display from the top of VSC). Type “Python” and select create python terminal. In the terminal, type `` python -m venv venv ’’ and that’s it! You’ll notice the appearance of a venv folder in your dedicated project folder.\nNow, you’ll need to install and load the libraries you’ll need for your project. To install them, use the python terminal you have opened. It should be displayed in the bottom of the working environment. Use the following (which should work with both Windows and Macbook, although Mac users can also use the general brew installer):\npip install pkg_name \n\n\n# Now, we can load the libraries\n\nimport numpy as np # can be used to perform a wide variety of mathematical operations on arrays.\nimport pandas as pd # mainly used for data analysis and associated manipulation of tabular data in DataFrames\nimport matplotlib as mpl # comprehensive library for creating static, animated, and interactive visualizations in Python\nimport sklearn as sk #  implement machine learning models and statistical modelling.\n\nWith some basic libraries open, let’s start with a brief introduction to Python programming. Like R, it is object oriented programming. But we call its elements differently. Python uses the following list of elements:\n\nliterals, variables, operators, delimiters, keywords, comments\n\n\n# Assigning VARIABLES with LITERALS\nan_integer = 4 # integer\na_float = 4.5 # numeric/float\na_boolean = False # boolean, true or false\na_string = \"Number of literals:\"\n\nprint(a_string , an_integer)\n\nNumber of literals: 4\n\n\nYou’ll notice that what we call an object in R, is a variable in Python. Likewise, an element in R is now a literal in Python. We’ll stop with the comparisons and proceed to work with Python language for now.\n\nIn Python, the data type is derived from the literal and does not have to be described explicitly.\nYou can overwrite a variable (i.e. an object) with new information.\n\ne.g.\n\nan_integer = 7\nprint(an_integer)\n\n7\n\nx = np.linspace(-2,2,41)\nprint(x)\n\n[-2.  -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.  -0.9 -0.8 -0.7\n -0.6 -0.5 -0.4 -0.3 -0.2 -0.1  0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7\n  0.8  0.9  1.   1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2. ]\n\n\nIf you’re working with a VSC script, you’ll want to select a snippet of the script and use shift + enter to only run that snippet (and not the whole code chunk).\nSome basics:\nArithmetic operators\n\n5 + 1 \n\n6\n\n5*3 \n\n15\n\n6 == 100 # returns FALSE\n\nFalse\n\n6 != 100 # returns TRUE\n\nTrue\n\n20 / 2\n\n10.0\n\n2 ** 3 # 2 to the power of 3, ** is used for exponentiation\n\n8\n\n\nKeywords and comments\n\n\na_var = \"This is a variable, which contains a string.\"\n#and this is a comment about the string variable\nprint(a_var)\n\n\n# lists: they are ordered arrays of elements, accesible via an index, e.g.\n\n\nmany_strings = [\"Tolkien\", \"Orwell\", \"Austen\"]\nmany_strings[0] # returns first element of list\n# note that python starts counting from zero\n\nmany_strings.append(\"Mephistopheles\")\nprint(many_strings) # we have succesfully appended an element to our list\n\nmany_strings.insert(2,\"Shrek\") # Insert element Shrek in position 3 of list (0,1,2)\nprint(many_strings)\n\n# finally, sort the list... \nmany_strings.sort()\nprint(many_strings)\n# python orders alphabetically :) \n\nLet’s use a toy dataset from the sklearn package to practice some basic estimations.\n\n # import sklearn dataset\nfrom sklearn.datasets import load_diabetes\n \n # load the  dataset, assign it to an object\n\ndiabetes = load_diabetes()\nprint(diabetes.DESCR) # DESCR is a description option from sklearn library. Be mindful of the Note!\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n:Number of Instances: 442\n\n:Number of Attributes: First 10 columns are numeric predictive values\n\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n:Attribute Information:\n    - age     age in years\n    - sex\n    - bmi     body mass index\n    - bp      average blood pressure\n    - s1      tc, total serum cholesterol\n    - s2      ldl, low-density lipoproteins\n    - s3      hdl, high-density lipoproteins\n    - s4      tch, total cholesterol / HDL\n    - s5      ltg, possibly log of serum triglycerides level\n    - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n\nWhilst there are many things you can do directly with the sklearn package options, the pandas library is the best for dataframe manipulation.\n\n# 1. convert the dataframe object into a pandas dataframe object\npandas_diabetes = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n\n# now we can use the handy describe() and info() options from pandas\npandas_diabetes.describe() # returns descriptive statistics of variables in dataframe (df)\n\n                age           sex  ...            s5            s6\ncount  4.420000e+02  4.420000e+02  ...  4.420000e+02  4.420000e+02\nmean  -2.511817e-19  1.230790e-17  ...  9.293722e-17  1.130318e-17\nstd    4.761905e-02  4.761905e-02  ...  4.761905e-02  4.761905e-02\nmin   -1.072256e-01 -4.464164e-02  ... -1.260971e-01 -1.377672e-01\n25%   -3.729927e-02 -4.464164e-02  ... -3.324559e-02 -3.317903e-02\n50%    5.383060e-03 -4.464164e-02  ... -1.947171e-03 -1.077698e-03\n75%    3.807591e-02  5.068012e-02  ...  3.243232e-02  2.791705e-02\nmax    1.107267e-01  5.068012e-02  ...  1.335973e-01  1.356118e-01\n\n[8 rows x 10 columns]\n\npandas_diabetes.info() # returns basic information about observations and variable names/types\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 442 entries, 0 to 441\nData columns (total 10 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   age     442 non-null    float64\n 1   sex     442 non-null    float64\n 2   bmi     442 non-null    float64\n 3   bp      442 non-null    float64\n 4   s1      442 non-null    float64\n 5   s2      442 non-null    float64\n 6   s3      442 non-null    float64\n 7   s4      442 non-null    float64\n 8   s5      442 non-null    float64\n 9   s6      442 non-null    float64\ndtypes: float64(10)\nmemory usage: 34.7 KB\n\nlen(pandas_diabetes) # returns the length of the object (aka the number of observations in df)\n\n442\n\npandas_diabetes.head() # print the first 4 observations of the variables (glimpse of your df)\n\n        age       sex       bmi  ...        s4        s5        s6\n0  0.038076  0.050680  0.061696  ... -0.002592  0.019907 -0.017646\n1 -0.001882 -0.044642 -0.051474  ... -0.039493 -0.068332 -0.092204\n2  0.085299  0.050680  0.044451  ... -0.002592  0.002861 -0.025930\n3 -0.089063 -0.044642 -0.011595  ...  0.034309  0.022688 -0.009362\n4  0.005383 -0.044642 -0.036385  ... -0.002592 -0.031988 -0.046641\n\n[5 rows x 10 columns]\n\n\nIf you’re working with VSC and you’d like to get a nice display of your data:\ninstall the pkg ‘tabulate’ (pip install tabulate) using the terminal. You may have to open a new bash terminal (that is not using the Python REPL) to install packages. If that is the case (i.e. you could not pip install in the terminal you are using), you can ‘split’ the terminal using the interface split prompt.\n\n# this prints the dataframe nicely (but remember to only print the head/first observations, otherwise you display a large table!)\nprint(pandas_diabetes.head().to_markdown())\n\n|    |         age |        sex |        bmi |          bp |          s1 |         s2 |          s3 |          s4 |          s5 |          s6 |\n|---:|------------:|-----------:|-----------:|------------:|------------:|-----------:|------------:|------------:|------------:|------------:|\n|  0 |  0.0380759  |  0.0506801 |  0.0616962 |  0.0218724  | -0.0442235  | -0.0348208 | -0.0434008  | -0.00259226 |  0.0199075  | -0.0176461  |\n|  1 | -0.00188202 | -0.0446416 | -0.0514741 | -0.0263275  | -0.00844872 | -0.0191633 |  0.0744116  | -0.0394934  | -0.0683315  | -0.092204   |\n|  2 |  0.0852989  |  0.0506801 |  0.0444512 | -0.00567042 | -0.0455995  | -0.0341945 | -0.0323559  | -0.00259226 |  0.00286131 | -0.0259303  |\n|  3 | -0.0890629  | -0.0446416 | -0.011595  | -0.0366561  |  0.0121906  |  0.0249906 | -0.0360376  |  0.0343089  |  0.0226877  | -0.00936191 |\n|  4 |  0.00538306 | -0.0446416 | -0.0363847 |  0.0218724  |  0.00393485 |  0.0155961 |  0.00814208 | -0.00259226 | -0.0319876  | -0.0466409  |\n\n\nSome basic operations and estimation of a linear model:\nIndexig with pandas is done using the .iloc[] call. To estimate a linear model (OLS), you’ll need to split your data into a dependent Y and independent X variables.\nLuckily for us, because the diabetes toy dataframe is from the sklearn library, you can use the target and data functions. For the same reason, we will use the diabetes dataset and not the pandas-converted dataset.\n\npandas_diabetes.iloc[:,1] # print first column: Name: sex, Length: 442, dtype: float64\n\n0      0.050680\n1     -0.044642\n2      0.050680\n3     -0.044642\n4     -0.044642\n         ...   \n437    0.050680\n438    0.050680\n439    0.050680\n440   -0.044642\n441   -0.044642\nName: sex, Length: 442, dtype: float64\n\npandas_diabetes.iloc[:,1].mean() # get the mean of the first column: 1.2307902309192911e-17\n\nnp.float64(1.2307902309192911e-17)\n\n# Linear Model (OLS), split data into dep Y and indep X variables\n\nY = diabetes.target # define targer/ Y var\nX = diabetes.data # all remaining variables in the dataset are now X covariates\n\nprint(Y.shape, X.shape) # y is one column with 442 observations, X is 10 columns with 442 observations\n\n(442,) (442, 10)\n\nprint(Y) # prints all the vaues of Y\n\n[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n  49.  64.  48. 178. 104. 132. 220.  57.]\n\n\nTo estimate a linear model, we’ll need t load the function from the sklearn library.\n\nfrom sklearn.linear_model import LinearRegression\n\nlm = LinearRegression(fit_intercept=True).fit(X, Y)\n\n#print model coefficients using pandas\n\nprint(lm.coef_) # displayed in the order of the column names\n\n[ -10.0098663  -239.81564367  519.84592005  324.3846455  -792.17563855\n  476.73902101  101.04326794  177.06323767  751.27369956   67.62669218]\n\nprint(lm.intercept_) # model intercept = 152.133\n\n152.13348416289597\n\n# in a nicer way\ncolumn_names = pandas_diabetes.columns # to retrieve column names we should use the pandas dataset\ncoefficients = pd.concat([pd.DataFrame(column_names),pd.DataFrame(np.transpose(lm.coef_))], axis = 1)\nprint(coefficients) # displays variable name and it's estimated coefficient. \n\n     0           0\n0  age  -10.009866\n1  sex -239.815644\n2  bmi  519.845920\n3   bp  324.384646\n4   s1 -792.175639\n5   s2  476.739021\n6   s3  101.043268\n7   s4  177.063238\n8   s5  751.273700\n9   s6   67.626692\n\n\nThe sklearn library will be our trusted Machine Learning ally. However, for general statistical analysis and easier to interpret output, there are other libraries. E.g.\n\n# an ever more intuitive output would look like this -&gt;\n# remember to pip install the libraries before importing them! And to do so in a different terminal (open a new bash terminal)\nimport statsmodels.api as sm\nfrom scipy import stats\n\nconstant = sm.add_constant(X) # create a new X (covariate) object that includes all our variables and allow for this object to also display an estimated constant\nest = sm.OLS(Y, constant) # estimate a linear regression model with dep Y and indep Xs (now constant)\nlm_2 = est.fit() # create an object that contains all the models' parameters\nprint(lm_2.summary()) # print linear model parameters\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.518\nModel:                            OLS   Adj. R-squared:                  0.507\nMethod:                 Least Squares   F-statistic:                     46.27\nDate:                Fri, 28 Mar 2025   Prob (F-statistic):           3.83e-62\nTime:                        13:36:42   Log-Likelihood:                -2386.0\nNo. Observations:                 442   AIC:                             4794.\nDf Residuals:                     431   BIC:                             4839.\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.1335      2.576     59.061      0.000     147.071     157.196\nx1           -10.0099     59.749     -0.168      0.867    -127.446     107.426\nx2          -239.8156     61.222     -3.917      0.000    -360.147    -119.484\nx3           519.8459     66.533      7.813      0.000     389.076     650.616\nx4           324.3846     65.422      4.958      0.000     195.799     452.970\nx5          -792.1756    416.680     -1.901      0.058   -1611.153      26.802\nx6           476.7390    339.030      1.406      0.160    -189.620    1143.098\nx7           101.0433    212.531      0.475      0.635    -316.684     518.770\nx8           177.0632    161.476      1.097      0.273    -140.315     494.441\nx9           751.2737    171.900      4.370      0.000     413.407    1089.140\nx10           67.6267     65.984      1.025      0.306     -62.064     197.318\n==============================================================================\nOmnibus:                        1.506   Durbin-Watson:                   2.029\nProb(Omnibus):                  0.471   Jarque-Bera (JB):                1.404\nSkew:                           0.017   Prob(JB):                        0.496\nKurtosis:                       2.726   Cond. No.                         227.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThis displayed output contains all the information we may need to interpret the suitability of our linear model.\nWe’re at the end of this brief intro to Python. Notice that there’s a forward structure throughout. We create an object X, which we define to be of class, say, pandas. It’s class will let us use that class/library’s functions by calling them after a dot: X.summary() will display a summary of the X object. ** Visualisation with matplotlib **\n\nimport matplotlib.pyplot as plt\n\npandas_diabetes.head() # to remember our variables\n\n        age       sex       bmi  ...        s4        s5        s6\n0  0.038076  0.050680  0.061696  ... -0.002592  0.019907 -0.017646\n1 -0.001882 -0.044642 -0.051474  ... -0.039493 -0.068332 -0.092204\n2  0.085299  0.050680  0.044451  ... -0.002592  0.002861 -0.025930\n3 -0.089063 -0.044642 -0.011595  ...  0.034309  0.022688 -0.009362\n4  0.005383 -0.044642 -0.036385  ... -0.002592 -0.031988 -0.046641\n\n[5 rows x 10 columns]\n\nbmi = pandas_diabetes.iloc[:,2] # select and store bmi variable\nage = pandas_diabetes.iloc[:,0] # select and store age variable\n\nplt.hist(bmi)\nplt.show() \n\n\n\n\n\n\n\nplt.clf()\n\nplt.scatter(age,bmi) # order is x,y\nplt.show()\n\n\n\n\n\n\n\n# let's add some elements#add line of best fit to plot\n\n# find line of best fit\na, b = np.polyfit(age, bmi, 1)\n\n# add the points to the plot\nplt.scatter(age,bmi) # order is x,y\n# add the line and then display\nplt.plot(age, a*age+b, color='red', linestyle='--', linewidth=2)\nplt.show()\n\n\n\n\n\n\n\n\nDefining functions and loops\nThe last useful tip before we begin to use Python in earnest, is to remember that we can define functions or use loops (general flow control) to help us with our data wrangling:\n\n# for loops, to iterate over some length and repeat an operation, e.g.\ncount = 0 # initialize empty variable\nnums = [1,2,3,4,5,6,7,8,9,10]\n\nfor i in nums:\n    count += i\n    \nprint(f\"We have a total sum of: {count}.\")\n\nWe have a total sum of: 55.\n\n# we've essentially set a counter: 1 + 2 + 3 + 4  + ... 10 = 55!\n\nif 101 == 100: \n    print(\"Hello, 5!\")\nelif 101 &gt; 100:\n    print(\"Much gold!\")\n\nMuch gold!\n\n# if logical statement is true, print(), else if new logical statement is true, print()\n\n# defining functions: lottery draw!\n\n\ndef French_talk(name,age):\n    print(\"Je m'appelle\", name)\n    print(\"Mon âge est\", age)\n    \nFrench_talk(\"Saruman\",1000)\n\nJe m'appelle Saruman\nMon âge est 1000\n\n\nYou can download the R script by clicking on the button below.\n Download Intro to Python script \n\n\n\n\nCopyright © 2025 Michelle González Amador & Stephan Dietrich. All rights reserved."
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification with Logistic Regression",
    "section": "",
    "text": "This section will cover:\nHave you heard the English proverb, “Birds of a feather flock together”? It references and old saying that indicates that people with similar characteristics tend to group and stay together. In Machine Learning, Classification problems deal with the evaluation of models of categorical response, such as:\nIn the video-lecture below you’ll get an intuitive explanation of what a logistic regression model is, and how we can use it in the context of a prediction policy framework.\nAfter watching the video, below you’ll find a continuation of our previous exercise. Previously, we were working on predicting per capita monthly expenditures of a sample of individuals from Malawi. Our assumption is that by predicting how much a person spends per month, we can infer whether they are in poverty (or not) by contrasting that value to other relevant information, such as the cost of food and rent in the country. Another way to go about this is to use the estimated poverty line, and generate a variable that takes on the value \\(1\\) if the person’s expenditure is below the poverty line (they are poor) and \\(0\\) otherwise (not poor). Thus, our policy problem becomes one of classification."
  },
  {
    "objectID": "classification.html#practical-example",
    "href": "classification.html#practical-example",
    "title": "Classification with Logistic Regression",
    "section": "Practical Example",
    "text": "Practical Example\nWe will continue to work with the Malawi dataset, which can be downloaded in the (Prediction Policy Problems)[predictionpolicy.qmd] tab of this website.\n\n1. Preliminaries: working directory, libraries, data upload\n\nrm(list = ls()) # this line cleans your Global Environment.\n#setwd(\"/Users/michellegonzalez/Documents/GitHub/Machine-Learning-for-Public-Policy\") # set your working directory\n\n# Do not forget to install a package with the install.packages() function if it's the first time you use it!\n# install.packages(caTools, plotROC) # these guys are new for us \n\nlibrary(dplyr) # core package for dataframe manipulation. Usually installed and loaded with the tidyverse, but sometimes needs to be loaded in conjunction to avoid warnings.\nlibrary(tidyverse) # a large collection of packages for data manipulation and visualisation.  \nlibrary(caret) # a library with key functions that streamline the process for predictive modelling \nlibrary(skimr) # a package with a set of functions to describe dataframes and more\nlibrary(plyr) # a package for data wrangling\nlibrary(caTools) # a library with several basic utility functions (e.g. ROC curves, LogitBoos classifier, etc.)\nlibrary(plotROC) # a companion to ggplot2 (loaded with the tidyverse) for plotting ROC curves\n\ndata_malawi &lt;- read_csv(\"data/malawi.csv\") # the file is directly read from the working directory/folder previously set\n\n\n\n2. Data pre-processing\nThis section will not be a thorough step-by-step of the pre-processing and visualisation of our data because we have already done that. However, we have to do something very important: recover a static variable from the original dataset that contains a single number: the poverty line in Malawi.\n\nFeature selection: subsetting the dataset\nThe variable that we’re interested in recovering is lnzline. The code below reproduces the dataframe subsetting from our previous exercise. Except, this time we will NOT delete de static vector lnzline.\n\n# object:vector that contains the names of the variables that we want to get rid of (notice this time lnzline is still there)\ncols &lt;- c(\"ea\", \"EA\", \"psu\",\"hhwght\", \"strataid\", \"case_id\",\"eatype\")\n\n\n# subset of the data_malawi object:datframe\ndata_malawi &lt;- data_malawi[,-which(colnames(data_malawi) %in% cols)] # the minus sign indicates deletion of cols\n\ncolnames(data_malawi) # print the names of the remaining vectors in our dataframe\n\n [1] \"lnexp_pc_month\" \"hhsize\"         \"hhsize2\"        \"agehead\"       \n [5] \"agehead2\"       \"north\"          \"central\"        \"rural\"         \n [9] \"nevermarried\"   \"sharenoedu\"     \"shareread\"      \"nrooms\"        \n[13] \"floor_cement\"   \"electricity\"    \"flushtoilet\"    \"soap\"          \n[17] \"bed\"            \"bike\"           \"musicplayer\"    \"coffeetable\"   \n[21] \"iron\"           \"dimbagarden\"    \"goats\"          \"dependratio\"   \n[25] \"hfem\"           \"grassroof\"      \"mortarpestle\"   \"table\"         \n[29] \"clock\"          \"region\"         \"lnzline\"       \n\n\nAt this point, we still need to do two more pre-processing steps: correctly define the vector/variable class in the dataframe, and create the binary outcome/target variable. We will repeat the class-transformation code chunk below so that you have all that is needed in one section. However, we won’t spend time explaining it in detail as that was done in the previous session.\n\n# transform all binary/categorical data into factor class\n\nmin_count &lt;- 3 # vector: 3 categories is our max number of categories found\n\n# store boolean (true/false) if the number of unique values is lower or equal to the min_count vector\nn_distinct2 &lt;- apply(data_malawi, 2, function(x) length(unique(x))) &lt;= min_count\n\n# select the identified categorical variables and transform them into factors\ndata_malawi[n_distinct2] &lt;- lapply(data_malawi[n_distinct2], factor) \n\n# recall poverty line contains 1 unique value (it is static), let's transform the variable into numeric again\ndata_malawi$lnzline &lt;- as.numeric(as.character(data_malawi$lnzline))\n\n# you can use ``skim(data_malawi)'' to check that the dataframe is in working order\n\n\n\nFeature creation: create a binary variable\n\n# print summary statistics of target variable\nsummary(data_malawi$lnexp_pc_month)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.777   6.893   7.305   7.359   7.758  11.064 \n\n# if the log of per capita expenditure is below the estimated poverty line, classify individual as poor, else classify individual as not poor. Store as factor (default with text is class character)\ndata_malawi$poor &lt;- as.factor(ifelse(data_malawi$lnexp_pc_month&lt;= data_malawi$lnzline,\"Y\",\"N\")) # Y(es) N(o)\n\n# make sure that the factor target variable has poor = Y as reference category (this step is important when running the logistic regression)\ndata_malawi$poor &lt;- relevel(data_malawi$poor, ref=\"Y\") # make Y reference category\n\n\n# print a proportions table to get a first impression of the state of poverty in Malawi\nprop.table(table(data_malawi$poor))\n\n\n   Y    N \n0.65 0.35 \n\n\nAccording to our sample, about 65% of Malawians are considered poor. This number is not unreasonable. According to The World Bank’s Country Report for Malawi, ca. \\(70\\%\\) of the population lives with under \\(\\$2.15\\) a day, and the poverty rate is estimated to be at \\(50\\%\\). About half of their population is labelled as poor. These estimates were done with \\(2019\\) data (so, a bit more recent than our sample).\n\n# Final data pre-processing: delete static variable (poverty line)\n# and along with it: remove the continuous target (as it perfectly predicts the binary target) \n\nwhich(colnames(data_malawi)==\"lnzline\") # returns column number 31\n\n[1] 31\n\nwhich(colnames(data_malawi)==\"lnexp_pc_month\") # returns column number 1\n\n[1] 1\n\ndata_malawi &lt;- data_malawi[,-c(1,31)] # delete columns no. 1 and 31 from the dataset\n\n\n\n\n3. Model Validation\nLet’s use a simple 80:20 split of our data. We will use the caret package again.\n\nset.seed(1234) # ensures reproducibility of our data split\n\n# data partitioning: train and test datasets\ntrain_idx &lt;- createDataPartition(data_malawi$poor, p = .8, list = FALSE, times = 1) \n\nTrain_df &lt;- data_malawi[ train_idx,]\nTest_df  &lt;- data_malawi[-train_idx,]\n\nNow, let’s fit a logistic model:\n\n# Step 1: create trainControl object\nTrControl &lt;- trainControl(\n    method = \"cv\",\n    number = 5,\n    summaryFunction = twoClassSummary,\n    classProbs = TRUE, # IMPORTANT!\n    verboseIter = FALSE\n)\n\nWe’re going to pass the TrControl object onto the caret model estimation to ask for the following:\n\ncross-validate with 5 folds\nshow model summary: performance metrics for when we have two distinct classes (binary outcome), including the area under the ROC curve, the sensitivity and specificity.\nthe ROC curve is based on the predicted class probabilities, so the classProbs = TRUE parameter must accompany a twoClassSummary setup.\nveboseIter = TRUE shows you the output for each iteration (but we don’t want to display all the details atm).\n\n\n# Step 2: train the model.\nset.seed(12345)\nm &lt;- train(\n    poor ~ ., \n    Train_df, \n    method = \"glm\",\n    family=\"binomial\",\n    trControl = TrControl,\n    preProcess=c(\"center\", \"scale\")\n)\n\nWarning in train.default(x, y, weights = w, ...): The metric \"Accuracy\" was not\nin the result set. ROC will be used instead.\n\n\nNotice the warning. If we want to report the “Accuracy” metric, we should remove the twoClassSummary parameter specification in the TrControl object.\n\n# print the model's performance metrics\nprint(m) \n\nGeneralized Linear Model \n\n9025 samples\n  29 predictor\n   2 classes: 'Y', 'N' \n\nPre-processing: centered (30), scaled (30) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 7219, 7220, 7220, 7221, 7220 \nResampling results:\n\n  ROC        Sens       Spec     \n  0.8825702  0.8943044  0.6774223\n\n\n\nPerformance metrics (on the train set!)\n\nROC: it is a probability curve plotted with the True Positive Rate (y-axis) against the False Positive Rate (x-axis); you can think of it as plotting the tradeoff between maximising the true positive rate and minimising the false positive rate. The preferred area under the curve is \\(1\\). Our estimated \\(0.88\\) score indicates that a logistic classification is a good fit for our data (close to \\(1\\)).\nSensitivity: it is a measure of the proportion of the positive (\\(1\\) = poor) values that are correctly identified. Therefore, we have correctly identified \\(89\\%\\) of the actual positives. The formula is: \\(\\frac{tp}{tp + fn}\\); where tp = true positive and fn = false negative. In the video-lecture, Stephan used the term Recall, where we now use sensitivity. This means that our model does pretty well at predicting/identifying people living below the poverty line in Malawi!\nSpecificity: measures the proportion of actual negatives that are correctly identified by the model; i.e. the ability of our model to predict if an observation doesn’t belong to a certain category. The formula is: \\(\\frac{tn}{tn + fp}\\); where tn = true negative and fp = false positive. At \\(67\\%\\), we can trust a predicted negative (\\(0\\)) value to be real more than half the time. Our model is not as good at predicting who doesn’t live below the poverty line in Malawi.\n\nThe performance metrics we have interpreted above are based on the training dataset only. We are interested in our model’s ability to make out-of-sample predictions. Therefore, we will use the definitions above, but to take the scores on the test-dataset predictions to make our final evaluation.\n\n\nOut-of-sample performance\nNotice that we have used cross-validation in our training dataset. In theory, our performance metrics have been validated in 5 different folds. To a certain extent, that means that our performance metrics above did reflect the model’s ability to extrapolate. Nevertheless, we will still see how our trained model performs in our test dataset. You can think of this step as predicting on a sixth fold. We know that the performance of a logistic classification model on the train set is relatively good, is it the same for the test dataset?\n\n# First, use the logistic classification model (trained on the Train_df) to make predictions on the test dataset:\n\nset.seed(12345)\npr1 &lt;- predict(m, Test_df, type = \"raw\")\nhead(pr1) # Yes and No output\n\n[1] N Y Y Y Y Y\nLevels: Y N\n\n\nWe have specified the type of prediction we want: raw. This will return the predicted classification (\\(0\\) or \\(1\\)) as opposed to the individual’s probability of falling into the selected category \\(1\\) (or the estimated probability of being poor). There is a rule of thumb that says you will be categorised as poor (or any chosen category) if your estimated probability is &gt;= to \\(0.5\\). With this information, we can create a Confusion Matrix which will be accompanied by performance metrics.\n\n# Next, we call the caret package's confusionMatrix function, and select the two elements to be contrasted:\n# the predicted classification vector, and the actual observed vector from the test dataframe. \nconfusionMatrix(pr1, Test_df[[\"poor\"]], positive = \"Y\") # positive = \"Y\" indicates that our category of interest is Y (1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    Y    N\n         Y 1294  270\n         N  172  519\n                                         \n               Accuracy : 0.804          \n                 95% CI : (0.787, 0.8202)\n    No Information Rate : 0.6501         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.5564         \n                                         \n Mcnemar's Test P-Value : 3.953e-06      \n                                         \n            Sensitivity : 0.8827         \n            Specificity : 0.6578         \n         Pos Pred Value : 0.8274         \n         Neg Pred Value : 0.7511         \n             Prevalence : 0.6501         \n         Detection Rate : 0.5738         \n   Detection Prevalence : 0.6936         \n      Balanced Accuracy : 0.7702         \n                                         \n       'Positive' Class : Y              \n                                         \n\n\nThe first element from the above function returns the confusion matrix, a 2×2 table that shows the predicted values from the model vs. the actual values from the test dataset. You may be acquainted with this sort of table, but know it as a cross-tabulation. From the confusion matrix, we obtain the information that we need to estimate some performance metrics. If you need a reminder of what each cell in the 2x2 matrix represents, recall that the structure of our target variable is [\\(Y(1),N(0)\\)]. Therefore, the first cell would be the intersection of Predicted \\(Y\\) vs Observed \\(Y\\) (or True Positive) = \\(1294\\), the fourth cell would be the intersection of Predicted \\(N\\) vs Observed \\(N\\) (or True Negative) = \\(519\\). These guys are the predictions that have hit the mark! On the other hand, the second cell would be the intersection of Predicted \\(Y\\) vs Observed \\(N\\) (or False Positive) = \\(270\\), and the third cell Predicted \\(N\\) vs Observed \\(Y\\) (or False Negative) = \\(172\\). These were incorrect predictions. We use these counts (true positives, true negatives, false positives, false negatives) to estimate performance metrics\nBesides the performance metrics discussed previously, this function also shows the Accuracy of our model (or \\(1\\) - the error rate) which, at \\(0.8\\), indicates that our classification algorithm is highly accurate.\nImbalanced data\nWhen you have a large number of zeros (or No, in this case), the Accuracy metric may not be the most reliable one. If we look at the formula: number of correct predictions / total number of predictions, we see why this might be an issue. It is a lot easier to correctly predict that of which there is plenty of (Yes), than the category for which we have less instances.\nImbalance is not a problem for our target variable, as we have roughly as many zeros as ones. In fact, we have more Yes (1) responses. Nonetheless, this sets the stage for us to introduce the Kappa statistic (\\(0.55\\)), which is a measure of model accuracy that is adjusted by accounting for the possibility of a correct prediction by chance alone. It ranges from 0 to 1, and can be interpreted using the following thresholds:\n\nPoor = Less than 0.20\nFair = 0.20 to 0.40\nModerate = 0.40 to 0.60\nGood = 0.60 to 0.80\nVery good = 0.80 to 1.00\n\nAt \\(0.55\\), our classification model performs moderately well. Finally, Sensitivity and Specificity scores on the test dataset are very close to the ones obtained from the train dataset. This is a good sign for the out-of-sample stability of our model.\n\n\n\nModel Visualisation\nWe can visualise the performance of our classification model in various ways. For now, we’ll focus on a simple ROC AUC.\n\n# ROC AUC: Area Under the Curve\n# colAUC function from the caTools library\n\n# transform predicted values and observed values into class numeric (needed for the colAUC function)\n\npr_numeric &lt;- as.numeric(as.factor(pr1))\n# sanity check:\nhead(cbind(pr_numeric, pr1)) # the numeric values of both vectors seem to be the same. \n\n     pr_numeric pr1\n[1,]          2   2\n[2,]          1   1\n[3,]          1   1\n[4,]          1   1\n[5,]          1   1\n[6,]          1   1\n\npoor_numeric &lt;- as.numeric(as.factor(Test_df$poor))\n# sanity check\nhead(cbind(poor_numeric,Test_df$poor)) # all good \n\n     poor_numeric  \n[1,]            1 1\n[2,]            1 1\n[3,]            1 1\n[4,]            1 1\n[5,]            1 1\n[6,]            1 1\n\n# plot the ROC area under the curve\ncolAUC(pr_numeric, poor_numeric, plotROC = TRUE)\n\n\n\n\n\n\n\n\n             [,1]\n1 vs. 2 0.7702343\n\n# We can also plot the ROC AUC with ggplot\n# First, we create a dataframe containing the observed and the predicted values (in numeric form)\nroc_df &lt;- data.frame(Observed = poor_numeric, Predicted = pr_numeric)\n\n# Second, we add the geom_roc() layer to a ggplot object\n\nroc_gg &lt;- ggplot(roc_df, aes (d = Observed, m = Predicted)) + \n            geom_roc(labels = FALSE, color='orange') +\n            style_roc(theme = theme_bw, guide = TRUE) # guide=TRUE adds a diagonal guideline, style_roc() adds minor grid lines, and optionally direct labels to ggplot objects containing a geom_roc layer\n\ndirect_label(roc_gg, nudge_y = 0.2) # direct_label tells you what the plotted line represents, nudge_y option places the label (you can play around with that number to see where different values place the label)\n\nWarning in verify_d(data$d): D not labeled 0/1, assuming 1 = 0 and 2 = 1!\nWarning in verify_d(data$d): D not labeled 0/1, assuming 1 = 0 and 2 = 1!\n\n\n\n\n\n\n\n\n\nNotice the warning on the ggplot2 ROC AUC plot. The assumption that they are making is correct, so we do not need to do anything else at this moment. You can check this by contrasting the values of the labelled vs. the numeric vectors (use the head() function)."
  },
  {
    "objectID": "classification.html#conclusion",
    "href": "classification.html#conclusion",
    "title": "Classification with Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nHow did our classification model do? Is a logistic regression the right algorithm? I trust you can form your own judgement based on the performance metrics above. Personally, I think we have improved from a linear regression, but perhaps we can do better with Ensemble Learning techniques!"
  },
  {
    "objectID": "classification.html#practice-at-home",
    "href": "classification.html#practice-at-home",
    "title": "Classification with Logistic Regression",
    "section": "Practice at home",
    "text": "Practice at home\nAs before, you can use the Bolivia dataset to try your hand at a logistic model. The data can be downloaded in the Practice at home section in the Prediction Policy Problems webpage of this site. \nLet us know you are completing the tasks by answering a simple question via this Qualtrics link. \nIt is important for us to know that you are engaging with the course. Sending an answer - even if it is incorrect - to our Qualtrics question lets us know just that. So please say hi on Qualtrics! (And on our blog, too.)"
  },
  {
    "objectID": "classification.html#readings",
    "href": "classification.html#readings",
    "title": "Classification with Logistic Regression",
    "section": "Readings",
    "text": "Readings\nMandatory Readings\n\nAn introduction to Statistical learning Chapter 4\nAthey, S., & Imbens, G. W. (2019). Machine learning methods that economists should know about. Annual Review of Economics, 11, 685-725.\nMcBride, L., & Nichols, A. (2018). Retooling poverty targeting using out-of-sample validation and machine learning. The World Bank Economic Review, 32(3), 531-550.pter 5.1\n\nOptional Readings\n\nBondi-Kelly et al. (2023)- Predicting micronutrient deficiency with publicly available satellite data. In AI Magazine.\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "fairml.html",
    "href": "fairml.html",
    "title": "Algorithmic Fairness",
    "section": "",
    "text": "This section will cover:\nMachine Learning promises to be an important tool for Policymakers who wish to improve their response to challenges such as efficient resource allocation, or timely and effective reaction to crises. However, privacy and fairness issues arise in the use (and misuse) of Machine Learning algorithms.\nIn this video lecture, Dr. Juba Ziani, Assistant Professor at Georgia Tech will give us an overview of the more common ethical dilemmas that arise in machine learning, and practical examples in the public sphere where these issues have had a regressive impact in society. The video also includes some ways in which we (data scientists, machine learning enthusiasts, and future policymakers) can minimise these biases and avoid negative impacts from ML in the policy decision-making process. Some of his recommendations to delve deeper into this topic include:\nThis lecture does not come with an applied R or Python exercise, but we do ask that you think about the different sources of bias and how they may come up in your (personal) research."
  },
  {
    "objectID": "fairml.html#readings",
    "href": "fairml.html#readings",
    "title": "Algorithmic Fairness",
    "section": "Readings",
    "text": "Readings\n\nFast AI: Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD, Chapter 4.\nKasy, M., & Abebe, R. (2021, March). Fairness, equality, and power in algorithmic decision-making. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 576-586).\nFairness and Machine Learning: Limitations and Opportunities, Chapter 4."
  },
  {
    "objectID": "predictionpolicy.html",
    "href": "predictionpolicy.html",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "",
    "text": "This section will cover:"
  },
  {
    "objectID": "predictionpolicy.html#introducing-the-prediction-policy-framework",
    "href": "predictionpolicy.html#introducing-the-prediction-policy-framework",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "Introducing the Prediction Policy Framework",
    "text": "Introducing the Prediction Policy Framework\nIn the video-lecture below you’ll be given a brief introduction to the prediction policy framework, and a primer on machine learning. Please take a moment to watch the 20 minute video.\n\nAre you still wondering what the difference is between Machine Learning and Econometrics? Take a few minutes to watch the video below.\n\nAfter watching the videos, we have a practical exercise.\n\nPredicting social assistance beneficiaries\nA key problem in the design of Social Policies is the identification of people in need of social assistance. Social policies usually work with tight budgets and limited fiscal space. To allocate resources efficiently, benefits need to be targeted to those who need them most. Yet, identifying needs isn’t easy and misclassifications can have severe and irreversible effects on people in need.\nThink of a social protection programme that allocates food vouchers to families with children at risk of malnutrition, or a programme that establishes needs-based school grants. What happens when these limited and finite resources are given to people that could do without, and those who need them most are excluded from them?\nIn this block we’ll work with real-world data from the country of Malawi to predict cash-transfer programme beneficiaries: People who live in poverty and need government assistance to make ends meet. The data comes from McBride and Nichol’s (2018) paper Retooling poverty targeting using out-of-sample validation and machine learning.\nDiscussion Points\nThe points below are meant to help you think critically about why we’re about to embark on a machine learning - targeting exercise. \n\n\nWhy is this a prediction policy problem? What would be a causal inference problem in this setting? Is it a regression or a classification problem?\n\n\nWhich variables and characteristics that we include in the prediction model can make a big difference?\n\n\nProgrammatically and conceptually, which type of characteristics do we want to consider for the prediction model?\n\n\nTechnically, how do we select which variables to include in a prediction model? How is this different from a causal inference problem?\n\n\n\n\nWhat are the practical implications of the bias-variance tradeoff in this application?\n\n\nWhat are potential risks of such a data driven targeting approach?\n\n\n\nIf you’d like to learn more about Social Protection Policies, take a look at this video Alex has made for us with a brilliant summary of the field\n(Yes, more videos!)"
  },
  {
    "objectID": "predictionpolicy.html#r-practical",
    "href": "predictionpolicy.html#r-practical",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "R Practical",
    "text": "R Practical\nYou can download the dataset by clicking on the button below.\n Download Malawi.csv \n\nThe script below is a step by step on how to go about coding a predictive model using a linear regression. Despite its simplicity and transparency, i.e. the ease with which we can interpret its results, a linear model is not without challenges in machine learning.\n\n1. Preliminaries: working directory, libraries, data upload\n\nrm(list = ls()) # this line cleans your Global Environment.\nsetwd(\"/Users/lucas/Documents/UNU-CDO/courses/ml4p/ml4p-website-v2\") # set your working directory\n\n# Libraries\n\n# If this is your first time using R, you need to install the libraries before loading them. \n# To do that, you can uncomment the line that starts with install.packages(...) by removing the # symbol.    \n\n#install.packages(\"dplyr\", \"tidyverse\", \"caret\", \"corrplot\", \"Hmisc\", \"modelsummary\", \"plyr\", \"gt\", \"stargazer\", \"elasticnet\", \"sandwich\")\n\nlibrary(dplyr) # core package for dataframe manipulation. Usually installed and loaded with the tidyverse, but sometimes needs to be loaded in conjunction to avoid warnings.\nlibrary(tidyverse) # a large collection of packages for data manipulation and visualisation.  \nlibrary(caret) # a package with key functions that streamline the process for predictive modelling \nlibrary(corrplot) # a package to plot correlation matrices\nlibrary(Hmisc) # a package for general-purpose data analysis \nlibrary(modelsummary) # a package to describe model outputs\nlibrary(skimr) # a package to describe dataframes\nlibrary(plyr) # a package for data wrangling\nlibrary(gt) # a package to edit modelsummary (and other) tables\nlibrary(stargazer) # a package to visualise model output\n\ndata_malawi &lt;- read_csv(\"data/malawi.csv\") # the file is directly read from the working directory/folder previously set\n\n\n\n2. Get to know your data: visualisation and pre-processing\n\nskim(data_malawi) # describes the dataset in a nice format \n\n\nData summary\n\n\nName\ndata_malawi\n\n\nNumber of rows\n11280\n\n\nNumber of columns\n38\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n36\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nregion\n0\n1\n5\n6\n0\n3\n0\n\n\neatype\n0\n1\n5\n17\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlnexp_pc_month\n0\n1\n7.360000e+00\n6.800000e-01\n4.7800e+00\n6.890000e+00\n7.310000e+00\n7.760000e+00\n1.106000e+01\n▁▇▇▁▁\n\n\nhhsize\n0\n1\n4.550000e+00\n2.340000e+00\n1.0000e+00\n3.000000e+00\n4.000000e+00\n6.000000e+00\n2.700000e+01\n▇▂▁▁▁\n\n\nhhsize2\n0\n1\n2.613000e+01\n2.799000e+01\n1.0000e+00\n9.000000e+00\n1.600000e+01\n3.600000e+01\n7.290000e+02\n▇▁▁▁▁\n\n\nagehead\n0\n1\n4.246000e+01\n1.636000e+01\n1.0000e+01\n2.900000e+01\n3.900000e+01\n5.400000e+01\n1.040000e+02\n▅▇▅▂▁\n\n\nagehead2\n0\n1\n2.070610e+03\n1.618600e+03\n1.0000e+02\n8.410000e+02\n1.521000e+03\n2.916000e+03\n1.081600e+04\n▇▃▁▁▁\n\n\nnorth\n0\n1\n1.500000e-01\n3.600000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ncentral\n0\n1\n3.800000e-01\n4.900000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▅\n\n\nrural\n0\n1\n8.700000e-01\n3.300000e-01\n0.0000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n▁▁▁▁▇\n\n\nnevermarried\n0\n1\n3.000000e-02\n1.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nsharenoedu\n0\n1\n1.700000e-01\n2.600000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n2.500000e-01\n1.000000e+00\n▇▂▁▁▁\n\n\nshareread\n0\n1\n6.100000e-01\n3.800000e-01\n0.0000e+00\n3.300000e-01\n6.700000e-01\n1.000000e+00\n1.000000e+00\n▅▁▅▂▇\n\n\nnrooms\n0\n1\n2.500000e+00\n1.300000e+00\n0.0000e+00\n2.000000e+00\n2.000000e+00\n3.000000e+00\n1.600000e+01\n▇▂▁▁▁\n\n\nfloor_cement\n0\n1\n2.000000e-01\n4.000000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\nelectricity\n0\n1\n6.000000e-02\n2.300000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nflushtoilet\n0\n1\n3.000000e-02\n1.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nsoap\n0\n1\n1.400000e-01\n3.400000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nbed\n0\n1\n3.200000e-01\n4.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▃\n\n\nbike\n0\n1\n3.600000e-01\n4.800000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▅\n\n\nmusicplayer\n0\n1\n1.600000e-01\n3.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ncoffeetable\n0\n1\n1.200000e-01\n3.200000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\niron\n0\n1\n2.100000e-01\n4.000000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ndimbagarden\n0\n1\n3.200000e-01\n4.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▃\n\n\ngoats\n0\n1\n2.100000e-01\n4.100000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ndependratio\n0\n1\n1.120000e+00\n9.500000e-01\n0.0000e+00\n5.000000e-01\n1.000000e+00\n1.500000e+00\n9.000000e+00\n▇▂▁▁▁\n\n\nhfem\n0\n1\n2.300000e-01\n4.200000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ngrassroof\n0\n1\n7.400000e-01\n4.400000e-01\n0.0000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n▃▁▁▁▇\n\n\nmortarpestle\n0\n1\n5.000000e-01\n5.000000e-01\n0.0000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▇\n\n\ntable\n0\n1\n3.600000e-01\n4.800000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▅\n\n\nclock\n0\n1\n2.000000e-01\n4.000000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\nea\n0\n1\n7.606000e+01\n1.885500e+02\n1.0000e+00\n8.000000e+00\n1.900000e+01\n4.525000e+01\n9.010000e+02\n▇▁▁▁▁\n\n\nEA\n0\n1\n2.372322e+07\n7.241514e+06\n1.0101e+07\n2.040204e+07\n2.090352e+07\n3.053301e+07\n3.120209e+07\n▂▁▆▁▇\n\n\nhhwght\n0\n1\n2.387900e+02\n7.001000e+01\n7.9000e+01\n2.076000e+02\n2.471000e+02\n2.913000e+02\n3.587000e+02\n▂▁▇▇▂\n\n\npsu\n0\n1\n2.372322e+07\n7.241514e+06\n1.0101e+07\n2.040204e+07\n2.090352e+07\n3.053301e+07\n3.120209e+07\n▂▁▆▁▇\n\n\nstrataid\n0\n1\n1.560000e+01\n8.090000e+00\n1.0000e+00\n9.000000e+00\n1.500000e+01\n2.200000e+01\n3.000000e+01\n▅▇▇▆▅\n\n\nlnzline\n0\n1\n7.550000e+00\n0.000000e+00\n7.5500e+00\n7.550000e+00\n7.550000e+00\n7.550000e+00\n7.550000e+00\n▁▁▇▁▁\n\n\ncase_id\n0\n1\n2.372322e+10\n7.241514e+09\n1.0101e+10\n2.040204e+10\n2.090352e+10\n3.053301e+10\n3.120209e+10\n▂▁▆▁▇\n\n\n\n\n\n\n\nThe dataset contains 38 variables and 11,280 observations.\nNot all of these variables are relevant for our prediction model.\nTo find the labels and description of the variables, you can refer to the paper. \n\n[hhsize, hhsize2, age_head, age_head2, regions, rural, never married, share_of_adults_without_education, share_of_adults_who_can_read, number of rooms, cement floor, electricity, flush toilet, soap, bed, bike, music player, coffee table, iron, garden, goats]\n\nLuckily for us, we have no missing values (n_missing in summary output)!\n\nMany machine learning models cannot be trained when missing values are present (some exceptions exist).\nDealing with missingness is a non-trivial task:\n\n\nFirst and foremost, we should assess whether there is a pattern to missingness and if so, what that means to what we can learn from our (sub)population. If there is no discernible pattern, we can proceed to delete the missing values or impute them. A more detailed explanation and course of action can be found here.\n\nFeature selection: subsetting the dataset\nAs part of our data pre-processing we will subset the dataframe, such that only relevant variables are left. * Relevant: variables/features about a household that could help us determine whether they are in poverty. That way, we save some memory space; but also, we can call the full set of variables in a dataframe in one go!\nvariables to delete (not included in the identified set above):\n[ea, EA, hhwght, psu, strataid, lnzline, case_id, eatype] \n\nN/B: Feature selection is a critical process (and we normally don’t have a paper to guide us through it): from a practical point of view, a model with less predictors may be easier to interpret. Also, some models may be negatively affected by non-informative predictors. This process is similar to traditional econometric modelling, but we should not conflate predictive and explanatory modelling. Importantly, please note that we are not interested in knowing why something happens, but rather in what is likely to happen given some known data. Hence:\n\n# object:vector that contains the names of the variables that we want to get rid of\n\ncols &lt;- c(\"ea\", \"EA\", \"psu\",\"hhwght\", \"strataid\", \"lnzline\", \"case_id\",\"eatype\")\n\n\n# subset of the data_malawi object:datframe\ndata_malawi &lt;- data_malawi[,-which(colnames(data_malawi) %in% cols)] # the minus sign indicates deletion of cols\n\nA few notes for you:\n\na dataframe follows the form data[rows,colums]\ncolnames() is a function that identifies the column names of an object of class dataframe\nwhich() is an R base function that gives you the position of some value\na minus sign will delete either the identified position in the row or the column space\n\n\n\nData visualisation\nA quick and effective way to take a first glance at our data is to plot histograms of relevant (numeric) features.\nRecall (from the skim() dataframe summary output) that only two variables are non-numeric. However, we need to make a distinction between class factor and class numeric/numeric.\n\n# identify categorical variables to transform from class numeric to factor\n# using a for-loop: print the number of unique values by variable\n\nfor (i in 1:ncol(data_malawi)) { # iterate over the length of columns in the data_malawi df\n    \n    # store the number of unique values in column.i \n    x &lt;- length(unique(data_malawi[[i]]))\n    \n    # print the name of column.i\n    print(colnames(data_malawi[i]))\n    # print the number of unique values in column.i\n    print(x)\n    \n}\n\n[1] \"lnexp_pc_month\"\n[1] 11266\n[1] \"hhsize\"\n[1] 19\n[1] \"hhsize2\"\n[1] 19\n[1] \"agehead\"\n[1] 88\n[1] \"agehead2\"\n[1] 88\n[1] \"north\"\n[1] 2\n[1] \"central\"\n[1] 2\n[1] \"rural\"\n[1] 2\n[1] \"nevermarried\"\n[1] 2\n[1] \"sharenoedu\"\n[1] 47\n[1] \"shareread\"\n[1] 28\n[1] \"nrooms\"\n[1] 16\n[1] \"floor_cement\"\n[1] 2\n[1] \"electricity\"\n[1] 2\n[1] \"flushtoilet\"\n[1] 2\n[1] \"soap\"\n[1] 2\n[1] \"bed\"\n[1] 2\n[1] \"bike\"\n[1] 2\n[1] \"musicplayer\"\n[1] 2\n[1] \"coffeetable\"\n[1] 2\n[1] \"iron\"\n[1] 2\n[1] \"dimbagarden\"\n[1] 2\n[1] \"goats\"\n[1] 2\n[1] \"dependratio\"\n[1] 62\n[1] \"hfem\"\n[1] 2\n[1] \"grassroof\"\n[1] 2\n[1] \"mortarpestle\"\n[1] 2\n[1] \"table\"\n[1] 2\n[1] \"clock\"\n[1] 2\n[1] \"region\"\n[1] 3\n\n\nIf you want to optimise your code, using for loops is not ideal. In R, there exists a family of functions called Apply whose purpose is to apply some function to all the elements in an object. For the time being, iterating over 38 columns is fast enough and we don’t need to think about optimising our code. We’ll also see an example later on on how to use one of the functions from the apply family. You can also refer to this blog (https://www.r-bloggers.com/2021/05/apply-family-in-r-apply-lapply-sapply-mapply-and-tapply/) if you want to learn more about it.\nNotice that we have a few variables with 2 unique values and one variable with 3 unique values. We should transform these into factor() class. Recall from the introduction tab that in object-oriented programming correctly identifying the variable type (vector class) is crucial for data manipulation and arithmetic operations. We can do this one by one, or in one shot. I’ll give an example of both:\n\n# == One by one == # \n\n# transform a variable in df to factor, and pass it on to the df to keep the change\n# first, sneak peak at the first 5 observations (rows) of the vector\nhead(data_malawi$north) # returns 1 = North \n\n[1] 1 1 1 1 1 1\n\ndata_malawi$north &lt;- factor(data_malawi$north, levels = c(0, 1), labels = c(\"NotNorth\", \"North\"))\n\nstr(data_malawi$north) # returns 2?\n\n Factor w/ 2 levels \"NotNorth\",\"North\": 2 2 2 2 2 2 2 2 2 2 ...\n\nhead(data_malawi$north) # returns north (which was = 1 before), hooray!\n\n[1] North North North North North North\nLevels: NotNorth North\n\n# R stores factors as 1...n; but the label remains the initial numeric value assigned (1 for true, 0 false)\n# We have also explicityly told 0 is NotNorth and 1 is North (by the order that follows the line of code)\n\n\n# transform all binary/categorical data into factor class\nmin_count &lt;- 3 # vector: 3 categories is our max number of categories found\n\n# == Apply family: function apply and lapply==#\n# apply a length(unique(x)) function to all the columns (rows = 1, columns = 2) of the data_malawi dataframe, then\n# store boolean (true/false) if the number of unique values is lower or equal to the min_count vector\nn_distinct2 &lt;- apply(data_malawi, 2, function(x) length(unique(x))) &lt;= min_count\n\n# print(n_distinct2) # prints boolean indicator object (so you know what object you have created)\n\n# select the identified categorical variables and transform them into factors\ndata_malawi[n_distinct2] &lt;- lapply(data_malawi[n_distinct2], factor) \n\nVisualise your data: histograms, bar plots, tables\nLet’s start with histograms of numeric (continuous) variables.\n\n# == HISTOGRAMS == #\n\n# Select all variables in the dataframe which are numeric, and can therefore be plotted as a histogram.\nmalawi_continuous &lt;- as.data.frame(data_malawi %&gt;% select_if(~is.numeric(.))) \n\n# a quick glance at the summary statistics of our continuous variables\ndatasummary_skim(malawi_continuous) # from modelsummary pkg, output as plot in Plot Viewer\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  lnexp_pc_month\n                  11266\n                  0\n                  7.4\n                  0.7\n                  4.8\n                  7.3\n                  11.1\n                  \n                \n                \n                  hhsize\n                  19\n                  0\n                  4.5\n                  2.3\n                  1.0\n                  4.0\n                  27.0\n                  \n                \n                \n                  hhsize2\n                  19\n                  0\n                  26.1\n                  28.0\n                  1.0\n                  16.0\n                  729.0\n                  \n                \n                \n                  agehead\n                  88\n                  0\n                  42.5\n                  16.4\n                  10.0\n                  39.0\n                  104.0\n                  \n                \n                \n                  agehead2\n                  88\n                  0\n                  2070.6\n                  1618.6\n                  100.0\n                  1521.0\n                  10816.0\n                  \n                \n                \n                  sharenoedu\n                  47\n                  0\n                  0.2\n                  0.3\n                  0.0\n                  0.0\n                  1.0\n                  \n                \n                \n                  shareread\n                  28\n                  0\n                  0.6\n                  0.4\n                  0.0\n                  0.7\n                  1.0\n                  \n                \n                \n                  nrooms\n                  16\n                  0\n                  2.5\n                  1.3\n                  0.0\n                  2.0\n                  16.0\n                  \n                \n                \n                  dependratio\n                  62\n                  0\n                  1.1\n                  0.9\n                  0.0\n                  1.0\n                  9.0\n                  \n                \n        \n      \n    \n\n\n# Hmisc package, quick and painless hist.data.frame() function\n# but first, make sure to adjust the number of rows and columns to be displayed on your Plot Viewer\npar(mfrow = c(3, 3)) # 3 rows * 3 columns (9 variables)\nhist.data.frame(malawi_continuous)\n\n\n\n\n\n\n\n\nNow, let’s plot bar graphs and write tables of factor (categorical) variables:\n\n# == BAR GRAPHS == #\nmalawi_factor &lt;- data_malawi %&gt;% select_if(~is.factor(.)) # subset of the data with all factor variables\n\npar(mfrow = c(3, 7)) # 7 rows, 3 columns (21 variables = length of df)\n\nfor (i in 1:ncol(malawi_factor)) { # Loop over all the columns in the factor df subset\n    \n    # store data in column.i as x\n    x &lt;- malawi_factor[,i]\n    \n    # store name of column.i as x_name\n    x_name &lt;- colnames(malawi_factor[i])\n    \n    # Plot bar graph of x using Rbase plotting tools\n    barplot(table(x),\n            main = paste(x_name)\n    )\n    \n}\n\n\n\n\n\n\n\n# == TABLES == #\n\n# We can also show tables of all factor variables (to get precise frequencies not displayed in bar plots)\nllply(.data=malawi_factor, .fun=table) # create tables of all the variables in dataframe using the plyr package\n\n$north\n\nNotNorth    North \n    9600     1680 \n\n$central\n\n   0    1 \n6960 4320 \n\n$rural\n\n   0    1 \n1440 9840 \n\n$nevermarried\n\n    0     1 \n10930   350 \n\n$floor_cement\n\n   0    1 \n9036 2244 \n\n$electricity\n\n    0     1 \n10620   660 \n\n$flushtoilet\n\n    0     1 \n10962   318 \n\n$soap\n\n   0    1 \n9740 1540 \n\n$bed\n\n   0    1 \n7653 3627 \n\n$bike\n\n   0    1 \n7194 4086 \n\n$musicplayer\n\n   0    1 \n9426 1854 \n\n$coffeetable\n\n   0    1 \n9956 1324 \n\n$iron\n\n   0    1 \n8962 2318 \n\n$dimbagarden\n\n   0    1 \n7658 3622 \n\n$goats\n\n   0    1 \n8856 2424 \n\n$hfem\n\n   0    1 \n8697 2583 \n\n$grassroof\n\n   0    1 \n2953 8327 \n\n$mortarpestle\n\n   0    1 \n5635 5645 \n\n$table\n\n   0    1 \n7249 4031 \n\n$clock\n\n   0    1 \n9039 2241 \n\n$region\n\nCentre  North  South \n  4320   1680   5280 \n\n\n\nWhat have we learned from the data visualisation?\n\nNothing worrying about the data itself. McBride and Nichols did a good job of pre-processing the data for us. No pesky missing values, or unknown categories.\nFrom the bar plots, we can see that for the most part, people tend not to own assets. Worryingly, there is a lack of soap, flush toilets and electricity, all of which are crucial for human capital (health and education).\nFrom the histograms, we can see log per capita expenditure is normally distributed, but if we remove the log, it’s incredibly skewed. Poverty is endemic. Households tend not to have too many educated individuals, and their size is non-trivially large (with less rooms than people need).\n\n\n\nRelationships between features\nTo finalise our exploration of the dataset, we should define:\n\nthe target variable (a.k.a. outcome of interest)\ncorrelational insights\n\nLet’s visualise two distinct correlation matrices; for our numeric dataframe, which includes our target variable, we will plot a Pearson r correlation matrix. For our factor dataframe, to which we will add our continuous target, we will plot a Spearman rho correlation matrix. Both types of correlation coefficients are interpreted the same (0 = no correlation, 1 perfect positive correlation, -1 perfect negative correlation).\n\n# = = PEARSON CORRELATION MATRIX = = #\n\nM &lt;- cor(malawi_continuous) # create a correlation matrix of the continuous dataset, cor() uses Pearson's correlation coefficient as default. This means we can only take the correlation between continuous variables\ncorrplot(M, method=\"circle\", addCoef.col =\"black\", number.cex = 0.8) # visualise it in a nice way\n\n\n\n\n\n\n\n\nWe can already tell that the size of the household and dependent ratio are highly negatively correlated to our target variable.\n\n# = = SPEARMAN CORRELATION MATRIX = = #\n\nmalawi_factorC &lt;- as.data.frame(lapply(malawi_factor,as.numeric)) # coerce dataframe to numeric, as the cor() command only takes in numeric types\nmalawi_factorC$lnexp_pc_month &lt;- malawi_continuous$lnexp_pc_month # include target variable in the dataframe\n\nM2 &lt;- cor(malawi_factorC, method = \"spearman\")\ncorrplot(M2, method=\"circle\", addCoef.col =\"black\", number.cex = 0.3) # visualise it in a nice way\n\n\n\n\n\n\n\n\nOwnership of some assets stands out: soap, a cement floor, electricity, a bed… ownership of these (and a couple of other) assets is positively correlated to per capita expenditure. Living in a rural area, on the other hand, is negtively correlated to our target variable.\nWe can also spot some correlation coefficients that equal zero. In some situations, the data generating mechanism can create predictors that only have a single unique value (i.e. a “zero-variance predictor”). For many ML models (excluding tree-based models), this may cause the model to crash or the fit to be unstable. Here, the only \\(0\\) we’ve spotted is not in relation to our target variable.\nBut we do observe some near-zero-variance predictors. Besides uninformative, these can also create unstable model fits. There’s a few strategies to deal with these; the quickest solution is to remove them. A second option, which is especially interesting in scenarios with a large number of predictors/variables, is to work with penalised models. We’ll discuss this option below.\n\n\n\n3. Model fit: data partition and performance evaluation parameters\nWe now have a general idea of the structure of the data we are working with, and what we’re trying to predict: per capita expenditures, which we believe are a proxy for poverty prevalence. Measured by the log of per capita monthly expenditure in our dataset, the variable is named lnexp_pc_month.\nThe next step is create a simple linear model (OLS) to predict per capita expenditure using the variables in our dataset, and introduce the elements with which we will evaluate our model.\n\nData Partinioning\nWhen we want to build predictive models for machine learning purposes, it is important to have (at least) two data sets. A training data set from which our model will learn, and a test data set containing the same features as our training data set; we use the second dataset to see how well our predictive model extrapolates to other samples (i.e. is it generalisable?). To split our main data set into two, we will work with an 80/20 split.\nThe 80/20 split has its origins in the Pareto Principle, which states that ‘in most cases, 80% of effects from from 20% of causes’. Though there are other test/train splitting options, this partitioning method is a good place to start, and indeed standard in the machine learning field.\n\n# First, set a seed to guarantee replicability of the process\nset.seed(1234) # you can use any number you want, but to replicate the results in this tutorial you need to use this number\n\n# We could split the data manually, but the caret package includes an useful function\n\ntrain_idx &lt;- createDataPartition(data_malawi$lnexp_pc_month, p = .8, list = FALSE, times = 1)\nhead(train_idx) # notice that observation 5 corresponds to resame indicator 7 and so on. We're shuffling and picking!\n\n     Resample1\n[1,]         1\n[2,]         2\n[3,]         4\n[4,]         5\n[5,]         7\n[6,]         8\n\nTrain_df &lt;- data_malawi[ train_idx,]\nTest_df  &lt;- data_malawi[-train_idx,]\n\n# Note that we have created training and testing dataframes as an 80/20 split of the original dataset.\n\n\n\nPrediction with Linear Models\nWe will start by fitting a predictive model using the training dataset; that is, our target variable log of monthly per capita expenditures or lnexp_pc_month will be a \\(Y\\) dependent variable in a linear model \\(Y_i = \\alpha + x'\\beta_i + \\epsilon_i\\), and the remaining features in the data frame correspond to the row vectors \\(x'\\beta\\).\n\nmodel1 &lt;- lm(lnexp_pc_month ~ .,Train_df) # the dot after the squiggle ~ asks the lm() function tu use all other variables in the dataframe as predictors to the dependent variable lnexp_pc_month\n\nstargazer(model1, type = \"text\") # printed in the console as text\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                          lnexp_pc_month       \n-----------------------------------------------\nhhsize                       -0.292***         \n                              (0.006)          \n                                               \nhhsize2                      0.012***          \n                             (0.0005)          \n                                               \nagehead                        0.003           \n                              (0.002)          \n                                               \nagehead2                    -0.00004**         \n                             (0.00002)         \n                                               \nnorthNorth                   0.080***          \n                              (0.014)          \n                                               \ncentral1                     0.252***          \n                              (0.010)          \n                                               \nrural1                       -0.055***         \n                              (0.017)          \n                                               \nnevermarried1                0.271***          \n                              (0.028)          \n                                               \nsharenoedu                   -0.105***         \n                              (0.020)          \n                                               \nshareread                    0.075***          \n                              (0.015)          \n                                               \nnrooms                       0.039***          \n                              (0.004)          \n                                               \nfloor_cement1                0.102***          \n                              (0.017)          \n                                               \nelectricity1                 0.372***          \n                              (0.027)          \n                                               \nflushtoilet1                 0.329***          \n                              (0.033)          \n                                               \nsoap1                        0.215***          \n                              (0.014)          \n                                               \nbed1                         0.104***          \n                              (0.013)          \n                                               \nbike1                        0.094***          \n                              (0.011)          \n                                               \nmusicplayer1                 0.111***          \n                              (0.015)          \n                                               \ncoffeetable1                 0.137***          \n                              (0.019)          \n                                               \niron1                        0.130***          \n                              (0.014)          \n                                               \ndimbagarden1                 0.102***          \n                              (0.010)          \n                                               \ngoats1                       0.080***          \n                              (0.012)          \n                                               \ndependratio                  -0.045***         \n                              (0.006)          \n                                               \nhfem1                        -0.066***         \n                              (0.012)          \n                                               \ngrassroof1                   -0.096***         \n                              (0.016)          \n                                               \nmortarpestle1                0.033***          \n                              (0.010)          \n                                               \ntable1                       0.051***          \n                              (0.011)          \n                                               \nclock1                       0.058***          \n                              (0.014)          \n                                               \nregionNorth                                    \n                                               \n                                               \nregionSouth                                    \n                                               \n                                               \nConstant                     7.978***          \n                              (0.044)          \n                                               \n-----------------------------------------------\nObservations                   9,024           \nR2                             0.599           \nAdjusted R2                    0.597           \nResidual Std. Error      0.428 (df = 8995)     \nF Statistic         479.041*** (df = 28; 8995) \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n# We can also estimate iid and robust standard errors, using the model output package modelsummary()\nms &lt;- modelsummary(model1,\n             vcov = list(\"iid\",\"robust\"), # include iid and HC3 (robust) standard errors\n             statistic = c(\"p = {p.value}\",\"s.e. = {std.error}\"),\n             stars = TRUE,\n             output = \"gt\"\n             ) # plotted as an image / object in \"gt\" format\n\nms %&gt;% tab_header(\n    title = md(\"**Linear Models with iid and robust s.e.**\"),\n    subtitle = md(\"Target: (log) per capita monthly expenditure\")\n)\n\n\n\n\n\n\n\nLinear Models with iid and robust s.e.\n\n\nTarget: (log) per capita monthly expenditure\n\n\n\n(1)\n(2)\n\n\n\n\n(Intercept)\n7.978***\n7.978***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.044\ns.e. = 0.069\n\n\nhhsize\n-0.292***\n-0.292***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.006\ns.e. = 0.029\n\n\nhhsize2\n0.012***\n0.012***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.000\ns.e. = 0.003\n\n\nagehead\n0.003\n0.003\n\n\n\np = 0.108\np = 0.137\n\n\n\ns.e. = 0.002\ns.e. = 0.002\n\n\nagehead2\n-0.000*\n-0.000*\n\n\n\np = 0.015\np = 0.032\n\n\n\ns.e. = 0.000\ns.e. = 0.000\n\n\nnorthNorth\n0.080***\n0.080***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.014\ns.e. = 0.015\n\n\ncentral1\n0.252***\n0.252***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.010\ns.e. = 0.010\n\n\nrural1\n-0.055**\n-0.055**\n\n\n\np = 0.001\np = 0.001\n\n\n\ns.e. = 0.017\ns.e. = 0.017\n\n\nnevermarried1\n0.271***\n0.271***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.028\ns.e. = 0.037\n\n\nsharenoedu\n-0.105***\n-0.105***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.020\ns.e. = 0.021\n\n\nshareread\n0.075***\n0.075***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.015\ns.e. = 0.015\n\n\nnrooms\n0.039***\n0.039***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.004\ns.e. = 0.004\n\n\nfloor_cement1\n0.102***\n0.102***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.017\ns.e. = 0.018\n\n\nelectricity1\n0.372***\n0.372***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.027\ns.e. = 0.029\n\n\nflushtoilet1\n0.329***\n0.329***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.033\ns.e. = 0.042\n\n\nsoap1\n0.215***\n0.215***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.014\ns.e. = 0.014\n\n\nbed1\n0.104***\n0.104***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.013\ns.e. = 0.013\n\n\nbike1\n0.094***\n0.094***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.011\ns.e. = 0.011\n\n\nmusicplayer1\n0.111***\n0.111***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.015\ns.e. = 0.015\n\n\ncoffeetable1\n0.137***\n0.137***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.019\ns.e. = 0.019\n\n\niron1\n0.130***\n0.130***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.014\ns.e. = 0.014\n\n\ndimbagarden1\n0.102***\n0.102***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.010\ns.e. = 0.010\n\n\ngoats1\n0.080***\n0.080***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.012\ns.e. = 0.012\n\n\ndependratio\n-0.045***\n-0.045***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.006\ns.e. = 0.009\n\n\nhfem1\n-0.066***\n-0.066***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.012\ns.e. = 0.013\n\n\ngrassroof1\n-0.096***\n-0.096***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.016\ns.e. = 0.016\n\n\nmortarpestle1\n0.033**\n0.033**\n\n\n\np = 0.001\np = 0.001\n\n\n\ns.e. = 0.010\ns.e. = 0.010\n\n\ntable1\n0.051***\n0.051***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.011\ns.e. = 0.012\n\n\nclock1\n0.058***\n0.058***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.014\ns.e. = 0.014\n\n\nNum.Obs.\n9024\n9024\n\n\nR2\n0.599\n0.599\n\n\nR2 Adj.\n0.597\n0.597\n\n\nAIC\n10317.3\n10317.3\n\n\nBIC\n10530.5\n10530.5\n\n\nLog.Lik.\n-5128.658\n-5128.658\n\n\nRMSE\n0.43\n0.43\n\n\nStd.Errors\nIID\nHC3\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\nYou can see that the variable region was not included in the output (this is because we already have dummies of north (not north = south), and central region). We may need to clean our dataset un some further steps.\nRecall that one of the assumptions of a linear model is that errors are independent and identically distributed. We could run some tests to determine this, but with the contrast of the iid and robust errors (HC3) in the modelsummary output table we can already tell that this is not an issue/something to worry about in our estimations.\nBesides the regression output table, we can can also visualise the magnitude and significance of the coefficients with a plot. The further away the variable (dot) marker is from the \\(0\\) line, the larger the magnitude.\n\nmodelplot(model1) + \n          aes(color = ifelse(p.value &lt; 0.001, \"Significant\", \"Not significant\")) +\n              scale_color_manual(values = c(\"grey\", \"black\")\n                                 )\n\n\n\n\n\n\n\n# grey points indicate statistically insignificat (p&gt;0.001) coefficient estimates\n# The scale of the plot is large due to the intercept estimate\n\nNot unlike what we saw in out correlation matrix, household size, electricity, having a flush toilet… the magnitude of the impact of these variables on monthly per capita expenditure is significantly larger than that of other assets/predictors.\n\n\nPerformance Indicators\nIn predictive modelling, we are interested in the following performance metrics:\n\nModel residuals: recall residuals are the observed value minus the predicted value. We can estimate a model’s Root Mean Squared Error (RMSE) or the Mean Absolute Error (MAE). Residuals allow us to quantify the extent to which the predicted response value (for a given observation) is close to the true response value. Small RMSE or MAE values indicate that the prediction is close to the true observed value.\nThe p-values: represented by stars *** (and a pre-defined critical threshold, e.g. 0.05), they point to the predictive power of each feature in the model; i.e. that the event does not occur by chance. In the same vein, the magnitude of the coefficient is also important, especially given that we are interested in explanatory power and not causality.\nThe R-squared: arguably the go-to indicator for performance assessment. Low R^2 values are not uncommon, especially in the social sciences. However, when hoping to use a model for predictive purposes, a low R^2 is a bad sign, large number of statistically significant features notwithstanding. The drawback from relying solely on this measure is that it does not take into consideration the problem of model over-fitting; i.e. you can inflate the R-squared by adding as many variables as you want, even if those variables have little predicting power. This method will yield great results in the training data, but will under perform when extrapolating the model to the test (or indeed any other) data set.\n\nResiduals\n\n# = = Model Residuals = = #\nprint(summary(model1$residuals))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-3.483927 -0.288657 -0.007872  0.000000  0.266950  1.887123 \n\n\nRecall that the residual is estimated as the true (observed) value minus the predicted value. Thus: the max(imum) error of 1.88 suggests that the model under-predicted expenditure by circa (log) $2 (or $6.5) for at least one observation. Fifty percent of the predictions (between the first and third quartiles) lie between (log) $0.28 and (log) $0.26 over the true value. From the estimation of the prediction residuals we obtain the popular measure of performance evaluation known as the Root Mean Squared Error (RMSE, for short).\n\n# Calculate the RMSE for the training dataset, or the in-sample RMSE.\n\n# 1. Predict values on the training dataset\np0 &lt;- predict(model1, Train_df)\n\n# 2. Obtain the errors (predicted values minus observed values of target variable)\nerror0 &lt;- p0 - Train_df[[\"lnexp_pc_month\"]]\n\n# 3. In-sample RMSE\nRMSE_insample &lt;- sqrt(mean(error0 ^ 2))\nprint(RMSE_insample)\n\n[1] 0.4271572\n\n# TIP: Notice that the in-sample RMSE we have just estimated was also printed in the linear model (regression) output tables!\n# The table printed with the stargazer package returns this value at the bottom, under the header Residual Std. Error (0.428)\n# The table printed with the modelsummary package returns this value at the bottom, under the header RMSE (0.43), rounded up. \n\nThe RMSE (0.4271) gives us an absolute number that indicates how much our predicted values deviate from the true (observed) number. This is all in reference to the target vector (a.k.a. our outcome variable). Think of the question, how far, on average, are the residuals away from zero? Generally speaking, the lower the value, the better the model fit. Besides being a good measure of goodness of fit, the RMSE is also useful for comparing the ability of our model to make predictions on different (e.g. test) data sets. The in-sample RMSE should be close or equal to the out-of-sample RMSE.\nIn this case, our RMSE is ~0.4 units away from zero. Given the range of the target variable (roughly 4 to 11), the number seems to be relatively small and close enough to zero.\nP-values\nRecall the large number of statistically significant features in our model. The coefficient plot, where we indicate that statistical significance is defined by a p-value threshold of 0.001, a strict rule (given the popularity of the more relaxed 0.05 critical value), shows that only 4 out of 29 features/variables do not meet this criterion. We can conclude that the features we have selected are relevant predictors.\nR-squared\n\nprint(summary(model1))\n\n\nCall:\nlm(formula = lnexp_pc_month ~ ., data = Train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4839 -0.2887 -0.0079  0.2669  1.8871 \n\nCoefficients: (2 not defined because of singularities)\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.978e+00  4.409e-02 180.945  &lt; 2e-16 ***\nhhsize        -2.919e-01  6.429e-03 -45.397  &lt; 2e-16 ***\nhhsize2        1.206e-02  4.811e-04  25.067  &lt; 2e-16 ***\nagehead        2.749e-03  1.712e-03   1.606  0.10835    \nagehead2      -4.168e-05  1.717e-05  -2.428  0.01519 *  \nnorthNorth     8.001e-02  1.439e-02   5.560 2.78e-08 ***\ncentral1       2.521e-01  1.023e-02  24.639  &lt; 2e-16 ***\nrural1        -5.483e-02  1.687e-02  -3.250  0.00116 ** \nnevermarried1  2.706e-01  2.839e-02   9.533  &lt; 2e-16 ***\nsharenoedu    -1.045e-01  2.006e-02  -5.212 1.91e-07 ***\nshareread      7.514e-02  1.502e-02   5.004 5.72e-07 ***\nnrooms         3.898e-02  4.032e-03   9.670  &lt; 2e-16 ***\nfloor_cement1  1.016e-01  1.742e-02   5.835 5.58e-09 ***\nelectricity1   3.718e-01  2.696e-02  13.793  &lt; 2e-16 ***\nflushtoilet1   3.295e-01  3.263e-02  10.099  &lt; 2e-16 ***\nsoap1          2.150e-01  1.413e-02  15.220  &lt; 2e-16 ***\nbed1           1.040e-01  1.297e-02   8.019 1.20e-15 ***\nbike1          9.395e-02  1.064e-02   8.831  &lt; 2e-16 ***\nmusicplayer1   1.111e-01  1.451e-02   7.658 2.08e-14 ***\ncoffeetable1   1.369e-01  1.866e-02   7.338 2.36e-13 ***\niron1          1.302e-01  1.378e-02   9.455  &lt; 2e-16 ***\ndimbagarden1   1.018e-01  1.017e-02  10.017  &lt; 2e-16 ***\ngoats1         7.964e-02  1.168e-02   6.820 9.68e-12 ***\ndependratio   -4.501e-02  5.848e-03  -7.697 1.55e-14 ***\nhfem1         -6.570e-02  1.238e-02  -5.307 1.14e-07 ***\ngrassroof1    -9.590e-02  1.575e-02  -6.089 1.18e-09 ***\nmortarpestle1  3.288e-02  1.027e-02   3.200  0.00138 ** \ntable1         5.074e-02  1.149e-02   4.417 1.01e-05 ***\nclock1         5.840e-02  1.423e-02   4.103 4.11e-05 ***\nregionNorth           NA         NA      NA       NA    \nregionSouth           NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4278 on 8995 degrees of freedom\nMultiple R-squared:  0.5986,    Adjusted R-squared:  0.5973 \nF-statistic:   479 on 28 and 8995 DF,  p-value: &lt; 2.2e-16\n\n\nWe have printed our model’s output once more, now using the r base command summary(). The other packages (stargazer and modelsummary) are great if you want to export your results in text, html, latex format, but not necessary if you just want to print your output. The estimated (Multiple) R-squared of 0.59 tells us that our model predicts around 60 per cent of the variation in the independent variable (our target, log of per capita monthly expenditures). Also note that when we have a large number of predictors, it’s best to look at the Adjusted R-squared (of 0.59), which corrects or adjusts for this by only increasing when a new feature improves the model more so than what would be expected by chance.\n\n\nOut of sample predictions\nNow that we have built and evaluated our model in the training dataset, we can proceed to make out-of-sample predictions. That is, see how our model performs in the test dataset.\n\np &lt;- predict(model1, Test_df)\n\n### Observed summary statistics of target variable in full dataset\n\nprint(summary(data_malawi$lnexp_pc_month))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.777   6.893   7.305   7.359   7.758  11.064 \n\n### Predictions based on the training dataset\n\nprint(summary(p0))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  6.011   6.980   7.308   7.357   7.661  10.143 \n\n### Predictions from the testing dataset\n\nprint(summary(p))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  6.052   6.999   7.323   7.375   7.670   9.774 \n\n\nThe summary statistics for the predictions with the train and test datasets are very close to one another. This is an indication that our model extrapolates well to other datasets. Compared to the observed summary statistics of the target variable, they’re relatively close, with the largest deviations observed at the minimum and maximum values.\n\n\nThe bias-variance tradeoff in practice\nWe previously mentioned that the RMSE metric could also be used to compare between train and test model predictions. Let us estimate the out-of-sample RMSE:\n\nerror &lt;- p - Test_df[[\"lnexp_pc_month\"]] # predicted values minus actual values\nRMSE_test &lt;- sqrt(mean(error^2))\nprint(RMSE_test) # this is known as the out-of-sample RMSE\n\n[1] 0.4284404\n\n\nNotice that the in-sample RMSE [\\(0.4271572\\)] is very close to the out-of-sample RMSE [\\(0.4284404\\)]. This means that our model makes consistent predictions across different data sets. We also know by now that these predictions are relatively good. At least, we hit the mark around 60 per cent of the time. What we are observing here is a model that has found a balance between bias and variance. However, both measures can still improve. For one, McBride and Nichols (2018) report &gt; 80 per cent accuracy in their poverty predictions for Malawi. But please note that, at this point, we are not replicating their approach. They document using a classification model, which means that they previously used a poverty line score and divided the sample between individuals below and above the poverty line.\nWhat do we mean by bias in a model?\nThe bias is the difference between the average prediction of our model and the true (observed) value. Minimising the bias is analogous to minimising the RMSE.\nWhat do we mean by variance in a model?\nIt is the observed variability of our model prediction for a given data point (how much the model can adjust given the data set). A model with high variance would yield low error values in the training data but high errors in the test data.\nHence, consistent in and out of sample RMSE scores = bias/variance balance.\n\n\nFine-tuning model parameters\nCan we fine-tune model parameters? The quick answer is yes! Every algorithm has a set of parameters that can be adjusted/fine-tuned to improve our estimations. Even in the case of a simple linear model, we can try our hand at fine-tuning with, for example, cross-validation.\nWhat is cross-validation?\nBroadly speaking, it is a technique that allows us to assess the performance of our machine learning model. How so? Well, it looks at the ‘stability’ of the model. It’s a measure of how well our model would work on new, unseen data (is this ringing a bell yet?); i.e. it has correctly observed and recorded the patterns in the data and has not captured too much noise (what we know as the error term, or what we are unable to explain with our model). K-fold cross-validation is a good place to start for such a thing. In the words of The Internet™, what k-fold cross validation does is:\nSplit the input dataset into K groups\n    For each group:\n        - Take one group as the reserve or test data set.\n        - Use remaining groups as the training dataset.\n        - Fit the model on the training set and evaluate the performance of the model using the test set.\nTL;DR We’re improving our splitting technique!\nAn Illustration by Eugenia Anello\n\n\n\n\n\n\n# Let's rumble! \n\nset.seed(12345)\n\n# create an object that defines the training method as cross-validation and number of folds (caret pkg)\ncv_10fold &lt;- trainControl(\n    method = \"cv\", #cross-validation\n    number = 10 # k-fold = 10-fold (split the data into 10 similar-sized samples)\n)\n\nset.seed(12345)\n\n# train a model \nols_kfold &lt;- train(\n    lnexp_pc_month ~ .,\n    data = Train_df,\n    method = 'lm', # runs a linear regression model (or ols)\n    trControl = cv_10fold # use 10 folds to cross-validate\n)\nols_kfold2 &lt;- train(\n    lnexp_pc_month ~ .,\n    data = Test_df,\n    method = 'lm', # runs a linear regression model (or ols)\n    trControl = cv_10fold # use 10 folds to cross-validate\n)\n\nols_kfold3 &lt;- train(\n    lnexp_pc_month ~ .,\n    data = data_malawi,\n    method = 'lm', # runs a linear regression model (or ols)\n    trControl = cv_10fold # use 10 folds to cross-validate\n)\n\n### Linear model with train dataset\nprint(ols_kfold)\n\nLinear Regression \n\n9024 samples\n  29 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 8121, 8121, 8123, 8122, 8121, 8121, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.4295064  0.5947776  0.3365398\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n#### Linear model with test dataset\nprint(ols_kfold2)\n\nLinear Regression \n\n2256 samples\n  29 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 2030, 2030, 2030, 2030, 2031, 2031, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.4312944  0.5991526  0.3379473\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n### Linear model with full dataset\")\nprint(ols_kfold3)\n\nLinear Regression \n\n11280 samples\n   29 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 10152, 10152, 10152, 10152, 10152, 10152, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.4291473  0.5965825  0.3363802\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nHaving cross-validated our model with 10 folds, we can see that the results are virtually the same. We have a RMSE of around .43, an R^2 of .59, (and a MAE of .33). We ran the same model for the train, test and full datasets and find results are consistent. Recall the R^2 tells us the model’s predictive ability and the RMSE and MAE the model’s accuracy.\nNOTE: k-fold cross-validation replaces our original 80/20 split (we don’t need to do that anymore!) Therefore, we use the full dataset in the train() function. Using the train, test and full datasets in the example above was just for pedagogical purposes but it is no longer necessary and we can use the reported estimates of R^2 (close to 1!), RMSE (close to 0!) and MAE (let’s go low!) for model assessment without comparing them to another set of predictions. These reported parameters are estimated as the average of the R^2, RMSE, and MAE for all the folds.\n\n\n\n4. Feature selection with Lasso linear regression\nAn OLS regression is not the only model that can be written in the form of \\(Y_i = \\alpha + \\beta_1X_{1i}, \\beta_2X_{2i},..., \\beta_pX_{pi}+ u_i\\). In this section we will discuss ‘penalised’ models, which can also be expressed as a linear relationship between parameters. Penalised regression models are also known as regression shrinkage methods, and they take their name after the colloquial term for coefficient regularisation, ‘shrinkage’ of estimated coefficients. The goal of penalised models, as opposed to a traditional linear model, is not to minimise bias (least squares approach), but to reduce variance by adding a constraint to the equation and effectively pushing coefficient parameters towards 0. This results in the the worse model predictors having a coefficient of zero or close to zero.\nConsider a scenario where you have hundreds of predictors. Which covariates are truly important for our known outcome? Including all of the predictors leads to over-fitting. We’ll find that the R^2 value is high, and conclude that our in-sample fit is good. However, this may lead to bad out-of-sample predictions. Model selection is a particularly challenging endeavour when we encounter high-dimensional data; i.e. when the number of variables is close to or larger than the number of observations. Some examples where you may encounter high-dimensional data include:\n\nCross-country analyses: we have a small and finite number of countries, but we may collect/observe as many variables as we want.\nCluster-population analyses: we wish to understand the outcome of some unique population \\(n\\), e.g. all students from classroom A. We collect plenty of information on these students, but the sample and the population are analogous \\(n = N\\), and thus the sample number of observations is small and finite.\n\nThe LASSO - Least Absolute Shrinkage and Selection Operator imposes a shrinking penalty to those predictors that do not actually belong in the model, and reduces the size of the estimated \\(\\beta\\) coefficients towards and including zero (when the tuning parameter/ shrinkage penalty \\(\\lambda\\) is sufficiently large). Note that \\(lambda\\) is the penalty term called L1-norm, and corresponds to the sum of the absolute coefficients.\nWhat does this mean for our case? Well, McBride and Nichols (2018) used two sources to compile their final dataset (which we are using). One of those sources is the Living Standards Measurement Study (LSMS) from the World Bank Group. These surveys collect more than 500 variables/ features per country case. You can subset the dataset with critical reasoning: using your knowledge of the phenomenon, which features might best describe it? But even then, we might still end up with a large number of features. Perhaps a LASSO approach could work here.\nOur own data is not ideal for the approach, because it has a small number of features (\\(29\\)), compared to the large number of observations (\\(11,280\\)). But let’s run a lasso regression and see if my statement holds. How would I know if it holds? If the Lasso regression returns the same values for the performance assessment parameters as the OLS regression; this means that the penalisation parameter was either zero (leading back to an OLS) or too small to make a difference.\n\n### small detour ... \nsummary(model1)\n\n\nCall:\nlm(formula = lnexp_pc_month ~ ., data = Train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4839 -0.2887 -0.0079  0.2669  1.8871 \n\nCoefficients: (2 not defined because of singularities)\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.978e+00  4.409e-02 180.945  &lt; 2e-16 ***\nhhsize        -2.919e-01  6.429e-03 -45.397  &lt; 2e-16 ***\nhhsize2        1.206e-02  4.811e-04  25.067  &lt; 2e-16 ***\nagehead        2.749e-03  1.712e-03   1.606  0.10835    \nagehead2      -4.168e-05  1.717e-05  -2.428  0.01519 *  \nnorthNorth     8.001e-02  1.439e-02   5.560 2.78e-08 ***\ncentral1       2.521e-01  1.023e-02  24.639  &lt; 2e-16 ***\nrural1        -5.483e-02  1.687e-02  -3.250  0.00116 ** \nnevermarried1  2.706e-01  2.839e-02   9.533  &lt; 2e-16 ***\nsharenoedu    -1.045e-01  2.006e-02  -5.212 1.91e-07 ***\nshareread      7.514e-02  1.502e-02   5.004 5.72e-07 ***\nnrooms         3.898e-02  4.032e-03   9.670  &lt; 2e-16 ***\nfloor_cement1  1.016e-01  1.742e-02   5.835 5.58e-09 ***\nelectricity1   3.718e-01  2.696e-02  13.793  &lt; 2e-16 ***\nflushtoilet1   3.295e-01  3.263e-02  10.099  &lt; 2e-16 ***\nsoap1          2.150e-01  1.413e-02  15.220  &lt; 2e-16 ***\nbed1           1.040e-01  1.297e-02   8.019 1.20e-15 ***\nbike1          9.395e-02  1.064e-02   8.831  &lt; 2e-16 ***\nmusicplayer1   1.111e-01  1.451e-02   7.658 2.08e-14 ***\ncoffeetable1   1.369e-01  1.866e-02   7.338 2.36e-13 ***\niron1          1.302e-01  1.378e-02   9.455  &lt; 2e-16 ***\ndimbagarden1   1.018e-01  1.017e-02  10.017  &lt; 2e-16 ***\ngoats1         7.964e-02  1.168e-02   6.820 9.68e-12 ***\ndependratio   -4.501e-02  5.848e-03  -7.697 1.55e-14 ***\nhfem1         -6.570e-02  1.238e-02  -5.307 1.14e-07 ***\ngrassroof1    -9.590e-02  1.575e-02  -6.089 1.18e-09 ***\nmortarpestle1  3.288e-02  1.027e-02   3.200  0.00138 ** \ntable1         5.074e-02  1.149e-02   4.417 1.01e-05 ***\nclock1         5.840e-02  1.423e-02   4.103 4.11e-05 ***\nregionNorth           NA         NA      NA       NA    \nregionSouth           NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4278 on 8995 degrees of freedom\nMultiple R-squared:  0.5986,    Adjusted R-squared:  0.5973 \nF-statistic:   479 on 28 and 8995 DF,  p-value: &lt; 2.2e-16\n\n# notice that region dummies are repeated and therefore return NA in the output. We should clean this before proceeding.\n# these guys are troublesome (recall our conversation about zero-variance predictors?)\ndata_malawi2 &lt;- data_malawi[,-which(colnames(data_malawi)==\"region\")]\nsummary(lm(lnexp_pc_month~., data = data_malawi2)) # no more excluded categorical variables (recall this happened in our original linear model due to repeated measures?)\n\n\nCall:\nlm(formula = lnexp_pc_month ~ ., data = data_malawi2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6626 -0.2906 -0.0096  0.2665  1.8788 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.965e+00  3.947e-02 201.781  &lt; 2e-16 ***\nhhsize        -2.979e-01  5.854e-03 -50.893  &lt; 2e-16 ***\nhhsize2        1.246e-02  4.401e-04  28.307  &lt; 2e-16 ***\nagehead        3.545e-03  1.529e-03   2.319 0.020433 *  \nagehead2      -5.079e-05  1.532e-05  -3.315 0.000918 ***\nnorthNorth     7.452e-02  1.292e-02   5.766 8.32e-09 ***\ncentral1       2.552e-01  9.119e-03  27.979  &lt; 2e-16 ***\nrural1        -5.488e-02  1.507e-02  -3.642 0.000272 ***\nnevermarried1  2.416e-01  2.507e-02   9.638  &lt; 2e-16 ***\nsharenoedu    -1.053e-01  1.802e-02  -5.845 5.21e-09 ***\nshareread      7.838e-02  1.344e-02   5.832 5.63e-09 ***\nnrooms         4.182e-02  3.609e-03  11.587  &lt; 2e-16 ***\nfloor_cement1  9.460e-02  1.546e-02   6.119 9.73e-10 ***\nelectricity1   3.719e-01  2.387e-02  15.579  &lt; 2e-16 ***\nflushtoilet1   3.471e-01  2.886e-02  12.027  &lt; 2e-16 ***\nsoap1          2.163e-01  1.261e-02  17.154  &lt; 2e-16 ***\nbed1           1.051e-01  1.156e-02   9.099  &lt; 2e-16 ***\nbike1          9.405e-02  9.516e-03   9.883  &lt; 2e-16 ***\nmusicplayer1   1.090e-01  1.287e-02   8.466  &lt; 2e-16 ***\ncoffeetable1   1.311e-01  1.641e-02   7.991 1.47e-15 ***\niron1          1.325e-01  1.231e-02  10.761  &lt; 2e-16 ***\ndimbagarden1   9.140e-02  9.095e-03  10.049  &lt; 2e-16 ***\ngoats1         8.671e-02  1.049e-02   8.269  &lt; 2e-16 ***\ndependratio   -4.086e-02  5.225e-03  -7.820 5.74e-15 ***\nhfem1         -6.761e-02  1.102e-02  -6.135 8.80e-10 ***\ngrassroof1    -9.138e-02  1.394e-02  -6.556 5.77e-11 ***\nmortarpestle1  3.207e-02  9.193e-03   3.489 0.000487 ***\ntable1         5.132e-02  1.029e-02   4.987 6.23e-07 ***\nclock1         6.254e-02  1.273e-02   4.912 9.16e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4278 on 11251 degrees of freedom\nMultiple R-squared:  0.5997,    Adjusted R-squared:  0.5987 \nF-statistic: 601.9 on 28 and 11251 DF,  p-value: &lt; 2.2e-16\n\nset.seed(12345)\ncv_10fold &lt;- trainControl(\n    method = \"cv\", \n    number = 10 \n)\nset.seed(12345)\nmodel_lasso &lt;- caret::train(\n    lnexp_pc_month ~ .,\n    data = data_malawi,\n    method = 'lasso',\n    preProcess = c(\"center\", \"scale\"), #This will first center and then scale all relevant variables in the model\n    trControl = cv_10fold # use 10 folds to cross-validate // we already know this fine-tuning may not be needed!\n    )\n\nprint(model_lasso) # best R^2 and RMSE is similar to the one using a linear regression...( and so is the MAE!)\n\nThe lasso \n\n11280 samples\n   29 predictor\n\nPre-processing: centered (30), scaled (30) \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 10152, 10152, 10152, 10152, 10152, 10152, ... \nResampling results across tuning parameters:\n\n  fraction  RMSE       Rsquared   MAE      \n  0.1       0.5953796  0.4234291  0.4653872\n  0.5       0.4485973  0.5660193  0.3533998\n  0.9       0.4295497  0.5957600  0.3368464\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was fraction = 0.9.\n\n\n\nParameter-tuning in our Lasso regression\nLet’s start with centering and scaling variables; why do we do that?\nCentering and Scaling in Lasso: Recall that the L1-norm puts constraints on the size of the coefficients of the Lasso regression. The size, of course, differs based on the different scales the variables are measured. Having \\(1\\) and up to \\(55\\) electric gadgets at home is not the same as earning between \\(1000\\) and \\(100,000\\) monthly (of whichever currency you want to imagine here). There are two immediate consequences of centering and scaling (or normalising). 1) There is no longer an intercept. 2) It is easier to rank the relative magnitude of the coefficients post-shrinkage.\nCross-validation in Lasso: Beyond training and testing dataframes, what a k-fold (commonly 10-fold) cross validation does is resample the data to find an optimal (λ) lambda/penalisation for the lasso model and assess its predictive error. Recall: The optimal λ (lambda)/penalisation minimizes the out-of-sample (or test) mean prediction error.\nThis tuning is already included in the chunk of code above.\n\n# Let's visualise our lasso model:\nplot(model_lasso)\n\n\n\n\n\n\n\ncat(\"The x-axis is the fraction of the full solution (i.e., ordinary least squares with no penalty)\")\n\nThe x-axis is the fraction of the full solution (i.e., ordinary least squares with no penalty)\n\n\nIt seems that the fraction used (0.9) is close to the non-penalised model (or OLS/linear). Our original claim that this particular set-up does not call for a Lasso approach was correct. We may want to try other machine learning algorithms to see if we can improve our initial predictions!\nA final note: there is some discussion over whether k-fold or really any cross-validation technique is optimal for choosing the lambda parameter in Lasso models (see for example, this Coursera video). It is true that cross-validation is something that we want to do for ALL our machine learning models. Just make sure to make an informed choice of which cv technique you’ll implement in your model.\nA setup with multiple categorical variables is problematic with Lasso!\nLasso models do not perform well with multiple categorical variables. As is our case. Primarily because, in order to work with categorical data in lasso models we encode them as dummies (for example, north = \\(1\\) and \\(0\\) otherwise, center = \\(1\\) and \\(0\\) otherwise). Lasso will thus only consider the dummy, and not the concept of the ‘region’). An alternative to that is a Group Lasso. If you’d like to read more on that, you can have a look at this Towards Data Science Post."
  },
  {
    "objectID": "predictionpolicy.html#conclusion",
    "href": "predictionpolicy.html#conclusion",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "Conclusion",
    "text": "Conclusion\nAt this point, our best predictions are made with a linear regression algorithm, which bested a lasso model. As we progress, we will explore other common machine learning algorithms to see whether our prediction capabilities improve!."
  },
  {
    "objectID": "predictionpolicy.html#practice-at-home",
    "href": "predictionpolicy.html#practice-at-home",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "Practice at home",
    "text": "Practice at home\nYou can replicate the exercises in Python or R using the Malawi dataset. However, if you’d like a challenge, you can also try running a Linear and a Lasso Linear model using a similar dataset for the country of Bolivia. The dataset was put together by McBride and Nichols for the same paper for which they used the Malawi dataset.\n Download Bolivia df CSV"
  },
  {
    "objectID": "predictionpolicy.html#readings",
    "href": "predictionpolicy.html#readings",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "Readings",
    "text": "Readings\nMandatory\n\nAn introduction to Statistical learning, Chapter 2, 3 (Regression), 5 (Cross-validation) and 6 (for more about Lasso).\nAthey, S. (2017). Beyond prediction: Using big data for policy problems. Science, 355(6324), 483-485.\nKleinberg, J., Ludwig, J., Mullainathan, S. and Obermeyer, Z., 2015. Prediction policy problems. American Economic Review, 105(5), pp.491-95.\n\nOptional readings\n\nKleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J. and Mullainathan, S., 2017. Human decisions and machine predictions. The Quarterly Journal of Economics, 133(1), pp.237-293.\nHanna, R., & Olken, B. A. (2018). Universal basic incomes versus targeted transfers: Anti-poverty programs in developing countries. Journal of Economic Perspectives, 32(4), 201-26. (exercise application)\nMcBride, L., & Nichols, A. (2018). Retooling poverty targeting using out-of-sample validation and machine learning. The World Bank Economic Review, 32(3), 531-550.pter 5.1\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "index.html#gentle-introduction-to-r-and-rstudio-and-python.",
    "href": "index.html#gentle-introduction-to-r-and-rstudio-and-python.",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "1. Gentle Introduction to R and Rstudio, and Python.",
    "text": "1. Gentle Introduction to R and Rstudio, and Python.\n\nIntroduction to the course\nIntroduction to the R statistical programming language with the Rstudio IDE\nIntroduction to the Python programming language with Visual Studio Code\n\nInstructors: Stephan, Alex and Michelle (who will give you a warm welcome!)"
  },
  {
    "objectID": "index.html#introduction-to-machine-learning-for-public-policy",
    "href": "index.html#introduction-to-machine-learning-for-public-policy",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "2. Introduction to Machine Learning for Public Policy",
    "text": "2. Introduction to Machine Learning for Public Policy\n\nPrediction Policy problems\nInference vs. prediction for policy analysis\nAssessing accuracy: bias-variance tradeoff\nTraining error vs. test error\nFeature selection: brief introduction to Lasso\n\nInstructors: Michelle González Amador\nReadings:\nMandatory\n\nAn introduction to Statistical learning, Chapter 2, 3 (Regression), 5 (Cross-validation) and 6 (for more about Lasso).\nAthey, S. (2017). Beyond prediction: Using big data for policy problems. Science, 355(6324), 483-485.\nKleinberg, J., Ludwig, J., Mullainathan, S. and Obermeyer, Z., 2015. Prediction policy problems. American Economic Review, 105(5), pp.491-95.\n\nOptional readings\n\nKleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J. and Mullainathan, S., 2017. Human decisions and machine predictions. The Quarterly Journal of Economics, 133(1), pp.237-293.\nHanna, R., & Olken, B. A. (2018). Universal basic incomes versus targeted transfers: Anti-poverty programs in developing countries. Journal of Economic Perspectives, 32(4), 201-26. (exercise application)\nMcBride, L., & Nichols, A. (2018). Retooling poverty targeting using out-of-sample validation and machine learning. The World Bank Economic Review, 32(3), 531-550.pter 5.1"
  },
  {
    "objectID": "index.html#classification",
    "href": "index.html#classification",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "3. Classification",
    "text": "3. Classification\n\nLogistic regression\nConfusion matrix\nPerformance metrics: Accuracy, Recall, Precision (…)\n\nInstructor: Dr. Stephan Dietrich\nReadings:\n\nAn introduction to Statistical learning Chapter 4\nAthey, S., & Imbens, G. W. (2019). Machine learning methods that economists should know about. Annual Review of Economics, 11, 685-725.\nMcBride, L., & Nichols, A. (2018). Retooling poverty targeting using out-of-sample validation and machine learning. The World Bank Economic Review, 32(3), 531-550.pter 5.1\n\nOptional Readings\n\nBondi-Kelly et al. (2023)- Predicting micronutrient deficiency with publicly available satellite data. In AI Magazine."
  },
  {
    "objectID": "index.html#tree-based-methods",
    "href": "index.html#tree-based-methods",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "4. Tree-based methods",
    "text": "4. Tree-based methods\n\nDecision Trees: a classification approach\nEnsemble learning: bagging and boosting.\n\nInstructor: Dr. Francisco Rosales\nReadings:\n\nAn introduction to Statistical Learning, Chapter 8.\n\nOptional Readings\n\nDietrich et al. (2022) - Economic Development, weather shocks, and child marriage in South Asia: A machine learning approach."
  },
  {
    "objectID": "index.html#fair-machine-learning-ethics",
    "href": "index.html#fair-machine-learning-ethics",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "5. Fair Machine Learning / Ethics",
    "text": "5. Fair Machine Learning / Ethics\n\nCommon Machine Learning algorithms in (public policy) action\nBlack box algorithms\nBiases\nEthical challenges\n\nReadings:\n\nFast AI: Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD, Chapter 4.\nKasy, M., & Abebe, R. (2021, March). Fairness, equality, and power in algorithmic decision-making. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 576-586).\nFairness and Machine Learning: Limitations and Opportunities, Chapter 4.\n\nInstructor: Dr. Juba Ziani"
  },
  {
    "objectID": "index.html#neural-networks",
    "href": "index.html#neural-networks",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "6. Neural Networks",
    "text": "6. Neural Networks\n\nNeural Network Architecture: neurons and layers\nInputs and output: the activation function (sigmoid, tahn…)\n\nInstructor: Prof. Dr. Robin Cowan\nOptional Readings\n\nChatsiou and Mikhaylov (2020). Deep Learning for Political Science. Arxiv preprint."
  },
  {
    "objectID": "policyChallenge.html",
    "href": "policyChallenge.html",
    "title": "Collaborative Policy Challenge",
    "section": "",
    "text": "The Machine Learning for Public Policy course has partnered with the United Nations High Commissioner for Refugees (UNHCR, for short) Innovation Office to bring you a live policy challenge:\nOn Thursday 7 March, 2024 Rebeca Moreno Jiménez, Lead Data Scientist at UNHCR Innovation, gave a brief lecture on how UNHCR uses Machine Learning and other technological innovations for human mobility policies (via &lt;&gt;Zoom); she introduced us to the live policy challenge she and her team have prepared for us. For those who were not able to attend the lecture, you can find the recording below. Challenge registrations for 2024 are closed. Thanks to all of you who are preparing for it!\nThe challenge consists of leveraging publicly available data to tackle human mobility challenges; below you can read the entire document prepared by UNHCR Innovation with details about:\nBelow you can download the PDF of the challenge:\nCollaborative Policy Challenge PDF"
  },
  {
    "objectID": "policyChallenge.html#challenge-format",
    "href": "policyChallenge.html#challenge-format",
    "title": "Collaborative Policy Challenge",
    "section": "Challenge Format",
    "text": "Challenge Format\nWhat do we expect to receive?\n\n📝 A one (at most two) page document where you:\n\nState the question you seek to answer with your ML model. The UNHCR PDF on the challenge has some nice suggestions for each stream. Feel free to use one of those, or get inspired by them and state your own question.\nCreate a policy text in which you describe - for a policy and general audience - which ML tools you are using, what your model has found, and why this finding is relevant for human mobility policies.\nA quick note or sentence on how your research is reproducible. Some options include: making the script (R or Python) available upon request to others; Creating a GitHub repository and including all materials there and linking it in your one-pager.\nFor the one page document, there is no format! Feel free to innovate on graphics, or stick to black and white. Whatever format you believe is best to convey your findings, that’s what you should do.\n\n\nWhat is the outcome of the challenge?\nOne team will be chosen as the winner of the Collaborative Policy Challenge, assessed by our ML4PP team and UNHCR Innovation. Depending on the theme of the winning project, they have the possibility to publish their findings as a UNHCR Innovation blog. Details on bragging and other perks will be discussed later 😊.\nAll participants who successfully submit a challenge proposal are also eligible for an EduBadge, which can be used for LinkedIn and/or for your CV. Participants from the MSc. of Public Policy an Human Development at UNU-MERIT will receive a certification upon completion of their degree."
  },
  {
    "objectID": "policyChallenge.html#organisation",
    "href": "policyChallenge.html#organisation",
    "title": "Collaborative Policy Challenge",
    "section": "Organisation",
    "text": "Organisation\nEnrollment to the challenge closed on 5 March (2024) COB. When we have the full list of participants, we will:\n\nCreate semi-random groups. The group you end up with should be relatively diverse, and this is the team with whom you’ll work with to tackle the policy challenge. We understand there are difficulties embedded in team work, including time-zone differences, availability, differing skills… but we encourage you to work through them! For this very same reason, we are giving a full month to work on the challenge. The month starts counting the moment Slack invitations are sent out 😉.\nSend you an invitation to a dedicated Slack space to coordinate the challenge. In the slack space, we’ll have a general channel where Stephan, Alex, Michelle, and some of our other instructors will be around to respond to questions, should they arise. There will also be private channels where you and your team can communicate. If you’ve never used Slack before, don’t worry! It’s very intuitive. Join the space via the invitation you’ll get in your email inbox and start exploring. If you’d like a tutorial, feel free to watch the 20 min. step-by-step youtube tutorial below (by Slack themselves!):\n\n\n\n\n\n Good luck, and happy coding!"
  }
]