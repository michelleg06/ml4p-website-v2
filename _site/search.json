[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An introduction to Machine Learning for Public Policy",
    "section": "",
    "text": ":::"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#gentle-introduction-to-r-and-rstudio-and-python.",
    "href": "index.html#gentle-introduction-to-r-and-rstudio-and-python.",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "1. Gentle Introduction to R and Rstudio, and Python.",
    "text": "1. Gentle Introduction to R and Rstudio, and Python.\n\nIntroduction to the course\nIntroduction to the R statistical programming language with the Rstudio IDE\nIntroduction to the Python programming language with Visual Studio Code\n\nInstructors: Stephan, Alex and Michelle (who will give you a warm welcome!)"
  },
  {
    "objectID": "index.html#introduction-to-machine-learning-for-public-policy",
    "href": "index.html#introduction-to-machine-learning-for-public-policy",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "2. Introduction to Machine Learning for Public Policy",
    "text": "2. Introduction to Machine Learning for Public Policy\n\nPrediction Policy problems\nInference vs. prediction for policy analysis\nAssessing accuracy: bias-variance tradeoff\nTraining error vs. test error\nFeature selection: brief introduction to Lasso\n\nInstructors: Michelle González Amador\nReadings:\nMandatory\n\nAn introduction to Statistical learning, Chapter 2, 3 (Regression), 5 (Cross-validation) and 6 (for more about Lasso).\nAthey, S. (2017). Beyond prediction: Using big data for policy problems. Science, 355(6324), 483-485.\nKleinberg, J., Ludwig, J., Mullainathan, S. and Obermeyer, Z., 2015. Prediction policy problems. American Economic Review, 105(5), pp.491-95.\n\nOptional readings\n\nKleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J. and Mullainathan, S., 2017. Human decisions and machine predictions. The Quarterly Journal of Economics, 133(1), pp.237-293.\nHanna, R., & Olken, B. A. (2018). Universal basic incomes versus targeted transfers: Anti-poverty programs in developing countries. Journal of Economic Perspectives, 32(4), 201-26. (exercise application)\nMcBride, L., & Nichols, A. (2018). Retooling poverty targeting using out-of-sample validation and machine learning. The World Bank Economic Review, 32(3), 531-550.pter 5.1"
  },
  {
    "objectID": "index.html#classification",
    "href": "index.html#classification",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "3. Classification",
    "text": "3. Classification\n\nLogistic regression\nConfusion matrix\nPerformance metrics: Accuracy, Recall, Precision (…)\n\nInstructor: Dr. Stephan Dietrich\nReadings:\n\nAn introduction to Statistical learning Chapter 4\nAthey, S., & Imbens, G. W. (2019). Machine learning methods that economists should know about. Annual Review of Economics, 11, 685-725.\nMcBride, L., & Nichols, A. (2018). Retooling poverty targeting using out-of-sample validation and machine learning. The World Bank Economic Review, 32(3), 531-550.pter 5.1\n\nOptional Readings\n\nBondi-Kelly et al. (2023)- Predicting micronutrient deficiency with publicly available satellite data. In AI Magazine."
  },
  {
    "objectID": "index.html#tree-based-methods",
    "href": "index.html#tree-based-methods",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "4. Tree-based methods",
    "text": "4. Tree-based methods\n\nDecision Trees: a classification approach\nEnsemble learning: bagging and boosting.\n\nInstructor: Dr. Francisco Rosales\nReadings:\n\nAn introduction to Statistical Learning, Chapter 8.\n\nOptional Readings\n\nDietrich et al. (2022) - Economic Development, weather shocks, and child marriage in South Asia: A machine learning approach."
  },
  {
    "objectID": "index.html#fair-machine-learning-ethics",
    "href": "index.html#fair-machine-learning-ethics",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "5. Fair Machine Learning / Ethics",
    "text": "5. Fair Machine Learning / Ethics\n\nCommon Machine Learning algorithms in (public policy) action\nBlack box algorithms\nBiases\nEthical challenges\n\nReadings:\n\nFast AI: Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD, Chapter 4.\nKasy, M., & Abebe, R. (2021, March). Fairness, equality, and power in algorithmic decision-making. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 576-586).\nFairness and Machine Learning: Limitations and Opportunities, Chapter 4.\n\nInstructor: Dr. Juba Ziani"
  },
  {
    "objectID": "index.html#neural-networks",
    "href": "index.html#neural-networks",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "6. Neural Networks",
    "text": "6. Neural Networks\n\nNeural Network Architecture: neurons and layers\nInputs and output: the activation function (sigmoid, tahn…)\n\nInstructor: Prof. Dr. Robin Cowan\nOptional Readings\n\nChatsiou and Mikhaylov (2020). Deep Learning for Political Science. Arxiv preprint."
  },
  {
    "objectID": "predictionpolicy.html",
    "href": "predictionpolicy.html",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "",
    "text": "In the video-lecture below you’ll be given a brief introduction to the prediction policy framework, and a primer on machine learning. Please take a moment to watch the 20 minute video.\n\nAre you still wondering what the difference is between Machine Learning and Econometrics? Take a few minutes to watch the video below.\n\nAfter watching the videos, we have a practical exercise.\n\n\nA key problem in the design of Social Policies is the identification of people in need of social assistance. Social policies usually work with tight budgets and limited fiscal space. To allocate resources efficiently, benefits need to be targeted to those who need them most. Yet, identifying needs isn’t easy and misclassifications can have severe and irreversible effects on people in need.\nThink of a social protection programme that allocates food vouchers to families with children at risk of malnutrition, or a programme that establishes needs-based school grants. What happens when these limited and finite resources are given to people that could do without, and those who need them most are excluded from them?\nIn this block we’ll work with real-world data from the country of Malawi to predict cash-transfer programme beneficiaries: People who live in poverty and need government assistance to make ends meet. The data comes from McBride and Nichol’s (2018) paper Retooling poverty targeting using out-of-sample validation and machine learning.\nDiscussion Points\nThe points below are meant to help you think critically about why we’re about to embark on a machine learning - targeting exercise. \n\n\nWhy is this a prediction policy problem? What would be a causal inference problem in this setting? Is it a regression or a classification problem?\n\n\nWhich variables and characteristics that we include in the prediction model can make a big difference?\n\n\nProgrammatically and conceptually, which type of characteristics do we want to consider for the prediction model?\n\n\nTechnically, how do we select which variables to include in a prediction model? How is this different from a causal inference problem?\n\n\n\n\nWhat are the practical implications of the bias-variance tradeoff in this application?\n\n\nWhat are potential risks of such a data driven targeting approach?\n\n\n\nIf you’d like to learn more about Social Protection Policies, take a look at this video Alex has made for us with a brilliant summary of the field\n(Yes, more videos!)"
  },
  {
    "objectID": "predictionpolicy.html#introducing-the-prediction-policy-framework",
    "href": "predictionpolicy.html#introducing-the-prediction-policy-framework",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "",
    "text": "In the video-lecture below you’ll be given a brief introduction to the prediction policy framework, and a primer on machine learning. Please take a moment to watch the 20 minute video.\n\nAre you still wondering what the difference is between Machine Learning and Econometrics? Take a few minutes to watch the video below.\n\nAfter watching the videos, we have a practical exercise.\n\n\nA key problem in the design of Social Policies is the identification of people in need of social assistance. Social policies usually work with tight budgets and limited fiscal space. To allocate resources efficiently, benefits need to be targeted to those who need them most. Yet, identifying needs isn’t easy and misclassifications can have severe and irreversible effects on people in need.\nThink of a social protection programme that allocates food vouchers to families with children at risk of malnutrition, or a programme that establishes needs-based school grants. What happens when these limited and finite resources are given to people that could do without, and those who need them most are excluded from them?\nIn this block we’ll work with real-world data from the country of Malawi to predict cash-transfer programme beneficiaries: People who live in poverty and need government assistance to make ends meet. The data comes from McBride and Nichol’s (2018) paper Retooling poverty targeting using out-of-sample validation and machine learning.\nDiscussion Points\nThe points below are meant to help you think critically about why we’re about to embark on a machine learning - targeting exercise. \n\n\nWhy is this a prediction policy problem? What would be a causal inference problem in this setting? Is it a regression or a classification problem?\n\n\nWhich variables and characteristics that we include in the prediction model can make a big difference?\n\n\nProgrammatically and conceptually, which type of characteristics do we want to consider for the prediction model?\n\n\nTechnically, how do we select which variables to include in a prediction model? How is this different from a causal inference problem?\n\n\n\n\nWhat are the practical implications of the bias-variance tradeoff in this application?\n\n\nWhat are potential risks of such a data driven targeting approach?\n\n\n\nIf you’d like to learn more about Social Protection Policies, take a look at this video Alex has made for us with a brilliant summary of the field\n(Yes, more videos!)"
  },
  {
    "objectID": "predictionpolicy.html#r-practical",
    "href": "predictionpolicy.html#r-practical",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "R Practical",
    "text": "R Practical\nYou can download the dataset by clicking on the button below.\n\nDownload Malawi.csv\n\n\nThe script below is a step by step on how to go about coding a predictive model using a linear regression. Despite its simplicity and transparency, i.e. the ease with which we can interpret its results, a linear model is not without challenges in machine learning.\n\n1. Preliminaries: working directory, libraries, data upload\n\nrm(list = ls()) # this line cleans your Global Environment.\nsetwd(\"/Users/lucas/Documents/UNU-CDO/courses/ml4p/ml4p-website-v2/data\") # set your working directory\n\n# Libraries\n\n# If this is your first time using R, you need to install the libraries before loading them. \n# To do that, you can uncomment the line that starts with install.packages(...) by removing the # symbol.    \n\n#install.packages(\"dplyr\", \"tidyverse\", \"caret\", \"corrplot\", \"Hmisc\", \"modelsummary\", \"plyr\", \"gt\", \"stargazer\", elasticnet\")\n\nlibrary(dplyr) # core package for dataframe manipulation. Usually installed and loaded with the tidyverse, but sometimes needs to be loaded in conjunction to avoid warnings.\nlibrary(tidyverse) # a large collection of packages for data manipulation and visualisation.  \nlibrary(caret) # a package with key functions that streamline the process for predictive modelling \nlibrary(corrplot) # a package to plot correlation matrices\nlibrary(Hmisc) # a package for general-purpose data analysis \nlibrary(modelsummary) # a package to describe model outputs\nlibrary(skimr) # a package to describe dataframes\nlibrary(plyr) # a package for data wrangling\nlibrary(gt) # a package to edit modelsummary (and other) tables\nlibrary(stargazer) # a package to visualise model output\n\ndata_malawi &lt;- read_csv(\"malawi.csv\") # the file is directly read from the working directory/folder previously set\n\n\n\n2. Get to know your data: visualisation and pre-processing\n\nskim(data_malawi) # describes the dataset in a nice format \n\n\nData summary\n\n\nName\ndata_malawi\n\n\nNumber of rows\n11280\n\n\nNumber of columns\n38\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n36\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nregion\n0\n1\n5\n6\n0\n3\n0\n\n\neatype\n0\n1\n5\n17\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlnexp_pc_month\n0\n1\n7.360000e+00\n6.800000e-01\n4.7800e+00\n6.890000e+00\n7.310000e+00\n7.760000e+00\n1.106000e+01\n▁▇▇▁▁\n\n\nhhsize\n0\n1\n4.550000e+00\n2.340000e+00\n1.0000e+00\n3.000000e+00\n4.000000e+00\n6.000000e+00\n2.700000e+01\n▇▂▁▁▁\n\n\nhhsize2\n0\n1\n2.613000e+01\n2.799000e+01\n1.0000e+00\n9.000000e+00\n1.600000e+01\n3.600000e+01\n7.290000e+02\n▇▁▁▁▁\n\n\nagehead\n0\n1\n4.246000e+01\n1.636000e+01\n1.0000e+01\n2.900000e+01\n3.900000e+01\n5.400000e+01\n1.040000e+02\n▅▇▅▂▁\n\n\nagehead2\n0\n1\n2.070610e+03\n1.618600e+03\n1.0000e+02\n8.410000e+02\n1.521000e+03\n2.916000e+03\n1.081600e+04\n▇▃▁▁▁\n\n\nnorth\n0\n1\n1.500000e-01\n3.600000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ncentral\n0\n1\n3.800000e-01\n4.900000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▅\n\n\nrural\n0\n1\n8.700000e-01\n3.300000e-01\n0.0000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n▁▁▁▁▇\n\n\nnevermarried\n0\n1\n3.000000e-02\n1.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nsharenoedu\n0\n1\n1.700000e-01\n2.600000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n2.500000e-01\n1.000000e+00\n▇▂▁▁▁\n\n\nshareread\n0\n1\n6.100000e-01\n3.800000e-01\n0.0000e+00\n3.300000e-01\n6.700000e-01\n1.000000e+00\n1.000000e+00\n▅▁▅▂▇\n\n\nnrooms\n0\n1\n2.500000e+00\n1.300000e+00\n0.0000e+00\n2.000000e+00\n2.000000e+00\n3.000000e+00\n1.600000e+01\n▇▂▁▁▁\n\n\nfloor_cement\n0\n1\n2.000000e-01\n4.000000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\nelectricity\n0\n1\n6.000000e-02\n2.300000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nflushtoilet\n0\n1\n3.000000e-02\n1.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nsoap\n0\n1\n1.400000e-01\n3.400000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nbed\n0\n1\n3.200000e-01\n4.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▃\n\n\nbike\n0\n1\n3.600000e-01\n4.800000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▅\n\n\nmusicplayer\n0\n1\n1.600000e-01\n3.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ncoffeetable\n0\n1\n1.200000e-01\n3.200000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\niron\n0\n1\n2.100000e-01\n4.000000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ndimbagarden\n0\n1\n3.200000e-01\n4.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▃\n\n\ngoats\n0\n1\n2.100000e-01\n4.100000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ndependratio\n0\n1\n1.120000e+00\n9.500000e-01\n0.0000e+00\n5.000000e-01\n1.000000e+00\n1.500000e+00\n9.000000e+00\n▇▂▁▁▁\n\n\nhfem\n0\n1\n2.300000e-01\n4.200000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ngrassroof\n0\n1\n7.400000e-01\n4.400000e-01\n0.0000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n▃▁▁▁▇\n\n\nmortarpestle\n0\n1\n5.000000e-01\n5.000000e-01\n0.0000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▇\n\n\ntable\n0\n1\n3.600000e-01\n4.800000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▅\n\n\nclock\n0\n1\n2.000000e-01\n4.000000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\nea\n0\n1\n7.606000e+01\n1.885500e+02\n1.0000e+00\n8.000000e+00\n1.900000e+01\n4.525000e+01\n9.010000e+02\n▇▁▁▁▁\n\n\nEA\n0\n1\n2.372322e+07\n7.241514e+06\n1.0101e+07\n2.040204e+07\n2.090352e+07\n3.053301e+07\n3.120209e+07\n▂▁▆▁▇\n\n\nhhwght\n0\n1\n2.387900e+02\n7.001000e+01\n7.9000e+01\n2.076000e+02\n2.471000e+02\n2.913000e+02\n3.587000e+02\n▂▁▇▇▂\n\n\npsu\n0\n1\n2.372322e+07\n7.241514e+06\n1.0101e+07\n2.040204e+07\n2.090352e+07\n3.053301e+07\n3.120209e+07\n▂▁▆▁▇\n\n\nstrataid\n0\n1\n1.560000e+01\n8.090000e+00\n1.0000e+00\n9.000000e+00\n1.500000e+01\n2.200000e+01\n3.000000e+01\n▅▇▇▆▅\n\n\nlnzline\n0\n1\n7.550000e+00\n0.000000e+00\n7.5500e+00\n7.550000e+00\n7.550000e+00\n7.550000e+00\n7.550000e+00\n▁▁▇▁▁\n\n\ncase_id\n0\n1\n2.372322e+10\n7.241514e+09\n1.0101e+10\n2.040204e+10\n2.090352e+10\n3.053301e+10\n3.120209e+10\n▂▁▆▁▇\n\n\n\n\n\n\n\nThe dataset contains 38 variables and 11,280 observations.\nNot all of these variables are relevant for our prediction model.\nTo find the labels and description of the variables, you can refer to the paper. \n\n[hhsize, hhsize2, age_head, age_head2, regions, rural, never married, share_of_adults_without_education, share_of_adults_who_can_read, number of rooms, cement floor, electricity, flush toilet, soap, bed, bike, music player, coffee table, iron, garden, goats]\n\nLuckily for us, we have no missing values (n_missing in summary output)!\n\nMany machine learning models cannot be trained when missing values are present (some exceptions exist).\nDealing with missingness is a non-trivial task:\n\n\nFirst and foremost, we should assess whether there is a pattern to missingness and if so, what that means to what we can learn from our (sub)population. If there is no discernible pattern, we can proceed to delete the missing values or impute them. A more detailed explanation and course of action can be found here.\n\nFeature selection: subsetting the dataset"
  }
]