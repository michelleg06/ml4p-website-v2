[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An introduction to Machine Learning for Public Policy",
    "section": "",
    "text": ":::"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#gentle-introduction-to-r-and-rstudio-and-python.",
    "href": "index.html#gentle-introduction-to-r-and-rstudio-and-python.",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "1. Gentle Introduction to R and Rstudio, and Python.",
    "text": "1. Gentle Introduction to R and Rstudio, and Python.\n\nIntroduction to the course\nIntroduction to the R statistical programming language with the Rstudio IDE\nIntroduction to the Python programming language with Visual Studio Code\n\nInstructors: Stephan, Alex and Michelle (who will give you a warm welcome!)"
  },
  {
    "objectID": "index.html#introduction-to-machine-learning-for-public-policy",
    "href": "index.html#introduction-to-machine-learning-for-public-policy",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "2. Introduction to Machine Learning for Public Policy",
    "text": "2. Introduction to Machine Learning for Public Policy\n\nPrediction Policy problems\nInference vs. prediction for policy analysis\nAssessing accuracy: bias-variance tradeoff\nTraining error vs. test error\nFeature selection: brief introduction to Lasso\n\nInstructors: Michelle González Amador\nReadings:\nMandatory\n\nAn introduction to Statistical learning, Chapter 2, 3 (Regression), 5 (Cross-validation) and 6 (for more about Lasso).\nAthey, S. (2017). Beyond prediction: Using big data for policy problems. Science, 355(6324), 483-485.\nKleinberg, J., Ludwig, J., Mullainathan, S. and Obermeyer, Z., 2015. Prediction policy problems. American Economic Review, 105(5), pp.491-95.\n\nOptional readings\n\nKleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J. and Mullainathan, S., 2017. Human decisions and machine predictions. The Quarterly Journal of Economics, 133(1), pp.237-293.\nHanna, R., & Olken, B. A. (2018). Universal basic incomes versus targeted transfers: Anti-poverty programs in developing countries. Journal of Economic Perspectives, 32(4), 201-26. (exercise application)\nMcBride, L., & Nichols, A. (2018). Retooling poverty targeting using out-of-sample validation and machine learning. The World Bank Economic Review, 32(3), 531-550.pter 5.1"
  },
  {
    "objectID": "index.html#classification",
    "href": "index.html#classification",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "3. Classification",
    "text": "3. Classification\n\nLogistic regression\nConfusion matrix\nPerformance metrics: Accuracy, Recall, Precision (…)\n\nInstructor: Dr. Stephan Dietrich\nReadings:\n\nAn introduction to Statistical learning Chapter 4\nAthey, S., & Imbens, G. W. (2019). Machine learning methods that economists should know about. Annual Review of Economics, 11, 685-725.\nMcBride, L., & Nichols, A. (2018). Retooling poverty targeting using out-of-sample validation and machine learning. The World Bank Economic Review, 32(3), 531-550.pter 5.1\n\nOptional Readings\n\nBondi-Kelly et al. (2023)- Predicting micronutrient deficiency with publicly available satellite data. In AI Magazine."
  },
  {
    "objectID": "index.html#tree-based-methods",
    "href": "index.html#tree-based-methods",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "4. Tree-based methods",
    "text": "4. Tree-based methods\n\nDecision Trees: a classification approach\nEnsemble learning: bagging and boosting.\n\nInstructor: Dr. Francisco Rosales\nReadings:\n\nAn introduction to Statistical Learning, Chapter 8.\n\nOptional Readings\n\nDietrich et al. (2022) - Economic Development, weather shocks, and child marriage in South Asia: A machine learning approach."
  },
  {
    "objectID": "index.html#fair-machine-learning-ethics",
    "href": "index.html#fair-machine-learning-ethics",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "5. Fair Machine Learning / Ethics",
    "text": "5. Fair Machine Learning / Ethics\n\nCommon Machine Learning algorithms in (public policy) action\nBlack box algorithms\nBiases\nEthical challenges\n\nReadings:\n\nFast AI: Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD, Chapter 4.\nKasy, M., & Abebe, R. (2021, March). Fairness, equality, and power in algorithmic decision-making. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 576-586).\nFairness and Machine Learning: Limitations and Opportunities, Chapter 4.\n\nInstructor: Dr. Juba Ziani"
  },
  {
    "objectID": "index.html#neural-networks",
    "href": "index.html#neural-networks",
    "title": "An Introduction to Machine Learning for Public Policy",
    "section": "6. Neural Networks",
    "text": "6. Neural Networks\n\nNeural Network Architecture: neurons and layers\nInputs and output: the activation function (sigmoid, tahn…)\n\nInstructor: Prof. Dr. Robin Cowan\nOptional Readings\n\nChatsiou and Mikhaylov (2020). Deep Learning for Political Science. Arxiv preprint."
  },
  {
    "objectID": "predictionpolicy.html",
    "href": "predictionpolicy.html",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "",
    "text": "In the video-lecture below you’ll be given a brief introduction to the prediction policy framework, and a primer on machine learning. Please take a moment to watch the 20 minute video.\n\nAre you still wondering what the difference is between Machine Learning and Econometrics? Take a few minutes to watch the video below.\n\nAfter watching the videos, we have a practical exercise.\n\n\nA key problem in the design of Social Policies is the identification of people in need of social assistance. Social policies usually work with tight budgets and limited fiscal space. To allocate resources efficiently, benefits need to be targeted to those who need them most. Yet, identifying needs isn’t easy and misclassifications can have severe and irreversible effects on people in need.\nThink of a social protection programme that allocates food vouchers to families with children at risk of malnutrition, or a programme that establishes needs-based school grants. What happens when these limited and finite resources are given to people that could do without, and those who need them most are excluded from them?\nIn this block we’ll work with real-world data from the country of Malawi to predict cash-transfer programme beneficiaries: People who live in poverty and need government assistance to make ends meet. The data comes from McBride and Nichol’s (2018) paper Retooling poverty targeting using out-of-sample validation and machine learning.\nDiscussion Points\nThe points below are meant to help you think critically about why we’re about to embark on a machine learning - targeting exercise. \n\n\nWhy is this a prediction policy problem? What would be a causal inference problem in this setting? Is it a regression or a classification problem?\n\n\nWhich variables and characteristics that we include in the prediction model can make a big difference?\n\n\nProgrammatically and conceptually, which type of characteristics do we want to consider for the prediction model?\n\n\nTechnically, how do we select which variables to include in a prediction model? How is this different from a causal inference problem?\n\n\n\n\nWhat are the practical implications of the bias-variance tradeoff in this application?\n\n\nWhat are potential risks of such a data driven targeting approach?\n\n\n\nIf you’d like to learn more about Social Protection Policies, take a look at this video Alex has made for us with a brilliant summary of the field\n(Yes, more videos!)"
  },
  {
    "objectID": "predictionpolicy.html#introducing-the-prediction-policy-framework",
    "href": "predictionpolicy.html#introducing-the-prediction-policy-framework",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "",
    "text": "In the video-lecture below you’ll be given a brief introduction to the prediction policy framework, and a primer on machine learning. Please take a moment to watch the 20 minute video.\n\nAre you still wondering what the difference is between Machine Learning and Econometrics? Take a few minutes to watch the video below.\n\nAfter watching the videos, we have a practical exercise.\n\n\nA key problem in the design of Social Policies is the identification of people in need of social assistance. Social policies usually work with tight budgets and limited fiscal space. To allocate resources efficiently, benefits need to be targeted to those who need them most. Yet, identifying needs isn’t easy and misclassifications can have severe and irreversible effects on people in need.\nThink of a social protection programme that allocates food vouchers to families with children at risk of malnutrition, or a programme that establishes needs-based school grants. What happens when these limited and finite resources are given to people that could do without, and those who need them most are excluded from them?\nIn this block we’ll work with real-world data from the country of Malawi to predict cash-transfer programme beneficiaries: People who live in poverty and need government assistance to make ends meet. The data comes from McBride and Nichol’s (2018) paper Retooling poverty targeting using out-of-sample validation and machine learning.\nDiscussion Points\nThe points below are meant to help you think critically about why we’re about to embark on a machine learning - targeting exercise. \n\n\nWhy is this a prediction policy problem? What would be a causal inference problem in this setting? Is it a regression or a classification problem?\n\n\nWhich variables and characteristics that we include in the prediction model can make a big difference?\n\n\nProgrammatically and conceptually, which type of characteristics do we want to consider for the prediction model?\n\n\nTechnically, how do we select which variables to include in a prediction model? How is this different from a causal inference problem?\n\n\n\n\nWhat are the practical implications of the bias-variance tradeoff in this application?\n\n\nWhat are potential risks of such a data driven targeting approach?\n\n\n\nIf you’d like to learn more about Social Protection Policies, take a look at this video Alex has made for us with a brilliant summary of the field\n(Yes, more videos!)"
  },
  {
    "objectID": "predictionpolicy.html#r-practical",
    "href": "predictionpolicy.html#r-practical",
    "title": "Prediction Policy Problems: Linear Models and Lasso Regression",
    "section": "R Practical",
    "text": "R Practical\nYou can download the dataset by clicking on the button below.\n\nDownload Malawi.csv\n\n\nThe script below is a step by step on how to go about coding a predictive model using a linear regression. Despite its simplicity and transparency, i.e. the ease with which we can interpret its results, a linear model is not without challenges in machine learning.\n\n1. Preliminaries: working directory, libraries, data upload\n\nrm(list = ls()) # this line cleans your Global Environment.\nsetwd(\"/Users/lucas/Documents/UNU-CDO/courses/ml4p/ml4p-website-v2\") # set your working directory\n\n# Libraries\n\n# If this is your first time using R, you need to install the libraries before loading them. \n# To do that, you can uncomment the line that starts with install.packages(...) by removing the # symbol.    \n\n#install.packages(\"dplyr\", \"tidyverse\", \"caret\", \"corrplot\", \"Hmisc\", \"modelsummary\", \"plyr\", \"gt\", \"stargazer\", \"elasticnet\", \"sandwich\")\n\nlibrary(dplyr) # core package for dataframe manipulation. Usually installed and loaded with the tidyverse, but sometimes needs to be loaded in conjunction to avoid warnings.\nlibrary(tidyverse) # a large collection of packages for data manipulation and visualisation.  \nlibrary(caret) # a package with key functions that streamline the process for predictive modelling \nlibrary(corrplot) # a package to plot correlation matrices\nlibrary(Hmisc) # a package for general-purpose data analysis \nlibrary(modelsummary) # a package to describe model outputs\nlibrary(skimr) # a package to describe dataframes\nlibrary(plyr) # a package for data wrangling\nlibrary(gt) # a package to edit modelsummary (and other) tables\nlibrary(stargazer) # a package to visualise model output\n\ndata_malawi &lt;- read_csv(\"data/malawi.csv\") # the file is directly read from the working directory/folder previously set\n\n\n\n2. Get to know your data: visualisation and pre-processing\n\nskim(data_malawi) # describes the dataset in a nice format \n\n\nData summary\n\n\nName\ndata_malawi\n\n\nNumber of rows\n11280\n\n\nNumber of columns\n38\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n36\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nregion\n0\n1\n5\n6\n0\n3\n0\n\n\neatype\n0\n1\n5\n17\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlnexp_pc_month\n0\n1\n7.360000e+00\n6.800000e-01\n4.7800e+00\n6.890000e+00\n7.310000e+00\n7.760000e+00\n1.106000e+01\n▁▇▇▁▁\n\n\nhhsize\n0\n1\n4.550000e+00\n2.340000e+00\n1.0000e+00\n3.000000e+00\n4.000000e+00\n6.000000e+00\n2.700000e+01\n▇▂▁▁▁\n\n\nhhsize2\n0\n1\n2.613000e+01\n2.799000e+01\n1.0000e+00\n9.000000e+00\n1.600000e+01\n3.600000e+01\n7.290000e+02\n▇▁▁▁▁\n\n\nagehead\n0\n1\n4.246000e+01\n1.636000e+01\n1.0000e+01\n2.900000e+01\n3.900000e+01\n5.400000e+01\n1.040000e+02\n▅▇▅▂▁\n\n\nagehead2\n0\n1\n2.070610e+03\n1.618600e+03\n1.0000e+02\n8.410000e+02\n1.521000e+03\n2.916000e+03\n1.081600e+04\n▇▃▁▁▁\n\n\nnorth\n0\n1\n1.500000e-01\n3.600000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ncentral\n0\n1\n3.800000e-01\n4.900000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▅\n\n\nrural\n0\n1\n8.700000e-01\n3.300000e-01\n0.0000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n▁▁▁▁▇\n\n\nnevermarried\n0\n1\n3.000000e-02\n1.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nsharenoedu\n0\n1\n1.700000e-01\n2.600000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n2.500000e-01\n1.000000e+00\n▇▂▁▁▁\n\n\nshareread\n0\n1\n6.100000e-01\n3.800000e-01\n0.0000e+00\n3.300000e-01\n6.700000e-01\n1.000000e+00\n1.000000e+00\n▅▁▅▂▇\n\n\nnrooms\n0\n1\n2.500000e+00\n1.300000e+00\n0.0000e+00\n2.000000e+00\n2.000000e+00\n3.000000e+00\n1.600000e+01\n▇▂▁▁▁\n\n\nfloor_cement\n0\n1\n2.000000e-01\n4.000000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\nelectricity\n0\n1\n6.000000e-02\n2.300000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nflushtoilet\n0\n1\n3.000000e-02\n1.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nsoap\n0\n1\n1.400000e-01\n3.400000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\nbed\n0\n1\n3.200000e-01\n4.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▃\n\n\nbike\n0\n1\n3.600000e-01\n4.800000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▅\n\n\nmusicplayer\n0\n1\n1.600000e-01\n3.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ncoffeetable\n0\n1\n1.200000e-01\n3.200000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▁\n\n\niron\n0\n1\n2.100000e-01\n4.000000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ndimbagarden\n0\n1\n3.200000e-01\n4.700000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▃\n\n\ngoats\n0\n1\n2.100000e-01\n4.100000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ndependratio\n0\n1\n1.120000e+00\n9.500000e-01\n0.0000e+00\n5.000000e-01\n1.000000e+00\n1.500000e+00\n9.000000e+00\n▇▂▁▁▁\n\n\nhfem\n0\n1\n2.300000e-01\n4.200000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\ngrassroof\n0\n1\n7.400000e-01\n4.400000e-01\n0.0000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n▃▁▁▁▇\n\n\nmortarpestle\n0\n1\n5.000000e-01\n5.000000e-01\n0.0000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▇\n\n\ntable\n0\n1\n3.600000e-01\n4.800000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n1.000000e+00\n▇▁▁▁▅\n\n\nclock\n0\n1\n2.000000e-01\n4.000000e-01\n0.0000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n1.000000e+00\n▇▁▁▁▂\n\n\nea\n0\n1\n7.606000e+01\n1.885500e+02\n1.0000e+00\n8.000000e+00\n1.900000e+01\n4.525000e+01\n9.010000e+02\n▇▁▁▁▁\n\n\nEA\n0\n1\n2.372322e+07\n7.241514e+06\n1.0101e+07\n2.040204e+07\n2.090352e+07\n3.053301e+07\n3.120209e+07\n▂▁▆▁▇\n\n\nhhwght\n0\n1\n2.387900e+02\n7.001000e+01\n7.9000e+01\n2.076000e+02\n2.471000e+02\n2.913000e+02\n3.587000e+02\n▂▁▇▇▂\n\n\npsu\n0\n1\n2.372322e+07\n7.241514e+06\n1.0101e+07\n2.040204e+07\n2.090352e+07\n3.053301e+07\n3.120209e+07\n▂▁▆▁▇\n\n\nstrataid\n0\n1\n1.560000e+01\n8.090000e+00\n1.0000e+00\n9.000000e+00\n1.500000e+01\n2.200000e+01\n3.000000e+01\n▅▇▇▆▅\n\n\nlnzline\n0\n1\n7.550000e+00\n0.000000e+00\n7.5500e+00\n7.550000e+00\n7.550000e+00\n7.550000e+00\n7.550000e+00\n▁▁▇▁▁\n\n\ncase_id\n0\n1\n2.372322e+10\n7.241514e+09\n1.0101e+10\n2.040204e+10\n2.090352e+10\n3.053301e+10\n3.120209e+10\n▂▁▆▁▇\n\n\n\n\n\n\n\nThe dataset contains 38 variables and 11,280 observations.\nNot all of these variables are relevant for our prediction model.\nTo find the labels and description of the variables, you can refer to the paper. \n\n[hhsize, hhsize2, age_head, age_head2, regions, rural, never married, share_of_adults_without_education, share_of_adults_who_can_read, number of rooms, cement floor, electricity, flush toilet, soap, bed, bike, music player, coffee table, iron, garden, goats]\n\nLuckily for us, we have no missing values (n_missing in summary output)!\n\nMany machine learning models cannot be trained when missing values are present (some exceptions exist).\nDealing with missingness is a non-trivial task:\n\n\nFirst and foremost, we should assess whether there is a pattern to missingness and if so, what that means to what we can learn from our (sub)population. If there is no discernible pattern, we can proceed to delete the missing values or impute them. A more detailed explanation and course of action can be found here.\n\nFeature selection: subsetting the dataset\nAs part of our data pre-processing we will subset the dataframe, such that only relevant variables are left. * Relevant: variables/features about a household that could help us determine whether they are in poverty. That way, we save some memory space; but also, we can call the full set of variables in a dataframe in one go!\nvariables to delete (not included in the identified set above):\n[ea, EA, hhwght, psu, strataid, lnzline, case_id, eatype] \n\nN/B: Feature selection is a critical process (and we normally don’t have a paper to guide us through it): from a practical point of view, a model with less predictors may be easier to interpret. Also, some models may be negatively affected by non-informative predictors. This process is similar to traditional econometric modelling, but we should not conflate predictive and explanatory modelling. Importantly, please note that we are not interested in knowing why something happens, but rather in what is likely to happen given some known data. Hence:\n\n# object:vector that contains the names of the variables that we want to get rid of\n\ncols &lt;- c(\"ea\", \"EA\", \"psu\",\"hhwght\", \"strataid\", \"lnzline\", \"case_id\",\"eatype\")\n\n\n# subset of the data_malawi object:datframe\ndata_malawi &lt;- data_malawi[,-which(colnames(data_malawi) %in% cols)] # the minus sign indicates deletion of cols\n\nA few notes for you:\n\na dataframe follows the form data[rows,colums]\ncolnames() is a function that identifies the column names of an object of class dataframe\nwhich() is an R base function that gives you the position of some value\na minus sign will delete either the identified position in the row or the column space\n\n\n\nData visualisation\nA quick and effective way to take a first glance at our data is to plot histograms of relevant (numeric) features.\nRecall (from the skim() dataframe summary output) that only two variables are non-numeric. However, we need to make a distinction between class factor and class numeric/numeric.\n\n# identify categorical variables to transform from class numeric to factor\n# using a for-loop: print the number of unique values by variable\n\nfor (i in 1:ncol(data_malawi)) { # iterate over the length of columns in the data_malawi df\n    \n    # store the number of unique values in column.i \n    x &lt;- length(unique(data_malawi[[i]]))\n    \n    # print the name of column.i\n    print(colnames(data_malawi[i]))\n    # print the number of unique values in column.i\n    print(x)\n    \n}\n\n[1] \"lnexp_pc_month\"\n[1] 11266\n[1] \"hhsize\"\n[1] 19\n[1] \"hhsize2\"\n[1] 19\n[1] \"agehead\"\n[1] 88\n[1] \"agehead2\"\n[1] 88\n[1] \"north\"\n[1] 2\n[1] \"central\"\n[1] 2\n[1] \"rural\"\n[1] 2\n[1] \"nevermarried\"\n[1] 2\n[1] \"sharenoedu\"\n[1] 47\n[1] \"shareread\"\n[1] 28\n[1] \"nrooms\"\n[1] 16\n[1] \"floor_cement\"\n[1] 2\n[1] \"electricity\"\n[1] 2\n[1] \"flushtoilet\"\n[1] 2\n[1] \"soap\"\n[1] 2\n[1] \"bed\"\n[1] 2\n[1] \"bike\"\n[1] 2\n[1] \"musicplayer\"\n[1] 2\n[1] \"coffeetable\"\n[1] 2\n[1] \"iron\"\n[1] 2\n[1] \"dimbagarden\"\n[1] 2\n[1] \"goats\"\n[1] 2\n[1] \"dependratio\"\n[1] 62\n[1] \"hfem\"\n[1] 2\n[1] \"grassroof\"\n[1] 2\n[1] \"mortarpestle\"\n[1] 2\n[1] \"table\"\n[1] 2\n[1] \"clock\"\n[1] 2\n[1] \"region\"\n[1] 3\n\n# If you want to optimise your code, using for loops is not ideal. In R, there exists a family of functions called Apply whose purpose is to apply some function to all the elements in an object. For the time being, iterating over 38 columns is fast enough and we don't need to think about optimising our code. We'll also see an example later on on how to use one of the functions from the apply family. You can also refer to this blog (https://www.r-bloggers.com/2021/05/apply-family-in-r-apply-lapply-sapply-mapply-and-tapply/) if you want to learn more about it. \n\n\nNotice that we have a few variables with 2 unique values and one variable with 3 unique values. We should transform these into factor() class. Recall from the introduction tab that in object-oriented programming correctly identifying the variable type (vector class) is crucial for data manipulation and arithmetic operations. We can do this one by one, or in one shot. I’ll give an example of both:\n\n# == One by one == # \n\n# transform a variable in df to factor, and pass it on to the df to keep the change\n# first, sneak peak at the first 5 observations (rows) of the vector\nhead(data_malawi$north) # returns 1 = North \n\n[1] 1 1 1 1 1 1\n\ndata_malawi$north &lt;- factor(data_malawi$north, levels = c(0, 1), labels = c(\"NotNorth\", \"North\"))\n\nstr(data_malawi$north) # returns 2?\n\n Factor w/ 2 levels \"NotNorth\",\"North\": 2 2 2 2 2 2 2 2 2 2 ...\n\nhead(data_malawi$north) # returns north (which was = 1 before), hooray!\n\n[1] North North North North North North\nLevels: NotNorth North\n\n# R stores factors as 1...n; but the label remains the initial numeric value assigned (1 for true, 0 false)\n# We have also explicityly told 0 is NotNorth and 1 is North (by the order that follows the line of code)\n\n\n# transform all binary/categorical data into factor class\nmin_count &lt;- 3 # vector: 3 categories is our max number of categories found\n\n# == Apply family: function apply and lapply==#\n# apply a length(unique(x)) function to all the columns (rows = 1, columns = 2) of the data_malawi dataframe, then\n# store boolean (true/false) if the number of unique values is lower or equal to the min_count vector\nn_distinct2 &lt;- apply(data_malawi, 2, function(x) length(unique(x))) &lt;= min_count\n\n# print(n_distinct2) # prints boolean indicator object (so you know what object you have created)\n\n# select the identified categorical variables and transform them into factors\ndata_malawi[n_distinct2] &lt;- lapply(data_malawi[n_distinct2], factor) \n\nVisualise your data: histograms, bar plots, tables\nLet’s start with histograms of numeric (continuous) variables.\n\n# == HISTOGRAMS == #\n\n# Select all variables in the dataframe which are numeric, and can therefore be plotted as a histogram.\nmalawi_continuous &lt;- as.data.frame(data_malawi %&gt;% select_if(~is.numeric(.))) \n\n# a quick glance at the summary statistics of our continuous variables\ndatasummary_skim(malawi_continuous) # from modelsummary pkg, output as plot in Plot Viewer\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  lnexp_pc_month\n                  11266\n                  0\n                  7.4\n                  0.7\n                  4.8\n                  7.3\n                  11.1\n                  \n                \n                \n                  hhsize\n                  19\n                  0\n                  4.5\n                  2.3\n                  1.0\n                  4.0\n                  27.0\n                  \n                \n                \n                  hhsize2\n                  19\n                  0\n                  26.1\n                  28.0\n                  1.0\n                  16.0\n                  729.0\n                  \n                \n                \n                  agehead\n                  88\n                  0\n                  42.5\n                  16.4\n                  10.0\n                  39.0\n                  104.0\n                  \n                \n                \n                  agehead2\n                  88\n                  0\n                  2070.6\n                  1618.6\n                  100.0\n                  1521.0\n                  10816.0\n                  \n                \n                \n                  sharenoedu\n                  47\n                  0\n                  0.2\n                  0.3\n                  0.0\n                  0.0\n                  1.0\n                  \n                \n                \n                  shareread\n                  28\n                  0\n                  0.6\n                  0.4\n                  0.0\n                  0.7\n                  1.0\n                  \n                \n                \n                  nrooms\n                  16\n                  0\n                  2.5\n                  1.3\n                  0.0\n                  2.0\n                  16.0\n                  \n                \n                \n                  dependratio\n                  62\n                  0\n                  1.1\n                  0.9\n                  0.0\n                  1.0\n                  9.0\n                  \n                \n        \n      \n    \n\n\n# Hmisc package, quick and painless hist.data.frame() function\n# but first, make sure to adjust the number of rows and columns to be displayed on your Plot Viewer\npar(mfrow = c(3, 3)) # 3 rows * 3 columns (9 variables)\nhist.data.frame(malawi_continuous)\n\n\n\n\n\n\n\n\nNow, let’s plot bar graphs and write tables of factor (categorical) variables:\n\n# == BAR GRAPHS == #\nmalawi_factor &lt;- data_malawi %&gt;% select_if(~is.factor(.)) # subset of the data with all factor variables\n\npar(mfrow = c(3, 7)) # 7 rows, 3 columns (21 variables = length of df)\n\nfor (i in 1:ncol(malawi_factor)) { # Loop over all the columns in the factor df subset\n    \n    # store data in column.i as x\n    x &lt;- malawi_factor[,i]\n    \n    # store name of column.i as x_name\n    x_name &lt;- colnames(malawi_factor[i])\n    \n    # Plot bar graph of x using Rbase plotting tools\n    barplot(table(x),\n            main = paste(x_name)\n    )\n    \n}\n\n\n\n\n\n\n\n# == TABLES == #\n\n# We can also show tables of all factor variables (to get precise frequencies not displayed in bar plots)\nllply(.data=malawi_factor, .fun=table) # create tables of all the variables in dataframe using the plyr package\n\n$north\n\nNotNorth    North \n    9600     1680 \n\n$central\n\n   0    1 \n6960 4320 \n\n$rural\n\n   0    1 \n1440 9840 \n\n$nevermarried\n\n    0     1 \n10930   350 \n\n$floor_cement\n\n   0    1 \n9036 2244 \n\n$electricity\n\n    0     1 \n10620   660 \n\n$flushtoilet\n\n    0     1 \n10962   318 \n\n$soap\n\n   0    1 \n9740 1540 \n\n$bed\n\n   0    1 \n7653 3627 \n\n$bike\n\n   0    1 \n7194 4086 \n\n$musicplayer\n\n   0    1 \n9426 1854 \n\n$coffeetable\n\n   0    1 \n9956 1324 \n\n$iron\n\n   0    1 \n8962 2318 \n\n$dimbagarden\n\n   0    1 \n7658 3622 \n\n$goats\n\n   0    1 \n8856 2424 \n\n$hfem\n\n   0    1 \n8697 2583 \n\n$grassroof\n\n   0    1 \n2953 8327 \n\n$mortarpestle\n\n   0    1 \n5635 5645 \n\n$table\n\n   0    1 \n7249 4031 \n\n$clock\n\n   0    1 \n9039 2241 \n\n$region\n\nCentre  North  South \n  4320   1680   5280 \n\n\n\nWhat have we learned from the data visualisation?\n\nNothing worrying about the data itself. McBride and Nichols did a good job of pre-processing the data for us. No pesky missing values, or unknown categories.\nFrom the bar plots, we can see that for the most part, people tend not to own assets. Worryingly, there is a lack of soap, flush toilets and electricity, all of which are crucial for human capital (health and education).\nFrom the histograms, we can see log per capita expenditure is normally distributed, but if we remove the log, it’s incredibly skewed. Poverty is endemic. Households tend not to have too many educated individuals, and their size is non-trivially large (with less rooms than people need).\n\n\n\nRelationships between features\nTo finalise our exploration of the dataset, we should define:\n\nthe target variable (a.k.a. outcome of interest)\ncorrelational insights\n\nLet’s visualise two distinct correlation matrices; for our numeric dataframe, which includes our target variable, we will plot a Pearson r correlation matrix. For our factor dataframe, to which we will add our continuous target, we will plot a Spearman rho correlation matrix. Both types of correlation coefficients are interpreted the same (0 = no correlation, 1 perfect positive correlation, -1 perfect negative correlation).\n\n# = = PEARSON CORRELATION MATRIX = = #\n\nM &lt;- cor(malawi_continuous) # create a correlation matrix of the continuous dataset, cor() uses Pearson's correlation coefficient as default. This means we can only take the correlation between continuous variables\ncorrplot(M, method=\"circle\", addCoef.col =\"black\", number.cex = 0.8) # visualise it in a nice way\n\n\n\n\n\n\n\n\nWe can already tell that the size of the household and dependent ratio are highly negatively correlated to our target variable.\n\n# = = SPEARMAN CORRELATION MATRIX = = #\n\nmalawi_factorC &lt;- as.data.frame(lapply(malawi_factor,as.numeric)) # coerce dataframe to numeric, as the cor() command only takes in numeric types\nmalawi_factorC$lnexp_pc_month &lt;- malawi_continuous$lnexp_pc_month # include target variable in the dataframe\n\nM2 &lt;- cor(malawi_factorC, method = \"spearman\")\ncorrplot(M2, method=\"circle\", addCoef.col =\"black\", number.cex = 0.3) # visualise it in a nice way\n\n\n\n\n\n\n\n\nOwnership of some assets stands out: soap, a cement floor, electricity, a bed… ownership of these (and a couple of other) assets is positively correlated to per capita expenditure. Living in a rural area, on the other hand, is negtively correlated to our target variable.\nWe can also spot some correlation coefficients that equal zero. In some situations, the data generating mechanism can create predictors that only have a single unique value (i.e. a “zero-variance predictor”). For many ML models (excluding tree-based models), this may cause the model to crash or the fit to be unstable. Here, the only \\(0\\) we’ve spotted is not in relation to our target variable.\nBut we do observe some near-zero-variance predictors. Besides uninformative, these can also create unstable model fits. There’s a few strategies to deal with these; the quickest solution is to remove them. A second option, which is especially interesting in scenarios with a large number of predictors/variables, is to work with penalised models. We’ll discuss this option below.\n\n\n\n3. Model fit: data partition and performance evaluation parameters\nWe now have a general idea of the structure of the data we are working with, and what we’re trying to predict: per capita expenditures, which we believe are a proxy for poverty prevalence. Measured by the log of per capita monthly expenditure in our dataset, the variable is named lnexp_pc_month.\nThe next step is create a simple linear model (OLS) to predict per capita expenditure using the variables in our dataset, and introduce the elements with which we will evaluate our model.\n\nData Partinioning\nWhen we want to build predictive models for machine learning purposes, it is important to have (at least) two data sets. A training data set from which our model will learn, and a test data set containing the same features as our training data set; we use the second dataset to see how well our predictive model extrapolates to other samples (i.e. is it generalisable?). To split our main data set into two, we will work with an 80/20 split.\nThe 80/20 split has its origins in the Pareto Principle, which states that ‘in most cases, 80% of effects from from 20% of causes’. Though there are other test/train splitting options, this partitioning method is a good place to start, and indeed standard in the machine learning field.\n\n# First, set a seed to guarantee replicability of the process\nset.seed(1234) # you can use any number you want, but to replicate the results in this tutorial you need to use this number\n\n# We could split the data manually, but the caret package includes an useful function\n\ntrain_idx &lt;- createDataPartition(data_malawi$lnexp_pc_month, p = .8, list = FALSE, times = 1)\nhead(train_idx) # notice that observation 5 corresponds to resame indicator 7 and so on. We're shuffling and picking!\n\n     Resample1\n[1,]         1\n[2,]         2\n[3,]         4\n[4,]         5\n[5,]         7\n[6,]         8\n\nTrain_df &lt;- data_malawi[ train_idx,]\nTest_df  &lt;- data_malawi[-train_idx,]\n\n# Note that we have created training and testing dataframes as an 80/20 split of the original dataset.\n\n\n\nPrediction with Linear Models\nWe will start by fitting a predictive model using the training dataset; that is, our target variable log of monthly per capita expenditures or lnexp_pc_month will be a \\(Y\\) dependent variable in a linear model \\(Y_i = \\alpha + x'\\beta_i + \\epsilon_i\\), and the remaining features in the data frame correspond to the row vectors \\(x'\\beta\\).\n\nmodel1 &lt;- lm(lnexp_pc_month ~ .,Train_df) # the dot after the squiggle ~ asks the lm() function tu use all other variables in the dataframe as predictors to the dependent variable lnexp_pc_month\n\nstargazer(model1, type = \"text\") # printed in the console as text\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                          lnexp_pc_month       \n-----------------------------------------------\nhhsize                       -0.292***         \n                              (0.006)          \n                                               \nhhsize2                      0.012***          \n                             (0.0005)          \n                                               \nagehead                        0.003           \n                              (0.002)          \n                                               \nagehead2                    -0.00004**         \n                             (0.00002)         \n                                               \nnorthNorth                   0.080***          \n                              (0.014)          \n                                               \ncentral1                     0.252***          \n                              (0.010)          \n                                               \nrural1                       -0.055***         \n                              (0.017)          \n                                               \nnevermarried1                0.271***          \n                              (0.028)          \n                                               \nsharenoedu                   -0.105***         \n                              (0.020)          \n                                               \nshareread                    0.075***          \n                              (0.015)          \n                                               \nnrooms                       0.039***          \n                              (0.004)          \n                                               \nfloor_cement1                0.102***          \n                              (0.017)          \n                                               \nelectricity1                 0.372***          \n                              (0.027)          \n                                               \nflushtoilet1                 0.329***          \n                              (0.033)          \n                                               \nsoap1                        0.215***          \n                              (0.014)          \n                                               \nbed1                         0.104***          \n                              (0.013)          \n                                               \nbike1                        0.094***          \n                              (0.011)          \n                                               \nmusicplayer1                 0.111***          \n                              (0.015)          \n                                               \ncoffeetable1                 0.137***          \n                              (0.019)          \n                                               \niron1                        0.130***          \n                              (0.014)          \n                                               \ndimbagarden1                 0.102***          \n                              (0.010)          \n                                               \ngoats1                       0.080***          \n                              (0.012)          \n                                               \ndependratio                  -0.045***         \n                              (0.006)          \n                                               \nhfem1                        -0.066***         \n                              (0.012)          \n                                               \ngrassroof1                   -0.096***         \n                              (0.016)          \n                                               \nmortarpestle1                0.033***          \n                              (0.010)          \n                                               \ntable1                       0.051***          \n                              (0.011)          \n                                               \nclock1                       0.058***          \n                              (0.014)          \n                                               \nregionNorth                                    \n                                               \n                                               \nregionSouth                                    \n                                               \n                                               \nConstant                     7.978***          \n                              (0.044)          \n                                               \n-----------------------------------------------\nObservations                   9,024           \nR2                             0.599           \nAdjusted R2                    0.597           \nResidual Std. Error      0.428 (df = 8995)     \nF Statistic         479.041*** (df = 28; 8995) \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n# We can also estimate iid and robust standard errors, using the model output package modelsummary()\nms &lt;- modelsummary(model1,\n             vcov = list(\"iid\",\"robust\"), # include iid and HC3 (robust) standard errors\n             statistic = c(\"p = {p.value}\",\"s.e. = {std.error}\"),\n             stars = TRUE,\n             output = \"gt\"\n             ) # plotted as an image / object in \"gt\" format\n\nms %&gt;% tab_header(\n    title = md(\"**Linear Models with iid and robust s.e.**\"),\n    subtitle = md(\"Target: (log) per capita monthly expenditure\")\n)\n\n\n\n\n\n\n\nLinear Models with iid and robust s.e.\n\n\nTarget: (log) per capita monthly expenditure\n\n\n\n(1)\n(2)\n\n\n\n\n(Intercept)\n7.978***\n7.978***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.044\ns.e. = 0.069\n\n\nhhsize\n-0.292***\n-0.292***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.006\ns.e. = 0.029\n\n\nhhsize2\n0.012***\n0.012***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.000\ns.e. = 0.003\n\n\nagehead\n0.003\n0.003\n\n\n\np = 0.108\np = 0.137\n\n\n\ns.e. = 0.002\ns.e. = 0.002\n\n\nagehead2\n-0.000*\n-0.000*\n\n\n\np = 0.015\np = 0.032\n\n\n\ns.e. = 0.000\ns.e. = 0.000\n\n\nnorthNorth\n0.080***\n0.080***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.014\ns.e. = 0.015\n\n\ncentral1\n0.252***\n0.252***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.010\ns.e. = 0.010\n\n\nrural1\n-0.055**\n-0.055**\n\n\n\np = 0.001\np = 0.001\n\n\n\ns.e. = 0.017\ns.e. = 0.017\n\n\nnevermarried1\n0.271***\n0.271***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.028\ns.e. = 0.037\n\n\nsharenoedu\n-0.105***\n-0.105***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.020\ns.e. = 0.021\n\n\nshareread\n0.075***\n0.075***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.015\ns.e. = 0.015\n\n\nnrooms\n0.039***\n0.039***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.004\ns.e. = 0.004\n\n\nfloor_cement1\n0.102***\n0.102***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.017\ns.e. = 0.018\n\n\nelectricity1\n0.372***\n0.372***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.027\ns.e. = 0.029\n\n\nflushtoilet1\n0.329***\n0.329***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.033\ns.e. = 0.042\n\n\nsoap1\n0.215***\n0.215***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.014\ns.e. = 0.014\n\n\nbed1\n0.104***\n0.104***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.013\ns.e. = 0.013\n\n\nbike1\n0.094***\n0.094***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.011\ns.e. = 0.011\n\n\nmusicplayer1\n0.111***\n0.111***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.015\ns.e. = 0.015\n\n\ncoffeetable1\n0.137***\n0.137***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.019\ns.e. = 0.019\n\n\niron1\n0.130***\n0.130***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.014\ns.e. = 0.014\n\n\ndimbagarden1\n0.102***\n0.102***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.010\ns.e. = 0.010\n\n\ngoats1\n0.080***\n0.080***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.012\ns.e. = 0.012\n\n\ndependratio\n-0.045***\n-0.045***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.006\ns.e. = 0.009\n\n\nhfem1\n-0.066***\n-0.066***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.012\ns.e. = 0.013\n\n\ngrassroof1\n-0.096***\n-0.096***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.016\ns.e. = 0.016\n\n\nmortarpestle1\n0.033**\n0.033**\n\n\n\np = 0.001\np = 0.001\n\n\n\ns.e. = 0.010\ns.e. = 0.010\n\n\ntable1\n0.051***\n0.051***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.011\ns.e. = 0.012\n\n\nclock1\n0.058***\n0.058***\n\n\n\np = &lt;0.001\np = &lt;0.001\n\n\n\ns.e. = 0.014\ns.e. = 0.014\n\n\nNum.Obs.\n9024\n9024\n\n\nR2\n0.599\n0.599\n\n\nR2 Adj.\n0.597\n0.597\n\n\nAIC\n10317.3\n10317.3\n\n\nBIC\n10530.5\n10530.5\n\n\nLog.Lik.\n-5128.658\n-5128.658\n\n\nRMSE\n0.43\n0.43\n\n\nStd.Errors\nIID\nHC3\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n# You can see that the variable region was not included in the output (this is because we already have dummies of north (not north = south), and central region). We may need to clean our dataset un some further steps. \n\nRecall that one of the assumptions of a linear model is that errors are independent and identically distributed. We could run some tests to determine this, but with the contrast of the iid and robust errors (HC3) in the modelsummary output table we can already tell that this is not an issue/something to worry about in our estimations.\nBesides the regression output table, we can can also visualise the magnitude and significance of the coefficients with a plot. The further away the variable (dot) marker is from the \\(0\\) line, the larger the magnitude.\n\nmodelplot(model1) + \n          aes(color = ifelse(p.value &lt; 0.001, \"Significant\", \"Not significant\")) +\n              scale_color_manual(values = c(\"grey\", \"black\")\n                                 )\n\n\n\n\n\n\n\n           # grey points indicate statistically insignificat (p&gt;0.001) coefficient estimates\n           # The scale of the plot is large due to the intercept estimate\n\nNot unlike what we saw in out correlation matrix, household size, electricity, having a flush toilet… the magnitude of the impact of these variables on monthly per capita expenditure is significantly larger than that of other assets/predictors.\n\n\nPerformance Indicators\nIn predictive modelling, we are interested in the following performance metrics:\n\nModel residuals: recall residuals are the observed value minus the predicted value. We can estimate a model’s Root Mean Squared Error (RMSE) or the Mean Absolute Error (MAE). Residuals allow us to quantify the extent to which the predicted response value (for a given observation) is close to the true response value. Small RMSE or MAE values indicate that the prediction is close to the true observed value.\nThe p-values: represented by stars *** (and a pre-defined critical threshold, e.g. 0.05), they point to the predictive power of each feature in the model; i.e. that the event does not occur by chance. In the same vein, the magnitude of the coefficient is also important, especially given that we are interested in explanatory power and not causality.\nThe R-squared: arguably the go-to indicator for performance assessment. Low R^2 values are not uncommon, especially in the social sciences. However, when hoping to use a model for predictive purposes, a low R^2 is a bad sign, large number of statistically significant features notwithstanding. The drawback from relying solely on this measure is that it does not take into consideration the problem of model over-fitting; i.e. you can inflate the R-squared by adding as many variables as you want, even if those variables have little predicting power. This method will yield great results in the training data, but will under perform when extrapolating the model to the test (or indeed any other) data set.\n\nResiduals\n\n# = = Model Residuals = = #\nprint(summary(model1$residuals))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-3.483927 -0.288657 -0.007872  0.000000  0.266950  1.887123 \n\n\nRecall that the residual is estimated as the true (observed) value minus the predicted value. Thus: the max(imum) error of 1.88 suggests that the model under-predicted expenditure by circa (log) $2 (or $6.5) for at least one observation. Fifty percent of the predictions (between the first and third quartiles) lie between (log) $0.28 and (log) $0.26 over the true value. From the estimation of the prediction residuals we obtain the popular measure of performance evaluation known as the Root Mean Squared Error (RMSE, for short).\n\n# Calculate the RMSE for the training dataset, or the in-sample RMSE.\n\n# 1. Predict values on the training dataset\np0 &lt;- predict(model1, Train_df)\n\n# 2. Obtain the errors (predicted values minus observed values of target variable)\nerror0 &lt;- p0 - Train_df[[\"lnexp_pc_month\"]]\n\n# 3. In-sample RMSE\nRMSE_insample &lt;- sqrt(mean(error0 ^ 2))\nprint(RMSE_insample)\n\n[1] 0.4271572\n\n# TIP: Notice that the in-sample RMSE we have just estimated was also printed in the linear model (regression) output tables!\n# The table printed with the stargazer package returns this value at the bottom, under the header Residual Std. Error (0.428)\n# The table printed with the modelsummary package returns this value at the bottom, under the header RMSE (0.43), rounded up. \n\nThe RMSE (0.4271) gives us an absolute number that indicates how much our predicted values deviate from the true (observed) number. This is all in reference to the target vector (a.k.a. our outcome variable). Think of the question, how far, on average, are the residuals away from zero? Generally speaking, the lower the value, the better the model fit. Besides being a good measure of goodness of fit, the RMSE is also useful for comparing the ability of our model to make predictions on different (e.g. test) data sets. The in-sample RMSE should be close or equal to the out-of-sample RMSE.\nIn this case, our RMSE is ~0.4 units away from zero. Given the range of the target variable (roughly 4 to 11), the number seems to be relatively small and close enough to zero.\nP-values\nRecall the large number of statistically significant features in our model. The coefficient plot, where we indicate that statistical significance is defined by a p-value threshold of 0.001, a strict rule (given the popularity of the more relaxed 0.05 critical value), shows that only 4 out of 29 features/variables do not meet this criterion. We can conclude that the features we have selected are relevant predictors.\nR-squared\n\nprint(summary(model1))\n\n\nCall:\nlm(formula = lnexp_pc_month ~ ., data = Train_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4839 -0.2887 -0.0079  0.2669  1.8871 \n\nCoefficients: (2 not defined because of singularities)\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7.978e+00  4.409e-02 180.945  &lt; 2e-16 ***\nhhsize        -2.919e-01  6.429e-03 -45.397  &lt; 2e-16 ***\nhhsize2        1.206e-02  4.811e-04  25.067  &lt; 2e-16 ***\nagehead        2.749e-03  1.712e-03   1.606  0.10835    \nagehead2      -4.168e-05  1.717e-05  -2.428  0.01519 *  \nnorthNorth     8.001e-02  1.439e-02   5.560 2.78e-08 ***\ncentral1       2.521e-01  1.023e-02  24.639  &lt; 2e-16 ***\nrural1        -5.483e-02  1.687e-02  -3.250  0.00116 ** \nnevermarried1  2.706e-01  2.839e-02   9.533  &lt; 2e-16 ***\nsharenoedu    -1.045e-01  2.006e-02  -5.212 1.91e-07 ***\nshareread      7.514e-02  1.502e-02   5.004 5.72e-07 ***\nnrooms         3.898e-02  4.032e-03   9.670  &lt; 2e-16 ***\nfloor_cement1  1.016e-01  1.742e-02   5.835 5.58e-09 ***\nelectricity1   3.718e-01  2.696e-02  13.793  &lt; 2e-16 ***\nflushtoilet1   3.295e-01  3.263e-02  10.099  &lt; 2e-16 ***\nsoap1          2.150e-01  1.413e-02  15.220  &lt; 2e-16 ***\nbed1           1.040e-01  1.297e-02   8.019 1.20e-15 ***\nbike1          9.395e-02  1.064e-02   8.831  &lt; 2e-16 ***\nmusicplayer1   1.111e-01  1.451e-02   7.658 2.08e-14 ***\ncoffeetable1   1.369e-01  1.866e-02   7.338 2.36e-13 ***\niron1          1.302e-01  1.378e-02   9.455  &lt; 2e-16 ***\ndimbagarden1   1.018e-01  1.017e-02  10.017  &lt; 2e-16 ***\ngoats1         7.964e-02  1.168e-02   6.820 9.68e-12 ***\ndependratio   -4.501e-02  5.848e-03  -7.697 1.55e-14 ***\nhfem1         -6.570e-02  1.238e-02  -5.307 1.14e-07 ***\ngrassroof1    -9.590e-02  1.575e-02  -6.089 1.18e-09 ***\nmortarpestle1  3.288e-02  1.027e-02   3.200  0.00138 ** \ntable1         5.074e-02  1.149e-02   4.417 1.01e-05 ***\nclock1         5.840e-02  1.423e-02   4.103 4.11e-05 ***\nregionNorth           NA         NA      NA       NA    \nregionSouth           NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4278 on 8995 degrees of freedom\nMultiple R-squared:  0.5986,    Adjusted R-squared:  0.5973 \nF-statistic:   479 on 28 and 8995 DF,  p-value: &lt; 2.2e-16\n\n\nWe have printed our model’s output once more, now using the r base command summary(). The other packages (stargazer and modelsummary) are great if you want to export your results in text, html, latex format, but not necessary if you just want to print your output. The estimated (Multiple) R-squared of 0.59 tells us that our model predicts around 60 per cent of the variation in the independent variable (our target, log of per capita monthly expenditures). Also note that when we have a large number of predictors, it’s best to look at the Adjusted R-squared (of 0.59), which corrects or adjusts for this by only increasing when a new feature improves the model more so than what would be expected by chance.\n\n\nOut of sample predictions\nNow that we have built and evaluated our model in the training dataset, we can proceed to make out-of-sample predictions. That is, see how our model performs in the test dataset.\n\np &lt;- predict(model1, Test_df)\n\n### Observed summary statistics of target variable in full dataset\n\nprint(summary(data_malawi$lnexp_pc_month))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.777   6.893   7.305   7.359   7.758  11.064 \n\n### Predictions based on the training dataset\n\nprint(summary(p0))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  6.011   6.980   7.308   7.357   7.661  10.143 \n\n### Predictions from the testing dataset\n\nprint(summary(p))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  6.052   6.999   7.323   7.375   7.670   9.774 \n\n\nThe summary statistics for the predictions with the train and test datasets are very close to one another. This is an indication that our model extrapolates well to other datasets. Compared to the observed summary statistics of the target variable, they’re relatively close, with the largest deviations observed at the minimum and maximum values.\n\n\nThe bias-variance tradeoff in practice\nWe previously mentioned that the RMSE metric could also be used to compare between train and test model predictions. Let us estimate the out-of-sample RMSE:\n\nerror &lt;- p - Test_df[[\"lnexp_pc_month\"]] # predicted values minus actual values\nRMSE_test &lt;- sqrt(mean(error^2))\nprint(RMSE_test) # this is known as the out-of-sample RMSE\n\n[1] 0.4284404\n\n\nNotice that the in-sample RMSE [\\(0.4271572\\)] is very close to the out-of-sample RMSE [\\(0.4284404\\)]. This means that our model makes consistent predictions across different data sets. We also know by now that these predictions are relatively good. At least, we hit the mark around 60 per cent of the time. What we are observing here is a model that has found a balance between bias and variance. However, both measures can still improve. For one, McBride and Nichols (2018) report &gt; 80 per cent accuracy in their poverty predictions for Malawi. But please note that, at this point, we are not replicating their approach. They document using a classification model, which means that they previously used a poverty line score and divided the sample between individuals below and above the poverty line.\nWhat do we mean by bias in a model?\nThe bias is the difference between the average prediction of our model and the true (observed) value. Minimising the bias is analogous to minimising the RMSE.\nWhat do we mean by variance in a model?\nIt is the observed variability of our model prediction for a given data point (how much the model can adjust given the data set). A model with high variance would yield low error values in the training data but high errors in the test data.\nHence, consistent in and out of sample RMSE scores = bias/variance balance.\n\n\nFine-tuning model parameters\nCan we fine-tune model parameters? The quick answer is yes! Every algorithm has a set of parameters that can be adjusted/fine-tuned to improve our estimations. Even in the case of a simple linear model, we can try our hand at fine-tuning with, for example, cross-validation.\nWhat is cross-validation?\nBroadly speaking, it is a technique that allows us to assess the performance of our machine learning model. How so? Well, it looks at the ‘stability’ of the model. It’s a measure of how well our model would work on new, unseen data (is this ringing a bell yet?); i.e. it has correctly observed and recorded the patterns in the data and has not captured too much noise (what we know as the error term, or what we are unable to explain with our model). K-fold cross-validation is a good place to start for such a thing. In the words of The Internet™, what k-fold cross validation does is:\nSplit the input dataset into K groups\n    For each group:\n        - Take one group as the reserve or test data set.\n        - Use remaining groups as the training dataset.\n        - Fit the model on the training set and evaluate the performance of the model using the test set.\nTL;DR We’re improving our splitting technique!\nAn Illustration by Eugenia Anello\n\n\n\n\n\n\n# Let's rumble! \n\nset.seed(12345)\n\n# create an object that defines the training method as cross-validation and number of folds (caret pkg)\ncv_10fold &lt;- trainControl(\n    method = \"cv\", #cross-validation\n    number = 10 # k-fold = 10-fold (split the data into 10 similar-sized samples)\n)\n\nset.seed(12345)\n\n# train a model \nols_kfold &lt;- train(\n    lnexp_pc_month ~ .,\n    data = Train_df,\n    method = 'lm', # runs a linear regression model (or ols)\n    trControl = cv_10fold # use 10 folds to cross-validate\n)\nols_kfold2 &lt;- train(\n    lnexp_pc_month ~ .,\n    data = Test_df,\n    method = 'lm', # runs a linear regression model (or ols)\n    trControl = cv_10fold # use 10 folds to cross-validate\n)\n\nols_kfold3 &lt;- train(\n    lnexp_pc_month ~ .,\n    data = data_malawi,\n    method = 'lm', # runs a linear regression model (or ols)\n    trControl = cv_10fold # use 10 folds to cross-validate\n)\n\n### Linear model with train dataset\nprint(ols_kfold)\n\nLinear Regression \n\n9024 samples\n  29 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 8121, 8121, 8123, 8122, 8121, 8121, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.4295064  0.5947776  0.3365398\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n#### Linear model with test dataset\nprint(ols_kfold2)\n\nLinear Regression \n\n2256 samples\n  29 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 2030, 2030, 2030, 2030, 2031, 2031, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.4312944  0.5991526  0.3379473\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n### Linear model with full dataset\")\nprint(ols_kfold3)\n\nLinear Regression \n\n11280 samples\n   29 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 10152, 10152, 10152, 10152, 10152, 10152, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.4291473  0.5965825  0.3363802\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nHaving cross-validated our model with 10 folds, we can see that the results are virtually the same. We have a RMSE of around .43, an R^2 of .59, (and a MAE of .33). We ran the same model for the train, test and full datasets and find results are consistent. Recall the R^2 tells us the model’s predictive ability and the RMSE and MAE the model’s accuracy.\nNOTE: k-fold cross-validation replaces our original 80/20 split (we don’t need to do that anymore!) Therefore, we use the full dataset in the train() function. Using the train, test and full datasets in the example above was just for pedagogical purposes but it is no longer necessary and we can use the reported estimates of R^2 (close to 1!), RMSE (close to 0!) and MAE (let’s go low!) for model assessment without comparing them to another set of predictions. These reported parameters are estimated as the average of the R^2, RMSE, and MAE for all the folds.\n\n\n\n4. Feature selection with Lasso linear regression"
  }
]